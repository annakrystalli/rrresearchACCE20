[["index.html", "Reproducible Research Data &amp; Project Management in R Course description Learning Outcomes Course Outline", " Reproducible Research Data &amp; Project Management in R Course Instructor: Anna Krystalli Mar-Apr 2020 Course description In order to ensure robustness of outputs and maximise the benefits of ACCE research to future researchers and society more generally, it is important to share the underlying code and data. But for sharing to have any impact, such materials need to be created FAIR (findable, accessible, interoperable, reusable), i.e. they must be adequately described, archived, and made discoverable to an appropriate standard. Additionally, if analyses are to be deemed robust, they must be at the very least reproducible, but ideally well documented and reviewable. R and Rstudio tools and conventions offer a powerful framework for making modern, open, reproducible and collaborative computational workflows more accessible to researchers. This course focuses on data and project management through R and Rstudio, will introduce students to best practice and equip them with modern tools and techniques for managing data and computational workflows to their full potential. The course is designed to be relevant to students with a wide range of backgrounds, working with anything from relatively small sets of data collected from field or experimental observations, to those taking a more computational approach and bigger datasets. Learning Outcomes By the end of the workshop, participants will be able to: Understand the basics of good research data management and be able to produce clean datasets with appropriate metadata. Manage computational projects for reproducibility, reuse and collaboration. Use version control to track the evolution of research projects. Use R tools and conventions to document code and analyses and produce reproducible reports. Be able to publish, share materials and collaborate through the web. Understand why this all matters! Course Outline Day 1: 30th April 10:00 - 16:30 OPTIONAL Welcome Basics Intro to R &amp; Rstudio R basics Data types, structures &amp; classes Indexing and Subsetting The tidyverse way Day 2: 4th May 09:00 - 17:00 Project Management Data management basics Projects in Rstudio Good File Naming Paths and projects structure Data Munging Iteration Merging data Functions Day 3: 5th May 09:00 - 17:00 Metadata Intro to metadata Creating metadata with dataspice Analysing &amp; Presenting data Plotting basics Literate programming Day 4: 7th May 09:00 - 17:00 Version Control Version control with Git Collaboration through GitHub Packaging Code Writing &amp; documenting functions Capturing metadata incl. dependencies Checking &amp; Testing functions Putting it all together: Research Compendia Creating a research compendium This work is licensed under a Creative Commons Attribution 4.0 International License. Sources of Materials The first few chapters of the Basics section were heavily sourced and adapted from “Software Carpentry: R for Reproducible Scientific Analysis.” Thomas Wright and Naupaka Zimmerman (eds): Version 2016.06, June 2016 https://github.com/swcarpentry/r-novice-gapminder, . Licensed under CC-BY 4.0 2018–2020 by The Carpentries. The Good File Naming chapter was heavily sourced from “File organization for reproducible research.” Data Carpentry Reproducible Research Committee. 2016. Licensed under CC-BY 4.0 2018–2020 by The Carpentries. Small sections in the Data Munging section where inspired by text in the online version of “R 4 Data Science”, Garrett Grolemund &amp; Hadley Wickham. Licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. Images contained throughout the materials and watermarked with Scriberia were sourced from “Illustrations from the Turing Way book dashes”, . Images were created by Scriberia for The Turing Way community. Licensed under CC-BY 4.0 by The Turing Way. Data for the the main practical parts of the course were sourced from the NEON Data Portal, provided by the National Ecological Observatory Network. 2019 Provisional data downloaded from http://data.neonscience.org on 2019-08-06. Battelle, Boulder, CO, USA. Data Products: NEON.DOM.SITE.DP1.10098.001 Name: Woody plant vegetation structure Description: Structure measurements, including height, canopy diameter, and stem diameter, as well as mapped position of individual woody plants Query information: Start Date-Time for Queried Data: 2018-08-15 16:00 (UTC) End Date-Time for Queried Data: 2018-08-29 16:00 (UTC) Domains: D01:D9 LICENSE Disclaimer THE NEON DATA PRODUCTS ARE PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE NEON DATA PRODUCTS BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE NEON DATA PRODUCTS. Materials for the Research Compendium section were sourced from Carl Boettiger. (2018, April 17). cboettig/noise-phenomena: Supplement to: “From noise to knowledge: how randomness generates novel phenomena and reveals information” , accompanying the publication: Carl Boettiger . From noise to knowledge: how randomness generates novel phenomena and reveals information. Published in Ecology Letters, 22 May 2018 https://doi.org/10.1111/ele.13085. "],["welcome-slides.html", "Welcome", " Welcome View Slides "],["basics.html", "(PART) Basics", " (PART) Basics "],["rstudio-basics.html", "Intro to R &amp; Rstudio R Rstudio Working with R in Rstudio cloud", " Intro to R &amp; Rstudio R R is an open source language and environment for statistical computing and graphics. Features Powerful analytical tools, well suited to scientific analyses and data science. Analyses are performed using scripted commands, making them easy to record, edit, rerun. Source code is open, ie available to access, inspect, modify remix and publish derivatives. Large, active creative communities around development and training. Ecosystem constantly under development with continuous improvements. Install R R Environment includes: an effective data handling and storage facility, a suite of operators for calculations on arrays, a large, coherent, integrated collection of intermediate tools for data analysis, graphical facilities for data analysis and display either on-screen or on hardcopy, a well-developed, simple and effective programming language which includes conditionals, loops, user-defined functions and input and output facilities. Rstudio integrated development environment (IDE) for R Features console syntax-highlighting editor that supports direct code execution, as well as tools for: plotting debugging workspace management Find out more Working with R in Rstudio cloud We will be working online in RStudio Cloud throughout the course so we can all work in the same computational environment. This will save a lot of time by avoiding having to debug individual installation problems. To start working in R and Rstudio, we need to log in to Rstudio cloud: Launch Rstudio Cloud This will normally end up with logging you into your account workspace. To start working in R, we need to create a new project. Click on New Project This creates and deploys a new untitled Rstudio cloud project in your Workspace. Edit the project names by clicking on Untitled. Name it something like r_basics We are now in Rstudio, running in the cloud and can start running R code! "],["r-basics.html", "R Basics Using R in the console Working in scripts Using R as a calculator Mathematical functions Variables and assignment Comparing things", " R Basics Using R in the console The most basic way to interact with R is to type code directly in the console type expression to evaluate hit return output of the evaluation of the expression is printed to the console below The simplest thing you could do with R is do arithmetic. 1 + 100 ## [1] 101 If you type in an incomplete command, R will wait for you to complete it: 1 + + Any time you hit return and the R session shows a + instead of a &gt;, it means it’s waiting for you to complete the command. If you want to cancel a command you can hit Esc and RStudio will give you back the &gt; prompt. Working in scripts To make code and workflow reproducible and easy to re-run, it’s better to save code in a script and use the script editor to edit it. This way, there is a complete record of te analysis. Creating a new script Click on File &gt; New File &gt; R Script. Click on the save icon or (like any other file) using keyboard shortcut CTRL / CMD + S Executing commands from scripts RStudio allows you to execute commands directly from the script editor by using Ctrl + Enter shortcut (on Macs, Cmd + Return will work, too). When you execute command from a script, the line of code in the script indicated by the cursor or all of the commands in the currently highlighted will be sent to the console. You can find other keyboard shortcuts in Tools &gt; Keyboard Shortcuts Help or in the RStudio IDE cheatsheet. Comments You can add comments to your code by using a hash symbol #. Any text on a line of code following # is ignored by R when it executes code. 1 + 10 # this text does nothing ## [1] 11 Using R as a calculator When using R as a calculator, the order of operations is the same as you would have learned back in school. From highest to lowest precedence: Parentheses: (, ) Exponents: ^ or ** Multiply: * Divide: / Add: + Subtract: - 3 + 5 * 2 ## [1] 13 Use parentheses to group operations in order to force the order of evaluation if it differs from the default, or to make clear what you intend. (3 + 5) * 2 ## [1] 16 Really small or large numbers get a scientific notation: 2/10000 ## [1] 2e-04 Which is shorthand for “multiplied by 10^XX”. So 2e-4 is shorthand for 2 * 10^(-4). You can write numbers in scientific notation too: 5e3 # Note the lack of minus here ## [1] 5000 Mathematical functions R has many built in mathematical functions. To call a function, we can type its name, followed by open and closing parentheses. Anything we type inside the parentheses is called the function’s arguments: sin(1) # trigonometry functions ## [1] 0.841471 log(1) # natural logarithm ## [1] 0 log10(10) # base-10 logarithm ## [1] 1 exp(0.5) # e^(1/2) ## [1] 1.648721 Don’t worry about trying to remember every function in R. You can look them up on Google, or if you can remember the start of the function’s name, use the tab completion in RStudio. This is one advantage that RStudio has over R on its own, it has auto-completion abilities that allow you to more easily look up functions, their arguments, and the values that they take. Typing a ? before the name of a command will open the help page for that command. As well as providing a detailed description of the command and how it works, scrolling to the bottom of the help page will usually show a collection of code examples which illustrate command usage. We’ll go through an example later. Variables and assignment We can store values in variables using the assignment operator &lt;-, like this: x &lt;- 1/40 Notice that assignment does not print a value. Instead, we stored it for later in something called a variable. x now contains the value 0.025: x ## [1] 0.025 Look for the Environment tab in one of the panes of RStudio, and you will see that x and its value have appeared. Our variable x can be used in place of a number in any calculation that expects a number: log(x) ## [1] -3.688879 Notice also that variables can be reassigned: x &lt;- 100 x used to contain the value 0.025 and now it has the value 100. Assignment values can contain the variable being assigned to: x &lt;- x + 1 #notice how RStudio updates its description of x on the top right tab y &lt;- x * 2 The right hand side of the assignment can be any valid R expression. The right hand side is fully evaluated before the assignment occurs. It is also possible to use the = operator for assignment: x = 1/40 But this is much less common among R users. The most important thing is to be consistent with the operator you use. There are occasionally places where it is less confusing to use &lt;- than =, and it is the most common symbol used in the community. So the recommendation is to use &lt;-. On variable names Variable names can contain letters, numbers, underscores and periods. They cannot start with a number nor contain spaces at all. Different people use different conventions for long variable names, these include periods.between.words camelCaseToSeparateWords snake_case: underscores_between_words While I suggest you use snake_case, ultimately what you use is up to you, but be consistent. Comparing things We can also do comparison in R: x &lt;- 1 x &lt; 2 # less than ## [1] TRUE x &lt;= 1 # less than or equal to ## [1] TRUE x &gt; 0 # greater than ## [1] TRUE x &gt;= -9 # greater than or equal to ## [1] TRUE x == 1 # equality (note two equals signs, read as &quot;is equal to&quot;) ## [1] TRUE 1 != 2 # inequality (read as &quot;is not equal to&quot;) ## [1] TRUE x %in% c(1, 5) # membership (read as &quot;is member of&quot;) ## [1] TRUE A word of warning about comparing numbers: you should never use == to compare two numbers unless they are integers (a data type which can specifically represent only whole numbers). Controlling flow using logical statements Comparing a single value results in TRUE or FALSE. This feature allows us to build conditional statements to control execution flow. if(x &gt; 5){ print(&quot;x is greater than 5&quot;) }else{ print(&quot;x is less than 5&quot;) } ## [1] &quot;x is less than 5&quot; "],["data-types-structures-and-classes.html", "Data types, structures and classes Base types Base data types Data Structures Lists Data.frames", " Data types, structures and classes Base types Every object has a base type and only R-core can create new types. Over all there are 25 different base object types. Base data types There are 5 base data types: double, integer, complex, logical, character as well as NULL. No matter how complicated your analyses become, all data in R is interpreted as one of these basic data types. You can inspect the type of a value or object through function typeof(). typeof(3.14) ## [1] &quot;double&quot; typeof(1L) # The L suffix forces the number to be an integer, since by default R uses float numbers ## [1] &quot;integer&quot; typeof(TRUE) ## [1] &quot;logical&quot; typeof(&#39;banana&#39;) ## [1] &quot;character&quot; typeof(NULL) ## [1] &quot;NULL&quot; Data Structures Arrays and type coersion The distinguishing feature of arrays is that all values are of the same data type. Arrays can take values of any base data type and span any number of dimensions. However, all values must be of the same base data type. This allows for efficent calculation and matrix mathematics. The strictness also has some really important consequences which introduces another key concept in R, that of type coersion. Vectors and Type Coercion Vectors Vectors are one dimensional arrays. To better understand the importance of data types and coersion, let’s meet a special case of an array, the vector. To create a new vector use function vector(). You can specify the length of the vector with argument length and the base data type through my_vector &lt;- vector(length = 3) my_vector ## [1] FALSE FALSE FALSE A vector in R is essentially an ordered list of things, with the special condition that everything in the vector must be the same basic data type. typeof(my_vector) ## [1] &quot;logical&quot; If you don’t choose the datatype, it’ll default to logical; or, you can declare an empty vector of whatever type you like. another_vector &lt;- vector(mode=&#39;character&#39;, length=3) another_vector ## [1] &quot;&quot; &quot;&quot; &quot;&quot; You can also make series of numbers: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(10) ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1,10, by=0.1) ## [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 ## [16] 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 ## [31] 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 5.4 ## [46] 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 ## [61] 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 ## [76] 8.5 8.6 8.7 8.8 8.9 9.0 9.1 9.2 9.3 9.4 9.5 9.6 9.7 9.8 9.9 ## [91] 10.0 You can also create vectors by combining individual elements using function c (for combine). combine_vector &lt;- c(2,6,3) combine_vector ## [1] 2 6 3 Type coercion Q: Given what we’ve learned so far, what do you think the following will produce? c(2,6,&#39;3&#39;) ## [1] &quot;2&quot; &quot;6&quot; &quot;3&quot; This is something called type coercion, and it is the source of many surprises and the reason why we need to be aware of the basic data types and how R will interpret them. When R encounters a mix of types (here numeric and character) to be combined into a single vector, it will force them all to be the same type. Not all types can be coerced into another, rather, R has a coercion hierarchy rule. All values are converted to the lowest data type in the hierarchy. R coercion rules: logical -&gt; integer -&gt; numeric -&gt; complex -&gt; character where -&gt; can be read as “are transformed into”. In our case, our 2, &amp; 3 integer values where converted to character. Some other examples: c(&#39;a&#39;, TRUE) ## [1] &quot;a&quot; &quot;TRUE&quot; c(&quot;FALSE&quot;, TRUE) ## [1] &quot;FALSE&quot; &quot;TRUE&quot; c(0, TRUE) ## [1] 0 1 You can try to force coercion against this flow using the as. functions: chars &lt;- c(&#39;0&#39;,&#39;2&#39;,&#39;4&#39;) as.numeric(chars) ## [1] 0 2 4 as.logical(chars) ## [1] NA NA NA as.logical(as.numeric(chars)) ## [1] FALSE TRUE TRUE as.logical(c(0, TRUE)) ## [1] FALSE TRUE as.logical(c(&quot;FALSE&quot;, TRUE)) ## [1] FALSE TRUE as.numeric(c(&quot;FALSE&quot;, TRUE)) ## Warning: NAs introduced by coercion ## [1] NA NA as.numeric(as.logical(c(&quot;FALSE&quot;, TRUE))) ## [1] 0 1 As you can see, some surprising things can happen when R forces one basic data type into another! If your data isn’t the data type you expected, type coercion may well be to blame; make sure everything is the same type in your vectors and your columns of data.frames, or you will get nasty surprises! We can ask a few questions about vectors: sequence_example &lt;- seq(10) head(sequence_example, n=2) ## [1] 1 2 tail(sequence_example, n=4) ## [1] 7 8 9 10 length(sequence_example) ## [1] 10 str(sequence_example) ## int [1:10] 1 2 3 4 5 6 7 8 9 10 The somewhat cryptic output from this command indicates the basic data type found in this vector - in this case int, integer; an indication of the number of things in the vector - actually, the indexes of the vector, in this case [1:10]; and a few examples of what’s actually in the vector - in this case empty character strings. If we similarly do Finally, you can give names to elements in your vector: my_example &lt;- 5:8 names(my_example) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) my_example ## a b c d ## 5 6 7 8 names(my_example) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; Find out more about vectors Matrices Matrices are 2 dimensional arrays The dimensions are defined by the number of rows and columns. We can declare a matrix full of zeros: matrix_example &lt;- matrix(0, ncol=6, nrow=3) matrix_example ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 We can get the dimensions of a matrix (or any array with dimensions &gt; 1). dim(matrix_example) ## [1] 3 6 Lists Lists can store objects of any data type and class Another key data structure is the list. List are the most flexible data structure because each element can hold any object, of any datat type and dimension, including other lists. Create lists using list() or coerce other objects using as.list(). list(1, &quot;a&quot;, TRUE) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;a&quot; ## ## [[3]] ## [1] TRUE as.list(1:4) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 We can name list elements: a_list &lt;- list(title = &quot;Numbers&quot;, numbers = 1:10, data = TRUE ) a_list ## $title ## [1] &quot;Numbers&quot; ## ## $numbers ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $data ## [1] TRUE Lists are a base type: typeof(a_list) ## [1] &quot;list&quot; Data.frames S3, S4 and S6 objects Arrays and lists are all immutable base types. However, there are other types of objects in R. These are S3, S4 &amp; S6 type objects, with S3 being the most common. Such objects have a class attribute (base types can have a class attribute too), enabling class specific functionality, a characteristic of object oriented programming. New classes can be created by users, allowing greater flexibility in the types of data structures available for analyses. Learn more about object types Data.frames The most important S3 object class in R is the data.frame. Data.frames are special types of lists. Data.frames are special types of lists where each element is a vector, each of equal length. So each column of a data.frame contains values of consistent data type but the data type can vary between columns (ie along rows). df &lt;- data.frame(id = 1:3, treatment = c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;), complete = c(TRUE, TRUE, FALSE)) df ## id treatment complete ## 1 1 a TRUE ## 2 2 b TRUE ## 3 3 b FALSE We can check that our data.frame is a list under the hood: typeof(df) ## [1] &quot;list&quot; As an S3 object, it also has a class attribute: class(df) ## [1] &quot;data.frame&quot; And we can check the type of object that it is: sloop::otype(df) ## [1] &quot;S3&quot; Compared to a vector? sloop::otype(1:10) ## [1] &quot;base&quot; We can check the dimensions of a data.frame dim(df) ## [1] 3 3 Get a certain number of rows from the top or bottom head(df, 1) ## id treatment complete ## 1 1 a TRUE tail(df, 1) ## id treatment complete ## 3 3 b FALSE Importantly, we can disply the structure of a data.frame str(df) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ id : int 1 2 3 ## $ treatment: chr &quot;a&quot; &quot;b&quot; &quot;b&quot; ## $ complete : logi TRUE TRUE FALSE A note on factors Note that the default behaviour of data.frame() is to covert character vectors to factors. Factors are another important data structure for handling categorical data, which have particular statistical properties. They can be useful during modeling and plotting but in the interest of time we will not be discuss them further here. You can suppress R default behaviour using: df &lt;- data.frame(id = 1:3, treatment = c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;), complete = c(TRUE, TRUE, FALSE), stringsAsFactors = FALSE) str(df) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ id : int 1 2 3 ## $ treatment: chr &quot;a&quot; &quot;b&quot; &quot;b&quot; ## $ complete : logi TRUE TRUE FALSE Find out more about factors. "],["indexing-and-subsetting.html", "Indexing and subsetting Subsetting vectors Matrix subsetting Subsetting lists Subsetting data.frames Advanced R Cheat Sheet", " Indexing and subsetting R has many powerful subset operators. Mastering them will allow you to easily perform complex operations on any kind of dataset. There many different ways we can subset any kind of object, and three different subsetting operators for the different data structures. Subsetting vectors Let’s start by examining subsetting in the simplest data structure, the vector. Subsetting a vector always returns another vector. x &lt;- 4:7 x ## [1] 4 5 6 7 Subsetting using [ and elements indices Extracting single elements To extract elements of a vector we can use the square bracket operator ([) and the target element index, starting from one: x[1] ## [1] 4 x[4] ## [1] 7 It may look different, but the square brackets operator is a function and means “get me the nth element”. If we ask for an index beyond the length of the vector, R will return a missing value: x[6] ## [1] NA If we ask for the 0th element, we get an empty vector: x[0] ## integer(0) Extracting multiple elements We can also ask for multiple elements at once: x[c(1, 3)] ## [1] 4 6 Or slices of the vector: x[2:4] ## [1] 5 6 7 We can ask for the same element multiple times: x[c(1,1,3)] ## [1] 4 4 6 Excluding and removing elements If we use a negative number as the index of a vector, R will return every element except for the one specified: x[-2] ## [1] 4 6 7 We can skip multiple elements: x[c(-1, -5)] # or x[-c(1,5)] ## [1] 5 6 7 In general, be aware that the result of subsetting using indices could change if the vector is reordered. Subsetting using element names If the vector has a name attribute, we can subset the vector more precisely using the element’s name names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) x[c(&quot;a&quot;, &quot;c&quot;)] ## a c ## 4 6 Subsetting using names in the most robust way to extract elements. The position of various elements can often change when chaining together subsetting operations, but the names will always remain the same! Subsetting using logical vectors We can also use any logical vector to subset: x[c(FALSE, FALSE, TRUE, TRUE)] ## c d ## 6 7 Since comparison operators (e.g. &gt;, &lt;, ==) evaluate to logical vectors, we can also use them to succinctly subset vectors: the following statement gives the same result as the previous one. x[x &gt; 5] ## c d ## 6 7 Breaking it down, this statement first evaluates x &gt; 5, generating a logical vector c(FALSE, FALSE, TRUE, TRUE), and then selects the elements of x corresponding to the TRUE values. We can use == to mimic the previous method of indexing by name (remember you have to use == rather than = for comparisons): x[names(x) == &quot;a&quot;] ## a ## 4 Avoid using == to compare numbers! See function dplyr::near() instead. We also might want to subset using a vector of potential values, that might not necessarily have matches in x. In this case we can use %in% x[names(x) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;e&quot;)] ## a c ## 4 6 Excluding named elements Excluding or removing named elements is a little harder. If we try to skip one named element by negating the string, R complains (slightly obscurely) that it doesn’t know how to take the negative of a string: x[-&quot;a&quot;] ## Error in -&quot;a&quot;: invalid argument to unary operator However, we can use the != (not-equals) operator to construct a logical vector that will do what we want: x[names(x) != &quot;a&quot;] ## b c d ## 5 6 7 Excluding multiple named indices requires a different tactic through. Suppose we want to drop the \"a\" and \"c\" elements, so we try this: x[names(x) != c(&quot;a&quot;,&quot;c&quot;)] ## b c d ## 5 6 7 R did something, but it gave us a warning that we ought to pay attention to - and it apparently gave us the wrong answer (the \"c\" element is still included in the vector)! This happens because we are trying to compare two vectors (names(x) and c(\"a\",\"c\")) and comparison operators are automatically vectorised in such a case. So in effect, R is comparing \"a\" in names(x) to \"a\" in c(\"a\",\"c\") and returning FALSE (ie \"a\" != \"a\" = FALSE), then \"b\" in names(x) to \"c\" in c(\"a\",\"c\") and returning TRUE. What happens with \"c\" in names(x) is R recycles the comparison vector c(\"a\",\"c\") and starts again with \"a\". \"c\" is not equal to \"a\" so \"a\" != \"c\" returns TRUE and the element is kept. On the other hand this works, but only by chance: x[names(x) != c(&quot;a&quot;,&quot;b&quot;)] ## c d ## 6 7 To perform such a subset robustly, we need to combine %in% and !. x[!names(x) %in% c(&quot;a&quot;,&quot;c&quot;)] ## b d ## 5 7 This checks whether names of x take any value of the values in c(\"a\",\"c\"), returning the elements where the condition is TRUE. The ! then negates the selection, returning only the elements whose names are not contained in c(\"a\",\"c\"). Matrix subsetting As matrices are just 2d vectors, all the subsetting operations using the [ can also be applied to matrices. Subsetting using element indices Let’s create a matrix m &lt;- matrix(1:12, ncol=4, nrow=3) m ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Indexing matrices with [ takes two arguments: the first expression is applied to the rows, the second to the columns: Say we want the 2 and 3rd rows of the last and first column (in that order) of our matrix. We can use all the subsetting we learned for vectors and apply them to each dimension of our matrix. m[2:3, c(4,1)] ## [,1] [,2] ## [1,] 11 2 ## [2,] 12 3 Subsetting whole rows or columns We can leave the first or second arguments blank to retrieve all the rows or columns respectively: m[, c(2,3)] ## [,1] [,2] ## [1,] 4 7 ## [2,] 5 8 ## [3,] 6 9 m[c(2,3),] ## [,1] [,2] [,3] [,4] ## [1,] 2 5 8 11 ## [2,] 3 6 9 12 If we only access one row or column, R will automatically convert the result to a vector: m[3,] ## [1] 3 6 9 12 If we want to keep the output as a matrix, we need to specify a third argument; drop = FALSE: m[3, , drop=FALSE] ## [,1] [,2] [,3] [,4] ## [1,] 3 6 9 12 Tip: Higher dimensional arrays When dealing with multi-dimensional arrays, each argument to [ corresponds to a dimension. For example, a 3D array, the first three arguments correspond to the rows, columns, and depth dimension. Subsetting lists There are three functions used to subset lists and extract individual elements: [, [[, and $. Subsetting list elements Using [ will always return a list. If you want to subset a list, but not extract an element, then you will likely use [. xlist &lt;- list(a = &quot;ACCE DTP&quot;, b = 1:10, data = head(iris)) Subsetting by element indices As with vectors, we can use element indices and [ to subset lists. xlist[1] ## $a ## [1] &quot;ACCE DTP&quot; This returns a list with one element. We can use multiple indices to subset multiple list elements: xlist[1:2] ## $a ## [1] &quot;ACCE DTP&quot; ## ## $b ## [1] 1 2 3 4 5 6 7 8 9 10 Subsetting by name We can also use names: xlist[c(&quot;a&quot;, &quot;b&quot;)] ## $a ## [1] &quot;ACCE DTP&quot; ## ## $b ## [1] 1 2 3 4 5 6 7 8 9 10 It is accessing the list as if it were a vector and returning a list. Comparison operations involving the contents of list elements however won’t work as they are not accessible at the level of [ indexing. Extracting individual elements Extracting individual elements allow us to access the objects contained in a list, which can be any type of object. Hence the result depends on the object each element contains. To extract individual elements of a list, we use the double-square bracket function: [[. Extracting by element index Again we can use element indices to extract the object contained in an element. xlist[[2]] ## [1] 1 2 3 4 5 6 7 8 9 10 Notice that now the result is a vector, not a list, which is what the second element contained. You can’t extract more than one element at once: xlist[[1:2]] ## Error in xlist[[1:2]]: subscript out of bounds Nor use it to skip elements: xlist[[-1]] ## Error in xlist[[-1]]: invalid negative subscript in get1index &lt;real&gt; Extracting by element name We can however use single names to extract elements: xlist[[&quot;a&quot;]] ## [1] &quot;ACCE DTP&quot; The $ operator The $ operator is a shorthand way for extracting single elements by name: xlist$data ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa List subsetting challenge Given the following list: xlist &lt;- list(a = &quot;ACCE DTP&quot;, b = 1:10, data = head(iris)) and using your knowledge of both list and vector subsetting, extract the number 2 from xlist. Hint: the number 2 is contained within the “b” item in the list. Solution Subsetting data.frames Data frames are lists underneath the hood, so similar rules apply subsetting rules apply. However they are also two dimensional objects. Subsetting data.frames as a list Using [ to subset Using the [ operator with one argument will act the same way as for lists, where each list element corresponds to a column. The resulting object will be a data.frame: trees[1] ## Girth ## 1 8.3 ## 2 8.6 ## 3 8.8 ## 4 10.5 ## 5 10.7 ## 6 10.8 ## 7 11.0 ## 8 11.0 ## 9 11.1 ## 10 11.2 ## 11 11.3 ## 12 11.4 ## 13 11.4 ## 14 11.7 ## 15 12.0 ## 16 12.9 ## 17 12.9 ## 18 13.3 ## 19 13.7 ## 20 13.8 ## 21 14.0 ## 22 14.2 ## 23 14.5 ## 24 16.0 ## 25 16.3 ## 26 17.3 ## 27 17.5 ## 28 17.9 ## 29 18.0 ## 30 18.0 ## 31 20.6 trees[&quot;Girth&quot;] ## Girth ## 1 8.3 ## 2 8.6 ## 3 8.8 ## 4 10.5 ## 5 10.7 ## 6 10.8 ## 7 11.0 ## 8 11.0 ## 9 11.1 ## 10 11.2 ## 11 11.3 ## 12 11.4 ## 13 11.4 ## 14 11.7 ## 15 12.0 ## 16 12.9 ## 17 12.9 ## 18 13.3 ## 19 13.7 ## 20 13.8 ## 21 14.0 ## 22 14.2 ## 23 14.5 ## 24 16.0 ## 25 16.3 ## 26 17.3 ## 27 17.5 ## 28 17.9 ## 29 18.0 ## 30 18.0 ## 31 20.6 Using [[ to extract Similarly, [[ will act to extract a single column as a vector: trees[[1]] ## [1] 8.3 8.6 8.8 10.5 10.7 10.8 11.0 11.0 11.1 11.2 11.3 11.4 11.4 11.7 12.0 ## [16] 12.9 12.9 13.3 13.7 13.8 14.0 14.2 14.5 16.0 16.3 17.3 17.5 17.9 18.0 18.0 ## [31] 20.6 trees[[&quot;Girth&quot;]] ## [1] 8.3 8.6 8.8 10.5 10.7 10.8 11.0 11.0 11.1 11.2 11.3 11.4 11.4 11.7 12.0 ## [16] 12.9 12.9 13.3 13.7 13.8 14.0 14.2 14.5 16.0 16.3 17.3 17.5 17.9 18.0 18.0 ## [31] 20.6 And $ provides a convenient shorthand to extract columns by name: trees$Girth ## [1] 8.3 8.6 8.8 10.5 10.7 10.8 11.0 11.0 11.1 11.2 11.3 11.4 11.4 11.7 12.0 ## [16] 12.9 12.9 13.3 13.7 13.8 14.0 14.2 14.5 16.0 16.3 17.3 17.5 17.9 18.0 18.0 ## [31] 20.6 Subsetting data.frames as a matrix With two arguments, [ behaves the same way as for matrices: trees[1:5, c(&quot;Girth&quot;, &quot;Volume&quot;)] ## Girth Volume ## 1 8.3 10.3 ## 2 8.6 10.3 ## 3 8.8 10.2 ## 4 10.5 16.4 ## 5 10.7 18.8 If we subset a single row, the result will be a data.frame (because the elements are mixed types): trees[3,] ## Girth Height Volume ## 3 8.8 63 10.2 But for a single column the result will be a vector. trees[, &quot;Girth&quot;] ## [1] 8.3 8.6 8.8 10.5 10.7 10.8 11.0 11.0 11.1 11.2 11.3 11.4 11.4 11.7 12.0 ## [16] 12.9 12.9 13.3 13.7 13.8 14.0 14.2 14.5 16.0 16.3 17.3 17.5 17.9 18.0 18.0 ## [31] 20.6 This can be changed with the third argument, drop = FALSE). trees[, &quot;Girth&quot;, drop=FALSE] ## Girth ## 1 8.3 ## 2 8.6 ## 3 8.8 ## 4 10.5 ## 5 10.7 ## 6 10.8 ## 7 11.0 ## 8 11.0 ## 9 11.1 ## 10 11.2 ## 11 11.3 ## 12 11.4 ## 13 11.4 ## 14 11.7 ## 15 12.0 ## 16 12.9 ## 17 12.9 ## 18 13.3 ## 19 13.7 ## 20 13.8 ## 21 14.0 ## 22 14.2 ## 23 14.5 ## 24 16.0 ## 25 16.3 ## 26 17.3 ## 27 17.5 ## 28 17.9 ## 29 18.0 ## 30 18.0 ## 31 20.6 Advanced R Cheat Sheet Figure 1: Environments, data Structures, Functions, Subsetting and more by Arianne Colton and Sean Chen "],["the-tidyverse-way.html", "The tidyverse way Intro to the tidyverse tibbles Subsetting tibbles The pipe operator %&gt;% Advanced R Cheat Sheet", " The tidyverse way Intro to the tidyverse The Tidyverse is a coherent system of packages for data manipulation, exploration and visualization that share a common design philosophy. Advantages of the tidyverse Consistent functions. Workflow coverage. A parsimonious approach to the development of data science tools. Tidyverse Principles tibbles as main data structures. Tidy data where rows are sigle observations and columns the variables observed. Piping the outputs of tidyverse functions as inputs to subsequent functions. install.packages(c(&quot;tibble&quot;, &quot;dplyr&quot;)) tibbles tibbles are one of the unifying features of the tidyverse, and are the tidyverse version of a data.frame (I will use them interchangeably in the rest of the text). Features Better printing behaviour. Never coerces characters to factors. More robust error handling. Creating tibbles Coercing data.frames You can coerce a data.frame to a tibble tree_tbl &lt;- tibble::as_tibble(trees) tree_tbl ## # A tibble: 31 x 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 ## 7 11 66 15.6 ## 8 11 75 18.2 ## 9 11.1 80 22.6 ## 10 11.2 75 19.9 ## # … with 21 more rows As you can see, printing tibbles is much tidier and informative and designed so that you don’t accidentally overwhelm your console when you print large data.frames. Creating new tibbles You can create a new tibble from individual vectors with tibble(). tibble() will automatically recycle inputs of length 1, and allows you to refer to variables that you just created: tibble::tibble( x = 1:5, y = 1, z = x ^ 2 + y ) ## # A tibble: 5 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 1 5 ## 3 3 1 10 ## 4 4 1 17 ## 5 5 1 26 Subsetting tibbles Base R subsetting We can use all the tools we learnt to subset data.frames to subset tibbles. Subsetting using the tidyverse You can also subset tibbles using tidyverse functions from package dplyr. dplyr verbs are inspired by SQL vocabulary and designed to be more intuitive. library(dplyr) The first argument of the main dplyr functions is a tibble (or data.frame) Filtering rows with filter() filter() allows us to subset observations (rows) based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. filter(tree_tbl, Girth &gt; 14) ## # A tibble: 10 x 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14.2 80 31.7 ## 2 14.5 74 36.3 ## 3 16 72 38.3 ## 4 16.3 77 42.6 ## 5 17.3 81 55.4 ## 6 17.5 82 55.7 ## 7 17.9 80 58.3 ## 8 18 80 51.5 ## 9 18 80 51 ## 10 20.6 87 77 dplyr executes the filtering operation by generating a logical vector and returns a new tibble of the rows that match the filtering conditions. You can therefore use any logical operators we learnt using [. Slicing rows with slice() Using slice() is similar to subsetting using element indices in that we provide element indices to select rows. slice(tree_tbl, 2) ## # A tibble: 1 x 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.6 65 10.3 slice(tree_tbl, 2:5) ## # A tibble: 4 x 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.6 65 10.3 ## 2 8.8 63 10.2 ## 3 10.5 72 16.4 ## 4 10.7 81 18.8 Selecting columns with select() select() allows us to subset columns in tibbles using operations based on the names of the variables. In dplyr we use unquoted column names (ie Volume rather than \"Volume\"). select(tree_tbl, Height, Volume) ## # A tibble: 31 x 2 ## Height Volume ## &lt;dbl&gt; &lt;dbl&gt; ## 1 70 10.3 ## 2 65 10.3 ## 3 63 10.2 ## 4 72 16.4 ## 5 81 18.8 ## 6 83 19.7 ## 7 66 15.6 ## 8 75 18.2 ## 9 80 22.6 ## 10 75 19.9 ## # … with 21 more rows Behind the scenes, select matches any variable arguments to column names creating a vector of column indices. This is then used to subset the tibble. As such we can create ranges of variables using their names and : select(tree_tbl, Height:Volume) ## # A tibble: 31 x 2 ## Height Volume ## &lt;dbl&gt; &lt;dbl&gt; ## 1 70 10.3 ## 2 65 10.3 ## 3 63 10.2 ## 4 72 16.4 ## 5 81 18.8 ## 6 83 19.7 ## 7 66 15.6 ## 8 75 18.2 ## 9 80 22.6 ## 10 75 19.9 ## # … with 21 more rows There’s also a number of helper functions to make selections easier. For example, we can use one_of() to provide a character vector of column names to select. select(tree_tbl, one_of(c(&quot;Height&quot;, &quot;Volume&quot;))) ## # A tibble: 31 x 2 ## Height Volume ## &lt;dbl&gt; &lt;dbl&gt; ## 1 70 10.3 ## 2 65 10.3 ## 3 63 10.2 ## 4 72 16.4 ## 5 81 18.8 ## 6 83 19.7 ## 7 66 15.6 ## 8 75 18.2 ## 9 80 22.6 ## 10 75 19.9 ## # … with 21 more rows Find out more about dplyr helper functions The pipe operator %&gt;% Pipes are a powerful tool for clearly expressing a sequence of multiple operations. They help us write code in a way that is easier to read and understand. They also remove the need for creating intermediate objects. Pipes take the output of the evaluation of the preceeding code and pipe it as the first argument to the subsequent expression. Suppose we want to get the first two rows and only columns Girth and Volume. We can chain the two operations together using the pipe. tree_tbl %&gt;% select(Girth, Volume) %&gt;% slice(1:2) ## # A tibble: 2 x 2 ## Girth Volume ## &lt;dbl&gt; &lt;dbl&gt; ## 1 8.3 10.3 ## 2 8.6 10.3 This is form is very understandable because it focusses on intuitive verbs, not nouns. You can read this series of function compositions like it’s a set of imperative actions. As mentioned, the default behaviour of the pipe is to pipe objects as the first argument of the next expression. However, we can pipe the object into a different argument using the . operator. tree_tbl %&gt;% lm(Girth ~ Height, data = .) ## ## Call: ## lm(formula = Girth ~ Height, data = .) ## ## Coefficients: ## (Intercept) Height ## -6.1884 0.2557 Note: The pipe, %&gt;%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %&gt;% for you automatically, so you don’t usually load magrittr explicitly. Advanced R Cheat Sheet "],["rdm-intro-view.html", "Data management Basics", " Data management Basics View Slides "],["proj-management.html", "(PART) Project Management", " (PART) Project Management "],["projects-in-rstudio.html", "Projects in Rstudio General Project Organisation Rstudio Projects Rstudio Cloud projects", " Projects in Rstudio Rstudio projects are a convenient way to manage research projects, providing the scaffolding for robust, self-contained and portable work. General Project Organisation Good project layout helps ensure: Integrity of data Portability of the project work is easy to revisit after a break and onboard new collaborators supports tool building which takes advantage of the shared structure. Principles Everything required is contained in the project or sourced automatically. Use paths relative to the project root directory. Separate data, methods, and output, while making the relationship between them clear. Document the contents of your project and how to use them. Use R package development and community conventions. Do not manually edit raw data. Keep a clean pipeline of data processing from raw to analytical. Incorporate checks to ensure correct processing and analysing. Rstudio Projects Rstudio projects are a convenient way to manage research projects, providing the scaffolding for self contained and portable work. Features Self contained and portable Clean environment on load Working directory and files tab set to project root Rstudio Cloud projects For the rest of the course, we will be working in a project where I have already set up the computational environment. This will save a lot of time by avoiding having to run a long installation scrip during the class or debug individual installation problems. Launch Rstudio Cloud project To access this project, please click on the supplied project link This should drop you into the ACCE DTP Reproducible research in R Workspace. If not, navigate to the space from the menu on the top left. In there you will find a project called wood-survey. Click on + Copy to create your own copy of the project. Once the project has been created you need to give it a name. Stick to wood-survey You now have your own version of the project. Creating projects locally To create a new project locally in Rstudio, you can either use File &gt; New Project &gt; New Directory, or in the console (you only need to run this once, so you don’t want it to be part of a repeatable script) run: usethis::create_project(&quot;~/Desktop/wood-survey&quot;) In general, do not use such hard code paths in repeatable scripts. They might not work across operating systems and are unlikely to generalise across someone elses file system. You will also need to run the install.R script. See the setup instructions for more details. "],["filenaming-view.html", "Good File Naming", " Good File Naming View Slides "],["path-proj-str.html", "Paths and Project structure Project Data Setting up a data-raw/ directory NEON Data Paths Basic checks", " Paths and Project structure Project Data We’re in our new project so the first thing we need to do is get the data we’ll be working with. This is a common start to any project where you start with a few data files, These might be generated through your data, given by others or published data products and you might need to clean, wrangle and combine them together to perform your analysis. Q: Where should I save my raw data files? conventions: Data management Store raw data in data-raw/: raw inputs to any pre-processing, read only. Keep any processing scripts in the same folder Whether and where you publish data depends on size and copyright considerations. Store analytical data in data/: any clean, processed data that is used as the input to the analysis. Should be published along side analysis. Setting up a data-raw/ directory We start by creating a data-raw directory in the root of our project. We can use usethis function usethis::use_data_raw(). This creates the data-raw directory and an .R script within where we can save code that turns raw data into analytical data in the data/ folder. We can supply a name for the analytical dataset we’ll be creating in our script which automatically names the .R script for easy provenance tracking. In this case, we’ll be calling it individual.csv so let’s use \"individual\" for our name. usethis::use_data_raw(name = &quot;individual&quot;) ✔ Setting active project to &#39;/Users/Anna/Desktop/wood-survey&#39; ✔ Creating &#39;data-raw/&#39; ✔ Adding &#39;^data-raw$&#39; to &#39;.Rbuildignore&#39; ✔ Writing &#39;data-raw/individual.R&#39; ● Modify &#39;data-raw/individual.R&#39; ● Finish the data preparation script in &#39;data-raw/individual.R&#39; ● Use `usethis::use_data()` to add prepared data to package The data-raw/individual.R script created contains: ## code to prepare `individual` dataset goes here usethis::use_data(&quot;individual&quot;) We will use this file to perform the necessary preprocessing on our raw data. Download data Now that we’ve got our data-raw folder, let’s download our data into it using function usethis::use_course() and supplying it with the url to the materials repository (bit.ly/wood-survey-data) and the path to the directory we want the materials saved into (\"data-raw\"). usethis::use_course(&quot;bit.ly/wood-survey-data&quot;, destdir = &quot;data-raw&quot;) ✔ Downloading from &#39;https://github.com/annakrystalli/wood-survey-data/archive/master.zip&#39; Downloaded: 0.03 MB ✔ Download stored in &#39;data-raw/wood-survey-data-master.zip&#39; ✔ Unpacking ZIP file into &#39;wood-survey-data-master/&#39; (13 files extracted) Shall we delete the ZIP file (&#39;wood-survey-data-master.zip&#39;)? 1: Negative 2: Absolutely not 3: I agree Selection: 3 ✔ Deleting &#39;wood-survey-data-master.zip&#39; ✔ Opening &#39;wood-survey-data-master/&#39; in the file manager NEON Data The downloaded folder contains a subset of data from the NEON Woody plant vegetation survey. Citation: National Ecological Observatory Network. 2020. Data Products: DP1.10098.001. Provisional data downloaded from http://data.neonscience.org on 2020-01-15. Battelle, Boulder, CO, USA This data product was downloaded from the NEON data portal and contains quality-controlled data from in-situ measurements of live and standing dead woody individuals and shrub groups, from all terrestrial NEON sites with qualifying woody vegetation. Surveys of each site are completed once every 3 years. Let’s have a look at what we’ve downloaded: . ├── R ├── data-raw │ ├── individual.R │ └── wood-survey-data-master │ ├── NEON_vst_variables.csv │ ├── README.md │ ├── individual [67 entries exceeds filelimit, not opening dir] │ ├── methods │ │ ├── NEON.DOC.000914vB.pdf │ │ ├── NEON.DOC.000987vH.pdf │ │ └── NEON_vegStructure_userGuide_vA.pdf │ ├── vst_mappingandtagging.csv │ └── vst_perplotperyear.csv └── wood-survey.Rproj The important files for the analysis we want to perform are ├── individual [67 entries exceeds filelimit, not opening dir] ├── vst_mappingandtagging.csv └── vst_perplotperyear.csv vst_perplotperyear: Plot level metadata, including plot geolocation, one record per plotID per eventID, describe the presence/absence of woody growth forms sampling area utilized for each growth form. vst_mappingandtagging: Mapping, identifying and tagging of individual stems for remeasurement one record per individualID, data invariant through time, including tagID, taxonID and mapped location. Records can be linked to vst_perplotperyear via the plotID and eventID fields. vst_apparentindividual: Biomass and productivity measurements of apparent individuals. may contain multiple records per individuals includes growth form, structure currently in separate files contained in individual/ may be linked vst_mappingandtagging records via individualID vst_perplotperyear via the plotID and eventID fields. As our first challenge, we are going to combined all the files in individual/ into a single analytical data file! Paths First let’s investigate our data. We want to access the files so we need to give R paths in order to load the data. We can work with the file system programmatically through R. here: Use here::here() to create paths relative to the project root directory. portable independent of the where code is evaluated or saved. Let’s start by creating a path to the downloaded data directory using here. raw_data_path &lt;- here::here(&quot;data-raw&quot;, &quot;wood-survey-data-master&quot;) raw_data_path ## [1] &quot;/Users/runner/work/rrresearchACCE20/rrresearchACCE20/data-raw/wood-survey-data-master&quot; &quot;/Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master&quot; We can use raw_data_path as our basis for specifying paths to files within it. There’s a number of ways we can do this in R but I wanted to introduce you to package fs. It has a nice interface and extensive functionality. fs::path(raw_data_path, &quot;individual&quot;) ## /Users/runner/work/rrresearchACCE20/rrresearchACCE20/data-raw/wood-survey-data-master/individual /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual Let’s now use function dir_ls to get a character vector of paths to all the individual files in directory individual. individual_paths &lt;- fs::dir_ls(fs::path(raw_data_path, &quot;individual&quot;)) head(individual_paths) ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2015-08.basic.20190806T172340Z.csv ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2015-09.basic.20190806T144119Z.csv ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2016-08.basic.20190806T143255Z.csv ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2016-09.basic.20190806T143433Z.csv ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2016-10.basic.20190806T144133Z.csv ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2017-07.basic.20190806T144111Z.csv We can check how many files we’ve got: length(individual_paths) ## [1] 67 We can now use this vector of paths to read in files. Let’s read the first file in and check it out. We use function read_csv() from readr package which reads comma delimited files into tibbles. indiv_df &lt;- readr::read_csv(individual_paths[1]) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## uid = col_character(), ## namedLocation = col_character(), ## date = col_date(format = &quot;&quot;), ## eventID = col_character(), ## domainID = col_character(), ## siteID = col_character(), ## plotID = col_character(), ## individualID = col_character(), ## growthForm = col_character(), ## stemDiameter = col_double(), ## measurementHeight = col_double(), ## height = col_double() ## ) indiv_df ## # A tibble: 376 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 366 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; Run ?read_delim for more details on reading in tabular data. Basic checks Let’s perform some of the basic checks we learnt before we proceed. View(indiv_df) names(indiv_df) ## [1] &quot;uid&quot; &quot;namedLocation&quot; &quot;date&quot; ## [4] &quot;eventID&quot; &quot;domainID&quot; &quot;siteID&quot; ## [7] &quot;plotID&quot; &quot;individualID&quot; &quot;growthForm&quot; ## [10] &quot;stemDiameter&quot; &quot;measurementHeight&quot; &quot;height&quot; ## spec_tbl_df[,12] [376 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ uid : chr [1:376] &quot;a36a162d-ed1f-4f80-ae45-88e973855c68&quot; &quot;68dc7adf-48e2-4f7a-9272-9a468fde6d55&quot; &quot;a8951ab9-4462-48dd-ab9e-7b89e24f2e03&quot; &quot;eb348eaf-3969-46a4-ac3b-523c3548efeb&quot; ... ## $ namedLocation : chr [1:376] &quot;BART_037.basePlot.vst&quot; &quot;BART_037.basePlot.vst&quot; &quot;BART_044.basePlot.vst&quot; &quot;BART_044.basePlot.vst&quot; ... ## $ date : Date[1:376], format: &quot;2015-08-26&quot; &quot;2015-08-26&quot; ... ## $ eventID : chr [1:376] &quot;vst_BART_2015&quot; &quot;vst_BART_2015&quot; &quot;vst_BART_2015&quot; &quot;vst_BART_2015&quot; ... ## $ domainID : chr [1:376] &quot;D01&quot; &quot;D01&quot; &quot;D01&quot; &quot;D01&quot; ... ## $ siteID : chr [1:376] &quot;BART&quot; &quot;BART&quot; &quot;BART&quot; &quot;BART&quot; ... ## $ plotID : chr [1:376] &quot;BART_037&quot; &quot;BART_037&quot; &quot;BART_044&quot; &quot;BART_044&quot; ... ## $ individualID : chr [1:376] &quot;NEON.PLA.D01.BART.05285&quot; &quot;NEON.PLA.D01.BART.05279&quot; &quot;NEON.PLA.D01.BART.05419&quot; &quot;NEON.PLA.D01.BART.05092&quot; ... ## $ growthForm : chr [1:376] &quot;single bole tree&quot; &quot;single bole tree&quot; &quot;single bole tree&quot; &quot;single bole tree&quot; ... ## $ stemDiameter : num [1:376] 17.1 13.7 12.3 12.1 29.2 12.1 23.4 39.5 10 10.6 ... ## $ measurementHeight: num [1:376] 130 130 130 130 130 130 130 130 130 130 ... ## $ height : num [1:376] 15.2 9.8 7.7 15.2 16.7 10.6 18.4 19 5.7 8.7 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. uid = col_character(), ## .. namedLocation = col_character(), ## .. date = col_date(format = &quot;&quot;), ## .. eventID = col_character(), ## .. domainID = col_character(), ## .. siteID = col_character(), ## .. plotID = col_character(), ## .. individualID = col_character(), ## .. growthForm = col_character(), ## .. stemDiameter = col_double(), ## .. measurementHeight = col_double(), ## .. height = col_double() ## .. ) ## uid namedLocation date eventID ## Length:376 Length:376 Min. :2015-08-26 Length:376 ## Class :character Class :character 1st Qu.:2015-08-27 Class :character ## Mode :character Mode :character Median :2015-08-27 Mode :character ## Mean :2015-08-27 ## 3rd Qu.:2015-08-31 ## Max. :2015-08-31 ## domainID siteID plotID individualID ## Length:376 Length:376 Length:376 Length:376 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## growthForm stemDiameter measurementHeight height ## Length:376 Min. : 2.00 Min. : 10.0 Min. : 0.50 ## Class :character 1st Qu.:13.90 1st Qu.:130.0 1st Qu.:10.60 ## Mode :character Median :20.20 Median :130.0 Median :14.30 ## Mean :23.01 Mean :129.5 Mean :13.91 ## 3rd Qu.:29.55 3rd Qu.:130.0 3rd Qu.:17.23 ## Max. :69.90 Max. :130.0 Max. :30.20 Everything looks good. So let’s move onto the next step of reading in all the files and combining them together. To do this, we’ll examine the principles of Iteration. "],["process.html", "(PART) Data Munging", " (PART) Data Munging "],["iteration.html", "Iteration Iterating using loops Functional programming Writing out our tibble to disk", " Iteration Let’s say we want to repeat a process multiple times, iterating over a number of inputs. In this case we want to load every file in /data-raw/wood-survey-data-master/individual/. We have a few options for how to approach this problem. In R there are two paradigms for iteration: imperative iterations: (for and while loops) great place to start because they make iteration very explicit. quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. functional programming: using functions to iterate over other functions. focus is on the operation being performed rather than the bookkeeping. can be more elegant and succinct. Iterating using loops Simple loop Here’s an example of a simple loop. During each iteration, it prints a message to the console, reporting the value of i. for(i in 1:10){ print(paste0(&quot;i is &quot;, i)) } ## [1] &quot;i is 1&quot; ## [1] &quot;i is 2&quot; ## [1] &quot;i is 3&quot; ## [1] &quot;i is 4&quot; ## [1] &quot;i is 5&quot; ## [1] &quot;i is 6&quot; ## [1] &quot;i is 7&quot; ## [1] &quot;i is 8&quot; ## [1] &quot;i is 9&quot; ## [1] &quot;i is 10&quot; The loop iterates over the vector of values supplied in 1:10, sequentially assigning a new value to variable i each iteration. i is therefore the varying input and everything else in the code stays the same during each iteration. Loops in practice Reading in multiple files Let’s now apply a loop to read in all 67 files at once. We have the file paths in our individual_paths vector. This is the input we want to iterate over. We can use a for loop to supply each path as the file argument in readr::read_csv(). Storing loop outputs The previous loop we saw didn’t generate any new objects, it just printed output to the console. We, however, need to store the output of each iteration (the tibble we’ve just read in). It’s important for efficiency to allocate sufficient space for the output before starting a for loop. Growing the for loop at each iteration, using c() for example, will be very slow. Let’s create an output vector to store the tibbles containing the read in data. We want it to be a list because we’ll be storing heterogeneous objects (tibbles) in each element. indiv_df_list &lt;- vector(&quot;list&quot;, length(individual_paths)) head(indiv_df_list) ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL We’ve used the length() of the input to specify the size of our output list so each path gets an output element. Looping over indices Next, we need a sequence of indices as long as the input vector (individual_paths). We can use seq_along() to create our index vector: seq_along(individual_paths) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 Now we’re ready to write our for loop. for(i in seq_along(individual_paths)){ indiv_df_list[[i]] &lt;- readr::read_csv(individual_paths[[i]]) } At each step of the iteration, the file specified in the ith element of individual_paths is read in and assigned to th ith element of our output list. We can extract individual tibbles using [[ sub-setting to inspect: indiv_df_list[[1]] ## # A tibble: 376 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 366 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; indiv_df_list[[2]] ## # A tibble: 714 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fb75a8… BART_036.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 30a7c7… BART_046.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 789d03… BART_072.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 0e4fb3… BART_072.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 cb0e45… BART_036.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 5fc5cf… BART_072.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 5d15fa… BART_046.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 d27a1b… BART_036.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 d5f9ab… BART_036.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 e52c3b… BART_036.base… 2015-09-01 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 704 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; We can also inspect the contents of our output list interactively using View() Looping over objects We can also loop over objects instead of indices. indiv_df_list &lt;- vector(&quot;list&quot;, length(individual_paths)) names(indiv_df_list) &lt;- basename(individual_paths) for(path in individual_paths){ indiv_df_list[[basename(path)]] &lt;- readr::read_csv(path) } In this case, we supply the paths themselves as the input to our loop and these are passed as-is to read_csv(). This time we don’t have our element indices to index the elements of the output list each tibble should be stored in. To get around this we assign names to each element and index the output list by name. I’ve chosen to use the basename (actual file name) of each path as a name, which I can get through basename(). individual_paths[1] basename(individual_paths[1]) ## /Users/Anna/Desktop/wood-survey/data-raw/wood-survey-data-master/individual/NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2015-08.basic.20190806T172340Z.csv ## [1] &quot;NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2015-08.basic.20190806T172340Z.csv&quot; Collapsing our output list into a single tibble. Now we’ve got our list of tibbles, we want to collapse or “reduce” our output list into a single tibble. There are a number of ways to do this in R. Base R One first approach we might think of is to use base function rbind(). This takes any number of tibbles as arguments and binds them all together. rbind(indiv_df_list) %&gt;% head() ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2015-08.basic.20190806T172340Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2015-09.basic.20190806T144119Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2016-08.basic.20190806T143255Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2016-09.basic.20190806T143433Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2016-10.basic.20190806T144133Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2017-07.basic.20190806T144111Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2017-08.basic.20190806T143426Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2017-09.basic.20190806T143740Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2018-08.basic.20190806T143026Z.csv ## indiv_df_list List,12 ## NEON.D01.BART.DP1.10098.001.vst_apparentindividual.2018-09.basic.20190806T144743Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2015-08.basic.20190806T155155Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2015-09.basic.20190806T155228Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2015-10.basic.20190806T160029Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2015-11.basic.20190806T155340Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2016-07.basic.20190806T154424Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2016-08.basic.20190806T155619Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2016-09.basic.20190806T155751Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2016-10.basic.20190806T154902Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2017-07.basic.20190806T161731Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2017-08.basic.20190806T155239Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2017-09.basic.20190806T154054Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2017-10.basic.20190806T154917Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2018-09.basic.20190806T154756Z.csv ## indiv_df_list List,12 ## NEON.D01.HARV.DP1.10098.001.vst_apparentindividual.2018-10.basic.20190904T080421Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2015-09.basic.20190806T180623Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2015-10.basic.20190806T180501Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2016-09.basic.20190806T180452Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2016-11.basic.20190806T162810Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2017-09.basic.20190806T180226Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2017-10.basic.20190806T162804Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2018-09.basic.20190806T162758Z.csv ## indiv_df_list List,12 ## NEON.D02.BLAN.DP1.10098.001.vst_apparentindividual.2018-11.basic.20190930T153245Z.csv ## indiv_df_list List,12 ## NEON.D03.DSNY.DP1.10098.001.vst_apparentindividual.2018-01.basic.20190806T170456Z.csv ## indiv_df_list List,12 ## NEON.D03.DSNY.DP1.10098.001.vst_apparentindividual.2018-05.basic.20190806T165614Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2015-08.basic.20190806T155333Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-02.basic.20190806T151351Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-03.basic.20190806T151416Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-04.basic.20190806T151437Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-05.basic.20190806T154733Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-06.basic.20190806T155301Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-07.basic.20190806T155324Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-08.basic.20190806T153300Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-09.basic.20190806T151857Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-10.basic.20190806T154351Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2016-11.basic.20190806T152215Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2017-03.basic.20190806T152514Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2017-04.basic.20190806T154915Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2017-12.basic.20190806T164409Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2018-01.basic.20190806T150606Z.csv ## indiv_df_list List,12 ## NEON.D04.GUAN.DP1.10098.001.vst_apparentindividual.2018-02.basic.20190806T150635Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2015-05.basic.20190806T151458Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2015-06.basic.20190806T151516Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2016-10.basic.20190806T154811Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2016-11.basic.20190806T154932Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2017-10.basic.20190806T155315Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2017-11.basic.20190806T164155Z.csv ## indiv_df_list List,12 ## NEON.D07.GRSM.DP1.10098.001.vst_apparentindividual.2018-11.basic.20190930T154643Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2015-06.basic.20190806T173627Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2015-07.basic.20190806T165116Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2015-09.basic.20190806T160924Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2016-09.basic.20190806T161107Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2016-10.basic.20190806T155600Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2017-10.basic.20190806T161504Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2017-11.basic.20190806T165044Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2018-10.basic.20190904T074322Z.csv ## indiv_df_list List,12 ## NEON.D08.DELA.DP1.10098.001.vst_apparentindividual.2018-11.basic.20190930T162311Z.csv ## indiv_df_list List,12 ## NEON.D09.DCFS.DP1.10098.001.vst_apparentindividual.2015-09.basic.20190806T161704Z.csv ## indiv_df_list List,12 Hmm, that doesn’t seem to have done what we want. That’s because rbind expects multiple tibbles as inputs and we’re giving it a single list. We somehow want to extract the contents of each element of indiv_df_list and pass them all to rbind. For this we can use do.call.do.call takes a function or the name of a function we want to execute as it’s first argument, what. The second argument of do.call, args is a list of arguments we want to pass to the function specified in what. When do.call is executed, it extracts the elements of args and passes them as arguments to what. do.call(what = &quot;rbind&quot;, args = indiv_df_list) ## # A tibble: 14,961 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## * &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; Success! Tidyverse There are also ways to do this using the tidyverse. purrr::reduce reduce from package purrr combines the elements of a vector or list into a single object according to the function supplied to .f. purrr::reduce(indiv_df_list, .f = rbind) ## # A tibble: 14,961 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; dplyr::bind_rows bind_rows offers a shortcut to reducing a list of tibbles. dplyr::bind_rows(indiv_df_list) ## # A tibble: 14,961 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; Functional programming Loops are an important basic concept in programming. However another approach available in R is functional programming which vectorises a function or pipe of functions over given input(s). We’ve actually just been using functional programming with do.call and reduce. This idea of passing a function to another function is one of the behaviours that makes R a functional programming language and is extremely powerful. It allows us to: use functions rather than for loops to perform iteration over other functions. wrap the code we want to iterate over in custom functions. This iin turn allows us to replace many for loops with code that is both more succinct and easier to read. In base R there is a family of apply functions (lapply, vapply, sapply, apply, mapply). These are handy to know if want to write workflows or software that are low on dependencies. However, I prefer using the functions in tidyverse package purrr. Iterating using purrr In the tidyverse such functionality is provided by package purrr, which provides a complete and consistent set of tools for working with functions and vectors of inputs. The first thing we might try is to replace our for loop with a function. map The basic purrr function is map() and it allows us to pass the elements of an input vector or list to a single argument of a function we want to repeat. It also has a handy shortcut for specifying the argument to pass the input object to. indiv_df_list &lt;- purrr::map(individual_paths, ~readr::read_csv(file = .x)) The first argument to map is the input vector of paths we want to iterate over. The next argument is a formula specifying the function we want to repeat as well as which argument the input is passed to. Here we’re saying that we want to repeatedly run read_csv and we indicate the argument we want the input passed to (file) by .x. Note as well the ~ notation before the function definition which is shorthand for .f =. map_df Just like our loop, map returns an output list. class(indiv_df_list) ## [1] &quot;list&quot; We would therefore need to combine them together in another step. However, one of the great things about purrr functions is that you can specify what you expect the output of the mapped function to be. THere are functions that take advantage of that knowledge and bind or format the outputs appropriately. Because we know the output of read_csv() is a tibble, we can use map_df() instead of map(). individual &lt;- purrr::map_df(individual_paths, ~readr::read_csv(.x)) individual ## # A tibble: 14,961 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; Success! We now have all our data in a single tibble is just two concise lines of code!! 🎉 👏 Some tips on efficiency While the above code is elegant, it might not be the most efficient. read_csv calls readr function type_convert() to determine the data type for each column when it reads a file in, which is relatively expensive. The elegant code above mean that type_convert() is for every file that is loaded, ie 67 times. A more efficent way of implementing this to set all columns as character on-read and then run type_convert ourselves, only once, and only after our data have been combined into a single tibble. We can set all columns to character by default by providing column formating function readr::cols(.default = \"c\")) as the read_csv col_types argument. individual &lt;- purrr::map_df(individual_paths, ~readr::read_csv(.x, col_types = readr::cols(.default = &quot;c&quot;))) %&gt;% readr::type_convert() individual ## # A tibble: 14,961 x 12 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 4 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt; This might come in handy if you are dealing with a huge number of data files. Other packages to be aware of, especially if you are dealing with very large tables, are data.table and vroom. Learn more about perfomance and efficency in general. simple benchmark microbenchmark::microbenchmark({ # tidyverse purrr::map_df(individual_paths, ~readr::read_csv(.x))}, # tidyverse + read in as character {purrr::map_df(individual_paths, ~readr::read_csv(.x, col_types = readr::cols(.default = &quot;c&quot;))) %&gt;% readr::type_convert()}, # vroom package {vroom::vroom(individual_paths)}, # data.table {lapply(individual_paths, data.table::fread, sep=&quot;,&quot;) %&gt;% do.call(&quot;rbind&quot;, .)}, # purrr + data.table {purrr::map_df(individual_paths, data.table::fread, sep=&quot;,&quot;)}, times = 20) min lq mean median uq max neval 372.77828 389.98348 416.70189 395.01877 437.90033 512.19128 20 150.52621 164.60759 190.92799 175.99858 192.44910 322.73216 20 265.06628 272.09506 307.39295 285.53955 320.84952 518.55182 20 50.09148 53.22408 72.28593 58.26753 63.00565 182.50278 20 57.48761 58.76176 63.39054 63.31130 66.08371 75.92438 20 Writing out our tibble to disk Remember the other two files included in our raw data, vst_mappingandtagging.csv, and vst_perplotperyear.csv? Well the truth is they also came in multiple files which I put together in pretty much the same way as you just did! So for posterity, let’s save this file out too. This isn’t our finished analytic data set, we still have some processing to do. So let’s just save it at raw_data_path, along with the other files. To write out a csv file we use readr::write_csv() individual %&gt;% readr::write_csv(file.path(raw_data_path, &quot;vst_individual.csv&quot;)) Learn more about iteration and the family of purrr functions in the iteration chapter in R for data science "],["data-merge.html", "Merging data Join Basics Joining our tables with dplyr", " Merging data Let’s say we want to geolocate every individual in our analytic data. As we’ve discussed, the various tables we downloaded hold different information collected during the various survey events. plot level metadata individual level tagging metadata individual level repeated measurement data (although we only have a single measurement event per individual in our data set). Currently, only the plot is geolocated, the data being contained in vst_perplotperyear.csv columns decimalLatitude and decimalLongitude. The location of each individual stem is defined in vst_mappingandtagging.csv. A number of variables are involved, including pointID which identifies a point on a 10m cell grid centred around decimalLatitude and decimalLongitude, and stemDistance and stemAzimuth which define the location of a stem, relative to the location of pointID. The full method used to locate individual stems is detailed in methods/NEON_vegStructure_userGuide_vA.pdf. So to geolocate our individuals, we need to join information from vst_perplotperyear.csv and vst_mappingandtagging.csv into our individuals tibble. We use the family of *_join function in dplyr to merge columns from different tibbles. Join Basics There are a number of joins we can perform with dplyr. Let’s have a look at a few of them with a simple example using some dplyr in-built data: band_members ## # A tibble: 3 x 2 ## name band ## &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones ## 2 John Beatles ## 3 Paul Beatles band_instruments ## # A tibble: 3 x 2 ## name plays ## &lt;chr&gt; &lt;chr&gt; ## 1 John guitar ## 2 Paul bass ## 3 Keith guitar The only variable shared between the two tables is name so this is the only variable we can perform joins over. By default, any *_join function will try to merge on the values of any matched columns in the tables being merged. band_members %&gt;% inner_join(band_instruments) ## Joining, by = &quot;name&quot; ## # A tibble: 2 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Beatles guitar ## 2 Paul Beatles bass inner_join has merged all three unique columns across the two tables into a single tibble. It has only kept the rows in which name values had a match in both tables. In this case only data about John and Paul was contained in both tables. band_members %&gt;% left_join(band_instruments) ## Joining, by = &quot;name&quot; ## # A tibble: 3 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass left_join joins on the names in the left hand table and appends any rows from the right hand table in which name match. In this case, there is no data for Keith in band_members so he is ignored completely. There is also no match for Mick in band_instruments so NA is returned for plays instead. band_members %&gt;% right_join(band_instruments) ## Joining, by = &quot;name&quot; ## # A tibble: 3 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Beatles guitar ## 2 Paul Beatles bass ## 3 Keith &lt;NA&gt; guitar right_join on the other hand joins on the name in the right hand table. In this case, Mick is dropped completely Keith gets NA for band. band_members %&gt;% full_join(band_instruments) ## Joining, by = &quot;name&quot; ## # A tibble: 4 x 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass ## 4 Keith &lt;NA&gt; guitar Finally, a full_join joins on all unique values of name found across the two tables, returning NA where there are no matches between the two tables. Joining our tables with dplyr Join vst_mappingandtagging.csv data Let’s start by merging data from vst_mappingandtagging.csv. Let’s read the data in. maptag &lt;- readr::read_csv(fs::path(raw_data_path, &quot;vst_mappingandtagging.csv&quot;)) This data set contains taxonomic and within-plot location metadata on individuals collected during mapping and tagging. There is one row per individual in the data set. names(maptag) ## [1] &quot;uid&quot; &quot;eventID&quot; &quot;pointID&quot; &quot;stemDistance&quot; ## [5] &quot;stemAzimuth&quot; &quot;individualID&quot; &quot;taxonID&quot; &quot;scientificName&quot; ## [9] &quot;taxonRank&quot; Let’s see how many matches in column names we have between the two datasets Challenge: Finding column name matches in two tables Given the two tables we are trying to join, can you write some code that checks which column names in individual have matches in maptag column names? Hint: This is the correct answer. ## [1] &quot;uid&quot; &quot;eventID&quot; &quot;individualID&quot; Solution Default left_join Because we want to match the rest of the tables to our individual data, we use left_join() and supply individual as the first argument and maptag as the second. individual %&gt;% dplyr::left_join(maptag) ## Joining, by = c(&quot;uid&quot;, &quot;eventID&quot;, &quot;individualID&quot;) ## # A tibble: 14,961 x 18 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 10 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt;, pointID &lt;dbl&gt;, ## # stemDistance &lt;dbl&gt;, stemAzimuth &lt;dbl&gt;, taxonID &lt;chr&gt;, scientificName &lt;chr&gt;, ## # taxonRank &lt;chr&gt; Great we have a merge! Looks successful right? How do we really know nothing has gone wrong though? Remember, to successfully merge the tables, the data in the columns the tables are being joined on need to have corresponding values across all columns to be linked successfully, otherwise it will return NAs. So, although our code ran successfully, it may well not have found any matching rows in maptag to merge into individual. To check whether things have worked, we can start with inspecting the output for the columns of interest, in this case the maptag columns we are trying to join into individual. When working interactively and testing out pipes, you can pipe objects into View() for quick inspection. If you provide a character string as an argument, it is used as a name for the data view tab it launches individual %&gt;% dplyr::left_join(maptag) %&gt;% View(&quot;default&quot;) Clearly this has not worked! We need to start digging into why but we don’t want to have to keep manually checking whether it worked or not. Enter DEFENSIVE PROGRAMMING. Defensive programming with data As I mentioned in the Data Management Basics slides, assertr is a useful package for including validation checks in our data pipelines. In our case, we can use assertr function assert to check that certain columns of interest (stemDistance, stemAzimuth, pointID) are joined successfully (i.e. there re no NA values). Note that this only works because I know for a fact that there is data avaliable for all individuals. There may be situations in which NAs are valid missing data, in which case this would not be an appropriate test. individual %&gt;% dplyr::left_join(maptag) %&gt;% assertr::assert(assertr::not_na, stemDistance, stemAzimuth, pointID) ## Joining, by = c(&quot;uid&quot;, &quot;eventID&quot;, &quot;individualID&quot;) ## Column &#39;stemDistance&#39; violates assertion &#39;not_na&#39; 14961 times ## verb redux_fn predicate column index value ## 1 assert NA not_na stemDistance 1 NA ## 2 assert NA not_na stemDistance 2 NA ## 3 assert NA not_na stemDistance 3 NA ## 4 assert NA not_na stemDistance 4 NA ## 5 assert NA not_na stemDistance 5 NA ## [omitted 14956 rows] ## ## ## Column &#39;stemAzimuth&#39; violates assertion &#39;not_na&#39; 14961 times ## verb redux_fn predicate column index value ## 1 assert NA not_na stemAzimuth 1 NA ## 2 assert NA not_na stemAzimuth 2 NA ## 3 assert NA not_na stemAzimuth 3 NA ## 4 assert NA not_na stemAzimuth 4 NA ## 5 assert NA not_na stemAzimuth 5 NA ## [omitted 14956 rows] ## ## ## Column &#39;pointID&#39; violates assertion &#39;not_na&#39; 14961 times ## verb redux_fn predicate column index value ## 1 assert NA not_na pointID 1 NA ## 2 assert NA not_na pointID 2 NA ## 3 assert NA not_na pointID 3 NA ## 4 assert NA not_na pointID 4 NA ## 5 assert NA not_na pointID 5 NA ## [omitted 14956 rows] ## Error: assertr stopped execution By including this check, I don’t have to guess or manually check whether the merge has been successful. The code will just error if it hasn’t 🙌. Debugging hidden mismatches: I’ve shown the most minimal implementation in which dplyr does a lot of the guessing for us and tries to join on all matched columns. But often, that can generate table mismatches We know that the only column we are interested in matching on is individualID. We want to get the mapping associated with each individual, regardless of when the mapping was collected. We can be specific about which variables we want to join on through argument by. individual %&gt;% dplyr::left_join(maptag, by = &quot;individualID&quot;) %&gt;% assertr::assert(assertr::not_na, stemDistance, stemAzimuth, pointID) ## # A tibble: 14,961 x 20 ## uid.x namedLocation date eventID.x domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a1… BART_037.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7… BART_037.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 3 a8951… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 4 eb348… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 5 2a447… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 6 e4852… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 7 280c9… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 8 0e506… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 9 4918c… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## 10 ef16c… BART_044.bas… 2015-08-26 vst_BART… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 12 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt;, uid.y &lt;chr&gt;, ## # eventID.y &lt;chr&gt;, pointID &lt;dbl&gt;, stemDistance &lt;dbl&gt;, stemAzimuth &lt;dbl&gt;, ## # taxonID &lt;chr&gt;, scientificName &lt;chr&gt;, taxonRank &lt;chr&gt; Excellent! Our code runs and our resulting merged tibble contains data for all the variables we are interested in! However, on closer inspection, we’ve ended up with some odd new columns, uid.x and uid.y and eventID.x and eventID.y! That’s because those columns are also present in both our tables but we are not explicitly joining them. They are retained and each suffixed with .x &amp; .y by default, to make them unique. So, what about these duplicate columns. Do we need them? With respect to eventID, we’re not really interested in the mapping eventIDs so we can just drop that column from maptag. maptag &lt;- select(maptag, -eventID) individual %&gt;% dplyr::left_join(maptag, by = &quot;individualID&quot;) %&gt;% assertr::assert(assertr::not_na, stemDistance, stemAzimuth, pointID) ## # A tibble: 14,961 x 19 ## uid.x namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 11 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt;, uid.y &lt;chr&gt;, ## # pointID &lt;dbl&gt;, stemDistance &lt;dbl&gt;, stemAzimuth &lt;dbl&gt;, taxonID &lt;chr&gt;, ## # scientificName &lt;chr&gt;, taxonRank &lt;chr&gt; On the other hand, \"uid contains unique identifiers for each observation in their respective table and could be useful metadata to store, enabling us to trace the provenance of individual values to the original data. So rather than remove them, let’s retain both uid, one for each table. We can give more informative suffixes using argument suffix. In our case, I want the individual column to stay as uid and the maptag column to get the suffix _map. individual %&gt;% dplyr::left_join(maptag, by = &quot;individualID&quot;, suffix = c(&quot;&quot;, &quot;_map&quot;)) %&gt;% assertr::assert(assertr::not_na, stemDistance, stemAzimuth, pointID) ## # A tibble: 14,961 x 19 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 11 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt;, uid_map &lt;chr&gt;, ## # pointID &lt;dbl&gt;, stemDistance &lt;dbl&gt;, stemAzimuth &lt;dbl&gt;, taxonID &lt;chr&gt;, ## # scientificName &lt;chr&gt;, taxonRank &lt;chr&gt; Nice! Join vst_perplotperyear.csv Now let’s carry on and join the perplot data. First let’s read it in. perplot &lt;- readr::read_csv(fs::path(raw_data_path, &quot;vst_perplotperyear.csv&quot;)) names(perplot) ## [1] &quot;uid&quot; &quot;plotID&quot; &quot;plotType&quot; ## [4] &quot;nlcdClass&quot; &quot;decimalLatitude&quot; &quot;decimalLongitude&quot; ## [7] &quot;geodeticDatum&quot; &quot;easting&quot; &quot;northing&quot; ## [10] &quot;utmZone&quot; &quot;elevation&quot; &quot;elevationUncertainty&quot; ## [13] &quot;eventID&quot; Similarly to maptag, we want to exclude eventID and suffix the uid column. This time, however, we will be joining by plotID Let’s also move our validation test to the end and add the new columns we want to check to it, i.e. stemDistance, stemAzimuth, pointID. perplot &lt;- perplot %&gt;% select(-eventID) individual %&gt;% dplyr::left_join(maptag, by = &quot;individualID&quot;, suffix = c(&quot;&quot;, &quot;_map&quot;)) %&gt;% dplyr::left_join(perplot, by = c(&quot;plotID&quot;), suffix = c(&quot;&quot;, &quot;_ppl&quot;)) %&gt;% assertr::assert(assertr::not_na, decimalLatitude, decimalLongitude, plotID, stemDistance, stemAzimuth, pointID) ## # A tibble: 14,961 x 30 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 22 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt;, uid_map &lt;chr&gt;, ## # pointID &lt;dbl&gt;, stemDistance &lt;dbl&gt;, stemAzimuth &lt;dbl&gt;, taxonID &lt;chr&gt;, ## # scientificName &lt;chr&gt;, taxonRank &lt;chr&gt;, uid_ppl &lt;chr&gt;, plotType &lt;chr&gt;, ## # nlcdClass &lt;chr&gt;, decimalLatitude &lt;dbl&gt;, decimalLongitude &lt;dbl&gt;, ## # geodeticDatum &lt;chr&gt;, easting &lt;dbl&gt;, northing &lt;dbl&gt;, utmZone &lt;chr&gt;, ## # elevation &lt;dbl&gt;, elevationUncertainty &lt;dbl&gt; Awesome!! It’s worked! Now that we are happy with our data we can use a new operator, the assignment pipe (%&lt;&gt;%). This allows us to both pipe an object forward into an expression and also update it with the resulting value. individual %&lt;&gt;% dplyr::left_join(maptag, by = &quot;individualID&quot;, suffix = c(&quot;&quot;, &quot;_map&quot;)) %&gt;% dplyr::left_join(perplot, by = c(&quot;plotID&quot;), suffix = c(&quot;&quot;, &quot;_ppl&quot;)) %&gt;% assertr::assert(assertr::not_na, decimalLatitude, decimalLongitude, plotID, stemDistance, stemAzimuth, pointID) We can now move on to geolocate our individuals! "],["functions.html", "Functions Function basics Elements of a function Creating our user-built functions Experimenting Developing our function Saving analytical data Final processing script: Final function", " Functions As we’ve mentioned, functions, and especially user built custom functions, are a key feature of R and are a really powerful of reducing code repetition. Function basics Functions allow us to: incorporate sets of instructions that we want to use repeatedly contain complex code in a neat sub-program reduce opportunity for errors make code more readable You can do anything with functions that you can do with vectors: assign them to variables store them in lists pass them as arguments to other functions create them inside functions return them as the result of a function In general, functions usually: - accept parameters (arguments) &lt;- INPUT - return value(s) &lt;- OUTPUT Elements of a function Here’s a simple skeleton of a function. name &lt;- function(variables) { } Function name This can be any valid variable name, but you should avoid using names that are used elsewhere in R, such as dir, function, plot, etc choose descriptive names use verbs check whether they are already in use: ? function.name (you can access a function from a specific package using package.name::function.name) Arguments Functions can have any number of arguments. These can be any R object: numbers, strings, arrays, data frames, of even pointers to other functions; anything that is needed for the function to run. Again, use descriptive names for arguments. Body The function code between the {} brackets is run every time the function is called. Ideally functions are short and do just one thing. All inputs required for computation in the body must be supplied as arguments. Simple example Let’s write a simple function that takes two arguments x and y and adds them together. add &lt;- function(x, y) { x + y } Let’s test it. x &lt;- 4 y &lt;- 2 add(x, y) ## [1] 6 Cool it works! Return value By default, the output of the last line of the code is evaluated is the value that will be returned by the function. We can override that default by using return to explicitly specify what is returned. add &lt;- function(x, y) { x + y return(NULL) } add(x, y) ## NULL It is not necessary that a function return anything, for example a function that makes a plot might not return anything, whereas a function that does a mathematical operation might return a number, or a list. Function Environment Every time a function is called, a new environment is created to host execution. Each invocation is completely independent of previous ones Variables used within are local, e.g. their scope lies within - and is limited to - the function itself. They are therefore invisible outside the function body Objects required by the function will be sought first in the local environment. If an argument specified in the function is missing, it will return an error, even if such an object exists in the global environment. Objects required by computation but not specified as function arguments will be sought in the containing environment iteratively until it reaches the global environment. This can be a source of bugs when developing with an untidy global environment. b &lt;- 10 f2 &lt;- function(a){a + b} f2(a = 10) ## [1] 20 rm(b) # remove object b f2(a = 10) ## Error in f2(a = 10): object &#39;b&#39; not found Solution: always make sure any required variables are passed as arguments to your functions. Creating our user-built functions Now that we’ve got all the information required, we can start building a function that will return our latitude and longitude for each individual. We can use function destPoint form package geosphere to calculate the destination latitude and longitude from a given starting point, the distance travelled and the direction (bearing) travelled in. I our case the distance travelled is equivalent to stemDistance and the direction or bearing is equivalent to stemAzimuth. The starting point is given by decimalLatitude and `decimalLongitude. Now, let’s write a function that takes these columns as inputs and returns the latitude and longitude of the location of our individuals. Storing functions in scripts. It’s best to store functions in separate scripts in the R/ directory. We can use function usethis::use_r() to create scripts in R/. Let’s create a new one to start working on our function. usethis::use_r(&quot;geolocate&quot;) ✔ Setting active project to &#39;/Users/Anna/Documents/workflows/workshops/books/rrresearchACCE20&#39; ● Modify &#39;R/geolocate.R&#39; This creates the required R/ directory, creates a new R script named geolocation.R within it and launches it for editing all in one go! Nice. Experimenting Now before we begin writing our function, let’s test destPoint out. To do that, let’s subset a single row from individual and use it to test out the function. We need to supply a vector of length two, containing the starting longitude and latitude to argument p. We pass stemAzimuth to argument b (for bearing) and stemDistance to argument d (for distance). x &lt;- individual[1,] geosphere::destPoint(p = c(x$decimalLongitude, x$decimalLatitude), b = x$stemAzimuth, d = x$stemDistance) ## lon lat ## [1,] -71.28333 44.06151 This looks like it’s working nicely. Let’s also check that it vectorises easily, i.e. that if we give it vectors of values instead of single ones that it works properly. x &lt;- individual[1:5,] geosphere::destPoint(p = c(x$decimalLongitude, x$decimalLatitude), b = x$stemAzimuth, d = x$stemDistance) ## Error in .pointsToMatrix(p): Wrong length for a vector, should be 2 geosphere::destPoint(p = cbind(x$decimalLongitude, x$decimalLatitude), b = x$stemAzimuth, d = x$stemDistance) ## lon lat ## [1,] -71.28333 44.06151 ## [2,] -71.28325 44.06165 ## [3,] -71.28419 44.06109 ## [4,] -71.28424 44.06096 ## [5,] -71.28409 44.06103 Excellent! I now get a two dimensional matrix of with two columns and a row for each input element! This is looking promising. We’re ready to start writing our function. Developing our function Let’s start by using a handy feature in Rstudio, code snippets. Code snippets are text macros that are used for quickly inserting common snippets of code. The fun snippet inserts an R function definition. To invoke it start typing fun until the auto-complete drop-down pops up. fun Select fun and hit Return / Enter. You end up with this handy function skeleton. Now, let’s start populating it with our own function. name &lt;- function(variables) { } First lets start with a descriptive name: get_stem_location &lt;- function(variables) { } Let’s add our arguments: get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ } Finally, let’s populate the body our our function: get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) } Let’s also convert the output to a tibble, for better printing. get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() } Now let’s test it out with vectors from individual. test &lt;- get_stem_location(x$decimalLongitude, x$decimalLatitude, x$stemAzimuth, x$stemDistance) test ## # A tibble: 5 x 2 ## lon lat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -71.3 44.1 ## 2 -71.3 44.1 ## 3 -71.3 44.1 ## 4 -71.3 44.1 ## 5 -71.3 44.1 Looks like it works nicely! Defensive programming in functions Our function seems to be working correctly but it’s good to incorporate checks, especially on our inputs and outputs. For example, if we supply a character vector to our function by mistake, our function won’t work. We can write smart checks using the suite of functions in package checkmate. One such function is assert_numeric() This checks whether the the object we give it is numeric. If the check is not successful, it returns the object invisibly. If the check is not successful, it throws an error. checkmate::assert_numeric(x$decimalLatitude) checkmate::assert_numeric(x$uid) ## Error in eval(expr, envir, enclos): Assertion on &#39;x$uid&#39; failed: Must be of type &#39;numeric&#39;, not &#39;character&#39;. There are two other versions, test_numeric which returns FALSE if the check is not successful, and check_numeric which returns a string with the error message. We want to throw an error and stop execution so we use assert_numeric. Let’s add a validation check for each argument in our function. get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ # validation checks checkmate::assert_numeric(decimalLongitude) checkmate::assert_numeric(decimalLatitude) checkmate::assert_numeric(stemAzimuth) checkmate::assert_numeric(stemDistance) geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() } Let’s also add a check to our output. Let’s throw a warning if there are any NA values in our output. First we store our output so we can evaluate it. get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ # validation checks checkmate::assert_numeric(decimalLongitude) checkmate::assert_numeric(decimalLatitude) checkmate::assert_numeric(stemAzimuth) checkmate::assert_numeric(stemDistance) out &lt;- geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() } Next we can add our check: We can check the whole tibble for NAs in one go. We get a 2 dimensional matrix of logical values. is.na(test) %&gt;% head() ## lon lat ## [1,] FALSE FALSE ## [2,] FALSE FALSE ## [3,] FALSE FALSE ## [4,] FALSE FALSE ## [5,] FALSE FALSE We can then wrap the output of that in any() which tests whether there are any TRUE values in a logical array. any(is.na(test)) ## [1] FALSE Let’s apply that to our function. get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ # validation checks checkmate::assert_numeric(decimalLongitude) checkmate::assert_numeric(decimalLatitude) checkmate::assert_numeric(stemAzimuth) checkmate::assert_numeric(stemDistance) out &lt;- geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() checkmate::assert_false(any(is.na(out))) } Lastly, we need to return our actual output! get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance){ # validation checks checkmate::assert_numeric(decimalLongitude) checkmate::assert_numeric(decimalLatitude) checkmate::assert_numeric(stemAzimuth) checkmate::assert_numeric(stemDistance) out &lt;- geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() checkmate::assert_false(any(is.na(out))) return(out) } Let’s test it again: get_stem_location(x$decimalLongitude, x$decimalLatitude, x$stemAzimuth, x$stemDistance) ## # A tibble: 5 x 2 ## lon lat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -71.3 44.1 ## 2 -71.3 44.1 ## 3 -71.3 44.1 ## 4 -71.3 44.1 ## 5 -71.3 44.1 And now, remove any excess code form our script and save. Our function is now ready to be sourced into our last preprocessing stage, adding the new stemLat and stemLon columns. 🎉. Making new variables Let’s move back to our individual.R script. At the top of our script, let’s add the code to source our function so it’s available during preprocessing: source(here::here(&quot;R&quot;, &quot;geolocation.R&quot;)) Now we want to use data in individual to geolocate our individulas while at the same time creating new columns stemLat and stemLon For this we use dplyr::mutate(): individual %&gt;% dplyr::mutate(stemLat = get_stem_location(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance)$lat, stemLon = get_stem_location(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance)$lon) ## # A tibble: 14,961 x 32 ## uid namedLocation date eventID domainID siteID plotID individualID ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a16… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 2 68dc7a… BART_037.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 3 a8951a… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 4 eb348e… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 5 2a4478… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 6 e48520… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 7 280c90… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 8 0e5060… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 9 4918ca… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## 10 ef16cb… BART_044.base… 2015-08-26 vst_BA… D01 BART BART_… NEON.PLA.D0… ## # … with 14,951 more rows, and 24 more variables: growthForm &lt;chr&gt;, ## # stemDiameter &lt;dbl&gt;, measurementHeight &lt;dbl&gt;, height &lt;dbl&gt;, uid_map &lt;chr&gt;, ## # pointID &lt;dbl&gt;, stemDistance &lt;dbl&gt;, stemAzimuth &lt;dbl&gt;, taxonID &lt;chr&gt;, ## # scientificName &lt;chr&gt;, taxonRank &lt;chr&gt;, uid_ppl &lt;chr&gt;, plotType &lt;chr&gt;, ## # nlcdClass &lt;chr&gt;, decimalLatitude &lt;dbl&gt;, decimalLongitude &lt;dbl&gt;, ## # geodeticDatum &lt;chr&gt;, easting &lt;dbl&gt;, northing &lt;dbl&gt;, utmZone &lt;chr&gt;, ## # elevation &lt;dbl&gt;, elevationUncertainty &lt;dbl&gt;, stemLat &lt;dbl&gt;, stemLon &lt;dbl&gt; We also need to extract the appropriate coordinate for each column. We do that by using the $ subsetting operation after we call get_stem_location(). It works! We’re almost done with our data munging! Lets use the assignment pipe again now that we are happy. individual %&lt;&gt;% dplyr::mutate(stemLat = get_stem_location(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance)$lat, stemLon = get_stem_location(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance)$lon) Let’s do a couple last sanity checks: View(individual) str(individual) ## spec_tbl_df[,32] [14,961 × 32] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ uid : chr [1:14961] &quot;a36a162d-ed1f-4f80-ae45-88e973855c68&quot; &quot;68dc7adf-48e2-4f7a-9272-9a468fde6d55&quot; &quot;a8951ab9-4462-48dd-ab9e-7b89e24f2e03&quot; &quot;eb348eaf-3969-46a4-ac3b-523c3548efeb&quot; ... ## $ namedLocation : chr [1:14961] &quot;BART_037.basePlot.vst&quot; &quot;BART_037.basePlot.vst&quot; &quot;BART_044.basePlot.vst&quot; &quot;BART_044.basePlot.vst&quot; ... ## $ date : Date[1:14961], format: &quot;2015-08-26&quot; &quot;2015-08-26&quot; ... ## $ eventID : chr [1:14961] &quot;vst_BART_2015&quot; &quot;vst_BART_2015&quot; &quot;vst_BART_2015&quot; &quot;vst_BART_2015&quot; ... ## $ domainID : chr [1:14961] &quot;D01&quot; &quot;D01&quot; &quot;D01&quot; &quot;D01&quot; ... ## $ siteID : chr [1:14961] &quot;BART&quot; &quot;BART&quot; &quot;BART&quot; &quot;BART&quot; ... ## $ plotID : chr [1:14961] &quot;BART_037&quot; &quot;BART_037&quot; &quot;BART_044&quot; &quot;BART_044&quot; ... ## $ individualID : chr [1:14961] &quot;NEON.PLA.D01.BART.05285&quot; &quot;NEON.PLA.D01.BART.05279&quot; &quot;NEON.PLA.D01.BART.05419&quot; &quot;NEON.PLA.D01.BART.05092&quot; ... ## $ growthForm : chr [1:14961] &quot;single bole tree&quot; &quot;single bole tree&quot; &quot;single bole tree&quot; &quot;single bole tree&quot; ... ## $ stemDiameter : num [1:14961] 17.1 13.7 12.3 12.1 29.2 12.1 23.4 39.5 10 10.6 ... ## $ measurementHeight : num [1:14961] 130 130 130 130 130 130 130 130 130 130 ... ## $ height : num [1:14961] 15.2 9.8 7.7 15.2 16.7 10.6 18.4 19 5.7 8.7 ... ## $ uid_map : chr [1:14961] &quot;31c5ffdb-25cb-474c-b34b-c88ddf520dc2&quot; &quot;a59c6688-ef88-46bb-979d-ba23b6e84d1a&quot; &quot;64a921b6-ce50-442e-8811-40e6c086c99e&quot; &quot;f6cba56b-b14f-42e0-ab20-06e2bfa216d2&quot; ... ## $ pointID : num [1:14961] 61 41 23 57 57 23 57 41 57 57 ... ## $ stemDistance : num [1:14961] 11.3 6.1 12 11.5 19 7 7.4 6.4 12.7 15.1 ... ## $ stemAzimuth : num [1:14961] 212.1 4 62.1 140.8 93 ... ## $ taxonID : chr [1:14961] &quot;ACRU&quot; &quot;FAGR&quot; &quot;TSCA&quot; &quot;FAGR&quot; ... ## $ scientificName : chr [1:14961] &quot;Acer rubrum L.&quot; &quot;Fagus grandifolia Ehrh.&quot; &quot;Tsuga canadensis (L.) Carrière&quot; &quot;Fagus grandifolia Ehrh.&quot; ... ## $ taxonRank : chr [1:14961] &quot;species&quot; &quot;species&quot; &quot;species&quot; &quot;species&quot; ... ## $ uid_ppl : chr [1:14961] &quot;5ac133b6-1089-4f32-9c26-27c3fd6b2597&quot; &quot;5ac133b6-1089-4f32-9c26-27c3fd6b2597&quot; &quot;c4207c9a-028a-4a26-a4ee-5d1a70df1e66&quot; &quot;c4207c9a-028a-4a26-a4ee-5d1a70df1e66&quot; ... ## $ plotType : chr [1:14961] &quot;tower&quot; &quot;tower&quot; &quot;tower&quot; &quot;tower&quot; ... ## $ nlcdClass : chr [1:14961] &quot;deciduousForest&quot; &quot;deciduousForest&quot; &quot;deciduousForest&quot; &quot;deciduousForest&quot; ... ## $ decimalLatitude : num [1:14961] 44.1 44.1 44.1 44.1 44.1 ... ## $ decimalLongitude : num [1:14961] -71.3 -71.3 -71.3 -71.3 -71.3 ... ## $ geodeticDatum : chr [1:14961] &quot;WGS84&quot; &quot;WGS84&quot; &quot;WGS84&quot; &quot;WGS84&quot; ... ## $ easting : num [1:14961] 317130 317130 317042 317042 317042 ... ## $ northing : num [1:14961] 4881249 4881249 4881189 4881189 4881189 ... ## $ utmZone : chr [1:14961] &quot;19N&quot; &quot;19N&quot; &quot;19N&quot; &quot;19N&quot; ... ## $ elevation : num [1:14961] 292 292 303 303 303 ... ## $ elevationUncertainty: num [1:14961] 0.2 0.2 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 ... ## $ stemLat : num [1:14961] 44.1 44.1 44.1 44.1 44.1 ... ## $ stemLon : num [1:14961] -71.3 -71.3 -71.3 -71.3 -71.3 ... And save our file. Saving analytical data At the bottom of individual.R there is some template code, usethis::use_data(\"individual\"). This functions invokes functionality to store an r object as an .Rdata object (ie as a tibble not a csv) in the data directory. This is the standard way to store exported data in packages but does not apply to our non-package project so won’t work. usethis::use_data(&quot;individual&quot;) ## ✔ Setting active project to &#39;/Users/runner/work/rrresearchACCE20/rrresearchACCE20&#39; ## Error: Can only save existing named objects. Let’s just get rid of it and instead, save our analytic data as a csv in our data directory. Frist lets create a data directory (you can do this in the console) fs::dir_create(&quot;data&quot;) Now were ready to write or data out. Before we do so, I will add one last touch. Before I do so, I would like to get rid of a pet hate of mine, and thats camelCase variable names! I use a handy function in package janitor, ! individual %&gt;% janitor::clean_names() ## # A tibble: 14,961 x 32 ## uid named_location date event_id domain_id site_id plot_id ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a36a162d-ed1… BART_037.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 2 68dc7adf-48e… BART_037.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 3 a8951ab9-446… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 4 eb348eaf-396… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 5 2a4478ef-597… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 6 e485203e-879… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 7 280c9049-191… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 8 0e5060ec-a6d… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 9 4918cac0-62f… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## 10 ef16cb9c-0b6… BART_044.basePl… 2015-08-26 vst_BART… D01 BART BART_0… ## # … with 14,951 more rows, and 25 more variables: individual_id &lt;chr&gt;, ## # growth_form &lt;chr&gt;, stem_diameter &lt;dbl&gt;, measurement_height &lt;dbl&gt;, ## # height &lt;dbl&gt;, uid_map &lt;chr&gt;, point_id &lt;dbl&gt;, stem_distance &lt;dbl&gt;, ## # stem_azimuth &lt;dbl&gt;, taxon_id &lt;chr&gt;, scientific_name &lt;chr&gt;, ## # taxon_rank &lt;chr&gt;, uid_ppl &lt;chr&gt;, plot_type &lt;chr&gt;, nlcd_class &lt;chr&gt;, ## # decimal_latitude &lt;dbl&gt;, decimal_longitude &lt;dbl&gt;, geodetic_datum &lt;chr&gt;, ## # easting &lt;dbl&gt;, northing &lt;dbl&gt;, utm_zone &lt;chr&gt;, elevation &lt;dbl&gt;, ## # elevation_uncertainty &lt;dbl&gt;, stem_lat &lt;dbl&gt;, stem_lon &lt;dbl&gt; This turns all the column names to my beloved snake case! Now, with that final tweek, we are ready to save our data. individual %&gt;% janitor::clean_names() %&gt;% readr::write_csv(here::here(&quot;data&quot;, &quot;individual.csv&quot;)) Final processing script: data-raw/individual.R ## code to prepare `individual` dataset goes here library(dplyr) source(here::here(&quot;R&quot;, &quot;geolocate.R&quot;)) # Read in and compile all individual data raw_data_path &lt;- here::here(&quot;data-raw&quot;, &quot;wood-survey-data-master&quot;) individual_path &lt;- fs::path(raw_data_path, &quot;individual&quot;) individual_paths &lt;- fs::dir_ls(individual_path) # read in all tables into one individual &lt;- purrr::map_df(individual_paths, ~readr::read_csv(file = .x, col_types = readr::cols(.default = &quot;c&quot;))) %&gt;% readr::type_convert() individual %&gt;% readr::write_csv(path = fs::path(raw_data_path, &quot;vst_individuals.csv&quot;)) # Combine NEON data tables # read in additonal table maptag &lt;- readr::read_csv(fs::path(raw_data_path, &quot;vst_mappingandtagging.csv&quot;)) %&gt;% select(-eventID) perplot &lt;- readr::read_csv(fs::path(raw_data_path, &quot;vst_perplotperyear.csv&quot;)) %&gt;% select(-eventID) individual %&lt;&gt;% left_join(maptag, by = &quot;individualID&quot;, suffix = c(&quot;&quot;, &quot;_map&quot;)) %&gt;% left_join(perplot, by = &quot;plotID&quot;, suffix = c(&quot;&quot;, &quot;_ppl&quot;)) %&gt;% assertr::assert(assertr::not_na, stemDistance, stemAzimuth, pointID, decimalLongitude, decimalLatitude, plotID) # ---- Geolocate individuals_functions ---- individual &lt;- individual %&gt;% dplyr::mutate(stemLat = get_stem_location(decimalLongitude = decimalLongitude, decimalLatitude = decimalLatitude, stemAzimuth = stemAzimuth, stemDistance = stemDistance)$lat, stemLon = get_stem_location(decimalLongitude = decimalLongitude, decimalLatitude = decimalLatitude, stemAzimuth = stemAzimuth, stemDistance = stemDistance)$lon) %&gt;% janitor::clean_names() # write out analytic file readr::write_csv(individual, here::here(&quot;data&quot;, &quot;individual.csv&quot;)) Final function R/geolocate.R # Function get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance) { # check inputs are correct type (numeric) checkmate::assert_numeric(decimalLatitude) checkmate::assert_numeric(decimalLongitude) checkmate::assert_numeric(stemAzimuth) checkmate::assert_numeric(stemDistance) out &lt;- geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() # check output for NAs checkmate::assert_false(any(is.na(out))) return(out) } "],["metadata.html", "(PART) Working with metadata", " (PART) Working with metadata "],["metadata-slides.html", "Metadata", " Metadata View Slides "],["dataspice.html", "Creating metadata with dataspice dataspice workflow Create the metadata folder Record metadata Create metadata json-ld file Build README site", " Creating metadata with dataspice The goal of this section is to provide a practical exercise in creating metadata for an example field collected data product using package dataspice. Understand basic metadata and why it is important. Understand where and how to store them. Understand how they can feed into more complex metadata objects. dataspice workflow see introductory slides Let’s load our library and start creating some metadata! library(dataspice) Create the metadata folder We’ll start by creating the basic metadata .csv files in which to collect metadata related to our example dataset using function dataspice::create_spice(). create_spice() This creates a metadata folder in your project’s data folder (although you can specify a different directory if required) containing 4 .csv files in which to record your metadata. access.csv: record details about where your data can be accessed. attributes.csv: record details about the variables in your data. biblio.csv: record dataset level metadata like title, description, licence and spatial and temoral coverage. creators.csv: record creator details. Record metadata creators.csv The creators.csv contains details of the dataset creators. Let’s start with a quick and easy file to complete, the creators. We can open and edit the file using in an interactive shiny app using dataspice::edit_creators(). Although we did not collect this data, just complete with your own details for the purposes of this tutorial. edit_creators() Remember to click on Save when you’re done editing. access.csv The access.csv contains details about where the data can be accessed. Before manually completing any details in the access.csv, we can use dataspice’s dedicated function prep_access() to extract relevant information from the data files themselves. prep_access() Next, we can use function edit_access() to view access. The final details required, namely the URL at which each dataset can be downloaded from cannot be completed now so just leave that blank for now. Eventually it should link to a permanent identifier from which the published. data set can be downloaded from. We can also edit details such as the name field to something more informative if required. edit_access() Remember to click on Save when you’re done editing. biblio.csv The biblio.csv contains dataset level metadata like title, description, licence and spatial and temporal coverage. Before we start filling this table in, we can use some base R functions to extract some of the information we require. In particular we can use function range() to extract the temporal and spatial extents of our data from the columns containing temporal and spatial data. get temporal extent Although dates are stored as a text string, because they are in ISO format (YYYY-MM-DD), sorting them results in correct chronological ordering. If your temporal data is not in ISO format, consider converting them (see package lubridate) range(individual$date) ## [1] &quot;2015-05-18&quot; &quot;2018-11-16&quot; get geographical extent The lat/lon coordinates are in decimal degrees which again are easy to sort or calculate the range in each dimension. South/North boundaries range(individual$decimal_latitude) ## [1] 17.95570 47.16582 West/East boundaries range(individual$decimal_longitude) ## [1] -99.11107 -66.82463 NB: you can also supply the geographic boundaries of your data as a single well-known text string in field wktString instead of supplying the four boundary coordinates. Geographic description We’ll also need a geographic textual description. Let’s check the unique values in domain_id and use those to create a geographic description. unique(individual$domain_id) ## [1] &quot;D01&quot; &quot;D02&quot; &quot;D03&quot; &quot;D04&quot; &quot;D07&quot; &quot;D08&quot; &quot;D09&quot; We could use NEON Domain areas D01:D09 for our geographic description. Now that we’ve got the values for our temporal and spatial extents and decided on the geographic description, we can complete the rest of the fields in the biblio.csv file using function dataspice::edit_biblio(). edit_biblio() 🔍 metadata hunt Complete the rest of the fields in biblio.csv Additional information required to complete these fields can be found on the NEON data portal page for this dataset and the data-raw/wood-survey-data-master README.md Citation: National Ecological Observatory Network. 2020. Data Products: DP1.10098.001. Provisional data downloaded from http://data.neonscience.org on 2020-01-15. Battelle, Boulder, CO, USA Here’s an example to get you started Remember to click on Save when you’re done editing. attributes.csv The attributes.csv contains details about the variables in your data. Again, dataspice provides functionality to populate the attributes.csv by extracting the variable names from our data file using function dataspice::prep_attributes(). The functions is vectorised and maps over each .csv file in our data/ folder. prep_attributes() All column names in individual.csv have been successfully extracted into the variableName column. Now, we could manually complete the description and unitText fields,… or we can use a secret weapon, NEON_vst_variables.csv in our raw data! Let’s read it in and have a look: variables &lt;- readr::read_csv(here::here(&quot;data-raw&quot;, &quot;wood-survey-data-master&quot;, &quot;NEON_vst_variables.csv&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## table = col_character(), ## fieldName = col_character(), ## description = col_character(), ## dataType = col_character(), ## units = col_character(), ## downloadPkg = col_character() ## ) variables ## # A tibble: 117 x 6 ## table fieldName description dataType units downloadPkg ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 vst_shr… uid Unique ID within NEON databa… string &lt;NA&gt; basic ## 2 vst_shr… namedLocat… Name of the measurement loca… string &lt;NA&gt; basic ## 3 vst_shr… date Date or date and time of mea… dateTime &lt;NA&gt; basic ## 4 vst_shr… eventID An identifier for the set of… string &lt;NA&gt; basic ## 5 vst_shr… domainID Unique identifier of the NEO… string &lt;NA&gt; basic ## 6 vst_shr… siteID NEON site code string &lt;NA&gt; basic ## 7 vst_shr… plotID Plot identifier (NEON site c… string &lt;NA&gt; basic ## 8 vst_shr… subplotID Identifier for the NEON subp… string &lt;NA&gt; basic ## 9 vst_shr… nestedSubp… Numeric identifier for neste… string &lt;NA&gt; basic ## 10 vst_shr… groupID Identifier for a group of in… string &lt;NA&gt; basic ## # … with 107 more rows All original data variable names are contained in fieldName. variables$fieldName ## [1] &quot;uid&quot; &quot;namedLocation&quot; ## [3] &quot;date&quot; &quot;eventID&quot; ## [5] &quot;domainID&quot; &quot;siteID&quot; ## [7] &quot;plotID&quot; &quot;subplotID&quot; ## [9] &quot;nestedSubplotID&quot; &quot;groupID&quot; ## [11] &quot;taxonID&quot; &quot;scientificName&quot; ## [13] &quot;taxonRank&quot; &quot;identificationReferences&quot; ## [15] &quot;identificationQualifier&quot; &quot;volumePercent&quot; ## [17] &quot;livePercent&quot; &quot;deadPercent&quot; ## [19] &quot;canopyArea&quot; &quot;meanHeight&quot; ## [21] &quot;remarks&quot; &quot;measuredBy&quot; ## [23] &quot;recordedBy&quot; &quot;dataQF&quot; ## [25] &quot;uid&quot; &quot;namedLocation&quot; ## [27] &quot;date&quot; &quot;eventID&quot; ## [29] &quot;domainID&quot; &quot;siteID&quot; ## [31] &quot;plotID&quot; &quot;subplotID&quot; ## [33] &quot;individualID&quot; &quot;tempShrubStemID&quot; ## [35] &quot;tagStatus&quot; &quot;growthForm&quot; ## [37] &quot;plantStatus&quot; &quot;stemDiameter&quot; ## [39] &quot;measurementHeight&quot; &quot;height&quot; ## [41] &quot;baseCrownHeight&quot; &quot;breakHeight&quot; ## [43] &quot;breakDiameter&quot; &quot;maxCrownDiameter&quot; ## [45] &quot;ninetyCrownDiameter&quot; &quot;canopyPosition&quot; ## [47] &quot;shape&quot; &quot;basalStemDiameter&quot; ## [49] &quot;basalStemDiameterMsrmntHeight&quot; &quot;maxBaseCrownDiameter&quot; ## [51] &quot;ninetyBaseCrownDiameter&quot; &quot;remarks&quot; ## [53] &quot;recordedBy&quot; &quot;measuredBy&quot; ## [55] &quot;dataQF&quot; &quot;uid&quot; ## [57] &quot;namedLocation&quot; &quot;date&quot; ## [59] &quot;domainID&quot; &quot;siteID&quot; ## [61] &quot;plotID&quot; &quot;plotType&quot; ## [63] &quot;nlcdClass&quot; &quot;decimalLatitude&quot; ## [65] &quot;decimalLongitude&quot; &quot;geodeticDatum&quot; ## [67] &quot;coordinateUncertainty&quot; &quot;easting&quot; ## [69] &quot;northing&quot; &quot;utmZone&quot; ## [71] &quot;elevation&quot; &quot;elevationUncertainty&quot; ## [73] &quot;eventID&quot; &quot;samplingProtocolVersion&quot; ## [75] &quot;treesPresent&quot; &quot;treesAbsentList&quot; ## [77] &quot;shrubsPresent&quot; &quot;shrubsAbsentList&quot; ## [79] &quot;lianasPresent&quot; &quot;lianasAbsentList&quot; ## [81] &quot;nestedSubplotAreaShrubSapling&quot; &quot;nestedSubplotAreaLiana&quot; ## [83] &quot;totalSampledAreaTrees&quot; &quot;totalSampledAreaShrubSapling&quot; ## [85] &quot;totalSampledAreaLiana&quot; &quot;remarks&quot; ## [87] &quot;measuredBy&quot; &quot;recordedBy&quot; ## [89] &quot;dataQF&quot; &quot;uid&quot; ## [91] &quot;namedLocation&quot; &quot;date&quot; ## [93] &quot;eventID&quot; &quot;domainID&quot; ## [95] &quot;siteID&quot; &quot;plotID&quot; ## [97] &quot;subplotID&quot; &quot;nestedSubplotID&quot; ## [99] &quot;pointID&quot; &quot;stemDistance&quot; ## [101] &quot;stemAzimuth&quot; &quot;recordType&quot; ## [103] &quot;individualID&quot; &quot;supportingStemIndividualID&quot; ## [105] &quot;previouslyTaggedAs&quot; &quot;samplingProtocolVersion&quot; ## [107] &quot;taxonID&quot; &quot;scientificName&quot; ## [109] &quot;taxonRank&quot; &quot;identificationReferences&quot; ## [111] &quot;morphospeciesID&quot; &quot;morphospeciesIDRemarks&quot; ## [113] &quot;identificationQualifier&quot; &quot;remarks&quot; ## [115] &quot;measuredBy&quot; &quot;recordedBy&quot; ## [117] &quot;dataQF&quot; Notice anything inconsistent with variableName in attributes? hint: a hump Yes you guessed it, the original fieldNames are still in camelCase. But! It also contains description and units columns! Just what we need! Mega-Challenge!! Your challenge is to successfully merge the relevant contents of variables into our attributes.csv You will need to save your merged table to data/metadata/attributes.csv. Have a look at janitor::make_clean_names() and see if you can combine it with any other functions you’ve learned to mutate the values of columns to get round the camelCase names in variables. Once you’ve completed your merge and saved it, use dataspice::edit_attributes() to fill in the final details for the few variables we created. Solution Create metadata json-ld file Now that all our metadata files are complete, we can compile it all into a structured dataspice.json file in our data/metadata/ folder. write_spice() install.packages(c(&quot;jsonlite&quot;, &quot;listviewer&quot;)) jsonlite::read_json(here::here(&quot;data&quot;, &quot;metadata&quot;, &quot;dataspice.json&quot;)) %&gt;% listviewer::jsonedit() Publishing this file on the web means it will be indexed by Google Datasets search! 😃 👍 Build README site Finally, we can use the dataspice.json file we just created to produce an informative README web page to include with our dataset for humans to enjoy!  We use function dataspice::build_site() which creates file index.html in the docs/ folder of your project (which it creates if it doesn’t already exist). build_site() View the resulting file here Here’s a screen shot! Example completed metadata files access.csv attributes.csv biblio.csv creators.csv back to the outro slides "],["analysing.html", "(PART) Analysing and presenting analyses", " (PART) Analysing and presenting analyses "],["plotting-with-ggplot2.html", "Plotting with ggplot2 Data visualisation and analysis basics Graphics in R Data visualisation in practice: Exploring individual with ggplot2", " Plotting with ggplot2 Data visualisation and analysis basics Statistical Analyses and data Statistical analysis is the science of collecting, exploring and presenting large amounts of data to discover underlying patterns and trends The theory of Statistical Analysis is NOT part of this course Rather, it is to introduce you with the computational building blocks and example workflows you can build on to develop your own analysis. The actual plots you create and statistical analyses you use will depend on the properties of the data and the questions you are trying to answer. For this, I recommend refering to many great books available on Statistics in R. A great place to start is The Elements of Statistical Learning: Data Mining, Inference, and Prediction. by Hastie, Tibshirani &amp; Friedman while there are also many more on specialist topics like: Generalized Additive Models Mixed Effects Models and Extensions in Ecology with R Statistical Rethinking: A Bayesian Course with Examples in R and Stan The foundation of any statistical analysis is DATA, most commonly, tabular data. We cannot easily establish comparative size and relationships between multiple data points from tabular data. We need a better representation to visually extract meaning. Data Visualisation: the visual encoding of data Data Visualisation is the visual encoding and presentation of data to facilitate understanding. Data properties guide visual encoding The visual encoding we use is determined by the data and the relationships and statistical properties we want to convey. You’ll find a handy guide at datavizcatalogue.com Once we’ve chosen the appropriate plots, we need tools to construct them. The grammar of grahics An abstraction which makes thinking, reasoning and communicating graphics easier. Developed by Leland Wilkinson, particularly in “The Grammar of Graphics” 1999/2005, Describes a consistent syntax for the construction of a wide range of complex graphics by a concise description of their components. Building a plot from its components. Start with some tabular data: mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Encoding variables on axes The first way we can visualise a single variable is to plot it on a single axis. For example, here’s variable mpg (miles per gallon) plotted along a single axis: We could use the same way to visualise a second variable, hp (horsepower) We can combine these two axes to visualise both the values of the data on each axis as well as the relationship of the two variables: Encoding a third variable using colour Now that we’ve used our two axes, we might want to consider another attribute to encode a third variable with. We can use colour to encode cyl (number of cylinders). Principles of good data encoding Good visualization can bring out important aspects of data, but visualization can also be used to conceal or mislead. Consistency: The properties of the image (visual variables) should match the properties of the data. Importance Ordering: Encode the most important information in the most effective way. Expressiveness: Tell the truth and nothing but the truth (don’t lie, and don’t lie by omission) Effectiveness: Use encodings that people decode better (where better = faster and/or more accurate) Check out Calling Bullshit: Data Reasoning in a Digital World Misleading axes Proportional Ink Graphics in R The R language has extensive graphical capabilities. Graphics in R may be created by many different methods including base graphics and more advanced plotting packages such as lattice and ggplot2. You’ll find a rich selection of graphs in R at The R Graph Gallery ggplot2 The ggplot2 package was created by Hadley Wickham and provides an intuitive plotting system to rapidly generate publication quality graphics. ggplot2 builds on the “Grammar of Graphics” Resources Paper on the grammar of graphics as the foundation for ggplot2 Online working draft of 3rd Edition of ggplot2: Elegant Graphics for Data Analysis ggplot2 documentation ggplot2 cheatsheet ggplot2 in practice. Plotting mtcars To demonstrate the use of ggplot2 to plot data according to the grammar of graphics, let’s recreate the mtcars plot I just showed you. Let’s work in our attic/sketchbook script for now. Initialising a plot The first thing we need to do is initialise a new plot. Function ggplot() is used to construct the initial plot object, and is almost always followed by + to add component to the plot. Let’s load the library and create an empty plot: library(ggplot2) ggplot() Specifying the data Next thing we need to do is specify the data. The first argument to ggplot() is the data we want to use for plotting, usually a data.frame or tibble. We can pipe that in to ggplot(). mtcars %&gt;% ggplot() Mapping to Aesthetics with aes() Now that we’ve specified our data, we can start encoding our variables to visual properties of our plot. Lets start by mapping mpg and hp to our two axes. We do that in the second argument mapping through function aes(). Function aes() is used to specify the set of aesthetic mappings between variables in the data and visual properties in our plot. Any aesthetics defined in ggplot(aes()) will apply to all subsequent layers unless they are overriden within the individual layers. Mapping variables to axes x and y. In our case, we want to map variable mpg to aesthetic x (the x axis) and variable hp to aesthetic y (the y axis). mtcars %&gt;% ggplot(mapping = aes(x = mpg, y = hp)) Adding geometries with geom_*() Currently, we only have our axes, initialised with reasonably infered scales from the data. Following the specification of data and aesthetics, we next need to specify which geometries (or geomss in ggplot) to use to present the data. The geom is a critical component that describes the type of plot used. Several geoms are available in ggplot2 as separate functions: geom_point() - Scatter plots geom_line() - Line plots geom_smooth() - Fitted line plots geom_bar() - Bar plots geom_boxplot() - Boxplots geom_jitter() - Jitter to plots geom_histogram() - Histogram plots geom_density() - Density plots geom_text() - Text to plots geom_errorbar() - Errorbars to plots geom_violin() - Violin plots Plotting a scatterplot To visualise the relationship between the data points in our two variables, and given both are numeric, we can plot them as points on a scatterplot using geom_point(). mtcars %&gt;% ggplot(aes(x = mpg, y = hp)) + geom_point() Mapping a third variable Now that we exhausted our first two options for visual encoding, we can use other aesthetics in our plot to map additional variables to. Example aesthetics available for geom_point() are color or colour: colour mapping. shape: mapping to symbols used for points. fill: colour mapping to shapes that have a fill attribute. size: mapping to the size of points. Let’s now map the number of cylinders cyl to the colour of points. mtcars %&gt;% ggplot(aes(x = mpg, y = hp, colour = cyl)) + geom_point() Now each point is coloured according to the value of cyl. Because cyl is numeric, the default behaviour of R is to present it on a continuous scale, hence the colour gradient legend. If we want to present cyl as a categorical variable, we can override that behaviour by turning it into a factor using factor() mtcars %&gt;% ggplot(aes(x = mpg, y = hp, colour = factor(cyl))) + geom_point() Adding a plot theme Finally, any design elements can be specified in the theme layer. Theme elements can be customised using function theme() ggplot2 also has a number of in built themes. Let’s just use one of those. mtcars %&gt;% ggplot(aes(x = mpg, y = hp, fill = factor(cyl))) + geom_point() + theme_minimal() Data visualisation in practice: Exploring individual with ggplot2 Now let’s start putting some the tools and concepts we just learned to use with our data/individual.csv data. Let’s say we are interested in exploring the relationship between individual plant stem_diameter and height. Let’s also say that, from previous knowledge, we expect that the relationship may vary according to the growth_form So lets use data visualisation to explore the properties and relationships between the three variables. Start by loading the ggplot2 as well as package magrittr so we have access to the pipe. Let’s read in the data and also narrow it down to our variables of interest using dplyr::select() library(ggplot2) library(magrittr) individual &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;individual.csv&quot;)) %&gt;% dplyr::select(stem_diameter, height, growth_form) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## date = col_date(format = &quot;&quot;), ## stem_diameter = col_double(), ## measurement_height = col_double(), ## height = col_double(), ## point_id = col_double(), ## stem_distance = col_double(), ## stem_azimuth = col_double(), ## decimal_latitude = col_double(), ## decimal_longitude = col_double(), ## easting = col_double(), ## northing = col_double(), ## elevation = col_double(), ## elevation_uncertainty = col_double(), ## stem_lat = col_double(), ## stem_lon = col_double() ## ) ## ℹ Use `spec()` for the full column specifications. Let’s also create a new .R script and save it as analysis.R in the root of our project. Copy that last section of code as the first few lines of our analysis.R script. Exploratory Data Analysis Before we start exploring the relationship between variables, it’s important to understand the statistical properties of the underlying data. Statistical summaries such as the output of summary() are a good starting point. summary(individual) ## stem_diameter height growth_form ## Min. : 1.00 Min. : 0.30 Length:14961 ## 1st Qu.: 11.60 1st Qu.: 5.30 Class :character ## Median : 16.50 Median : 10.60 Mode :character ## Mean : 20.21 Mean : 11.27 ## 3rd Qu.: 26.30 3rd Qu.: 16.20 ## Max. :373.30 Max. :119.80 ## NA&#39;s :1346 NA&#39;s :1711 But data visualisation can be an even more powerful tool in expressing statistical properties of our data. Distribution of data To begin with, we can explore the properties of individual variables, starting with the distribution of values. Let’s just work in our code attic/sketckbook.R script for now. We’ll transfer final plots to analysis.R when we are happy with them. Distribution of stem_diameter Lets start with stem_diameter which is a numeric variable. As such we can use geom_density to plot the distribution of the data. individual %&gt;% ggplot(aes(x = stem_diameter)) + geom_density() ## Warning: Removed 1346 rows containing non-finite values (stat_density). The distribution across all our data looks quite skewed towards lower values and a lomg tail of larger values and even shows a small dip in a certain range of stem_diameter. Many statistical tests assume normality of the data and a common tranformation to such skewed numeric data might be to log them. ggplot allows us to perform such tranformations during plotting and prints it as part of the variable axes label. individual %&gt;% ggplot(aes(x = log(stem_diameter))) + geom_density() ## Warning: Removed 1346 rows containing non-finite values (stat_density). Still rather wonky and may well violate assumptions of statistical tests down the line but it is slightly better than before so just for demonstration purposes, let’s carry own presenting our data on a log scale. Applying aesthetics to geometries One thing that can make such plots visually more appealing is to fill in the area under the distribution curve. This also gives us the opportunity to dig into what exactly the aesthetic mapping in aes() is doing. Let’s say we wanted to fill in the area with the colour grey. Our first instinct might me to use aes() in ggplot() and map aesthetic fill to the name of the colour \"grey\". individual %&gt;% ggplot(aes(x = log(stem_diameter), fill = &quot;grey&quot;)) + geom_density() ## Warning: Removed 1346 rows containing non-finite values (stat_density). But this has unexpected results! This happens because we are supplying the colour name within aes(). aes() is for mapping variables to aesthetics and expects a variable name in our data or a vector of values. Here we are giving it a single character value which it converts to a factor. It then uses it’s default function for creating categorical colour scales to assign the first colour in that scale (red) to our single factor level \"grey\". The important point here is that R is not interpretting \"grey\" as a colour, but as a categorical variable, as it did for factor(cyl). To specify explicitly the fill colour of our density geom, we instead supply it as an argument to our geom_density() function, outside of aes(). Let’s change both the colour of the line and fill to \"grey\". individual %&gt;% ggplot(aes(x = log(stem_diameter))) + geom_density(colour = &quot;grey&quot;, fill = &quot;grey&quot;) ## Warning: Removed 1346 rows containing non-finite values (stat_density). Distribution of height We can similarly create a density plot for height individual %&gt;% ggplot(aes(x = height)) + geom_density(colour = &quot;grey&quot;, fill = &quot;grey&quot;) ## Warning: Removed 1711 rows containing non-finite values (stat_density). It’s also a bit skewed so lets go ahead and work with log() values again. individual %&gt;% ggplot(aes(x = log(height))) + geom_density(colour = &quot;grey&quot;, fill = &quot;grey&quot;) ## Warning: Removed 1711 rows containing non-finite values (stat_density). Distribution of growth_form In contrast to stem_diameter and height, growth_form is a categorical variable. As such, we use geom_bar() to plot a barplot of the counts of values of each growth_form in our data. A barplot plots categorical data across the x axes and numeric data (in this case counts on the y axis). Let’s also map colour aesthetics to growth form. individual %&gt;% ggplot(aes(x = growth_form, colour = growth_form, fill = growth_form)) + geom_bar() We can see that there are very few entries for liana and also a whole bunch of NAs in growth_form. So let’s remove any such rows from our data by using dplyr::filter(). Let’s assign this new data to a new analysis_df and work with that from now on. analysis_df &lt;- individual %&gt;% dplyr::filter(!is.na(growth_form), growth_form != &quot;liana&quot;) Let’s also order our bars in order of ascending counts. The simplest way to do this is to convert growth_form to a factor and specify the ordering of the factor levels. To do that let’s create a vector of growth_form unique values, ordered according to their counts in the raw data. We can do this by using table to get the counts, order to order them in ascending order and names to extract the names! gf_levels &lt;- table(analysis_df$growth_form) %&gt;% sort() %&gt;% names() We can then specify the level order when we mutate growth_form to a factor using gf_levels through argument levels in factor. analysis_df %&lt;&gt;% dplyr::mutate(growth_form = factor(growth_form, levels = gf_levels)) Let’s move those data preparation steps to analysis.R Let’s have a look at our barplot now: analysis_df %&gt;% ggplot(aes(x = growth_form, colour = growth_form, fill = growth_form)) + geom_bar() The order of the levels in growth_form now dictates the order in which the bars are plotted! Finally, let’s just add a few extra touches to make the plot even more visually clear. Let’s flip the axis by mapping growth_form to y, that prevents growth_form axes labels from overlapping. Let’s also remove the superfluous legend and reduce the opacity of our bars so we can see the scales through them analysis_df %&gt;% ggplot(aes(y = growth_form, colour = growth_form, fill = growth_form)) + geom_bar(alpha = 0.5, show.legend = FALSE) That’s better. Let’s keep this plot and move it to our analysis script as fig 1. Plotting multiple densities according to the values of a second variable When plotting the density distributions we ended up having one plot per variable with little understanding of how values where distributed across the various growth forms. However, ggplot and the grammar of graphics allows us to build more informative plots, by combining the information in categorical variables to present cross variable distributions. The grammar of graphics allows to easily plot the same properties of interest across groups dictated by other aesthetics. Let’s explore what this means by focusing on the distribution of stem_diameter across the growth_form categories in our sketchbook.R. We can plot out a separate density curve for each growth form by mapping categorical variable grow_form to aes() argument group. analysis_df %&gt;% ggplot(aes(x = log(stem_diameter), group = growth_form)) + geom_density() ## Warning: Removed 1031 rows containing non-finite values (stat_density). Now we have separate distribution curves for each growth_form! This first pass is not however visually easy to interpret. Let’s assign growth_form to some additional aesthetics to make the visual encoding clearer. We can in fact get rid of the group argument and use fill and colour instead. The grouping behaviour is equivalent. analysis_df %&gt;% ggplot(aes(x = log(stem_diameter), colour = growth_form, fill = growth_form)) + geom_density() ## Warning: Removed 1031 rows containing non-finite values (stat_density). To make things even clearer we can supply additional argument to geom_density. Let’s decrease the opacity of each density geom and trim it to the ranges of values across each growth_form. analysis_df %&gt;% ggplot(aes(x = log(stem_diameter), colour = growth_form, fill = growth_form)) + geom_density(alpha = 0.5, trim = TRUE) ## Warning: Removed 1031 rows containing non-finite values (stat_density). That’s a lot clearer, and now we can see that overall the distribution of log(stem_diameter) across growth forms follows a broably bimodal distribution. However, the distributions across each growth formal appear more normal. To allow us to focus more on the individual distributions, we can use facet_wrap() and formula ~growth_form to create individual panels for each growth form on a grid: analysis_df %&gt;% ggplot(aes(x = log(stem_diameter), colour = growth_form, fill = growth_form)) + geom_density(alpha = 0.5, trim = TRUE) + facet_wrap(~growth_form) ## Warning: Removed 1031 rows containing non-finite values (stat_density). Encoding multiple distribution with violin plots Another way we can present the distribution of a continuous variable in a compact way and grouped according to the value of a categorical variable is to use a violin plot. Let’s straight away add some colour aesthetics also. analysis_df %&gt;% ggplot(aes(x = log(stem_diameter), y = growth_form, colour = growth_form, fill = growth_form)) + geom_violin(alpha = 0.5, trim = T) ## Warning: Removed 1031 rows containing non-finite values (stat_ydensity). Adding statistical summaries with geom_boxplot() We can go a step further and add statistical information about our variable by overlaying a boxplot (or box and whiskers plot). The boxplot compactly displays the distribution of a continuous variable by visualising five key summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually. Let’s also suppress the legend for the box plot layer and reduce the opacity. analysis_df %&gt;% ggplot(aes(x = log(stem_diameter), y = growth_form, colour = growth_form, fill = growth_form)) + geom_violin(alpha = 0.5, trim = T) + geom_boxplot(alpha = 0.7, show.legend = FALSE) ## Warning: Removed 1031 rows containing non-finite values (stat_ydensity). ## Warning: Removed 1031 rows containing non-finite values (stat_boxplot). The central line in each box corresponds to the median. The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles) and define the Interquartile range (IQR). The whiskers are calculated from the IQR and identify points considered statistical outliers. Again, here we can see the bimodal distribution across all values, the two loose groups of multibole trees, single bole trees and small trees, but this time we have a much more informative and compact plot. We would still need a plot per variable though. We could try and utilise facet_grid to combine a boxplot for each continuous variable stem_diameter and height into a single plot. Hovever, there isn’t a variable in the data in the current form that could be used to facet on. The data is instead held acroos two separate columns, stem_diameter and height. Pivoting data to longer To take advantage of facet_wrap we need to pivot our data into a longer format using pivot_longer from package tidyr. pivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns. We use it to stack the values of stem_diameter and height into a column called value and store the original column names which define the variable each value relates to in a new column var. The values of growth_form are duplicated and stacked. analysis_df %&gt;% tidyr::pivot_longer(cols = c(stem_diameter, height), names_to = &quot;var&quot;, values_to = &quot;value&quot;) ## # A tibble: 27,776 x 3 ## growth_form var value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 single bole tree stem_diameter 17.1 ## 2 single bole tree height 15.2 ## 3 single bole tree stem_diameter 13.7 ## 4 single bole tree height 9.8 ## 5 single bole tree stem_diameter 12.3 ## 6 single bole tree height 7.7 ## 7 single bole tree stem_diameter 12.1 ## 8 single bole tree height 15.2 ## 9 single bole tree stem_diameter 29.2 ## 10 single bole tree height 16.7 ## # … with 27,766 more rows Note that the number of rows is now twice the size of the original data because two columns have been stacked. Those columns have also now been removed. With data in this format, we can use variable var to create a facet for each variable. Figure 2: Data characteristics of our raw data Let’s add the pivot as a step in our plotting pipe analysis_df %&gt;% tidyr::pivot_longer(cols = c(stem_diameter, height), names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = log(value), y = growth_form, colour = growth_form, fill = growth_form)) + geom_violin(alpha = 0.5, trim = T) + geom_boxplot(alpha = 0.7, show.legend = FALSE) + facet_grid(~var) ## Warning: Removed 2388 rows containing non-finite values (stat_ydensity). ## Warning: Removed 2388 rows containing non-finite values (stat_boxplot). Hurray! Now we have a super informative plot, containing visual encodings of distributions and statistical summaries for both our continuous variables in one compact plot! Let’s move that to analysis.R as Figure 2 Analysis in practice: fitting and visualising a simple linear model Now that we’ve explored our variables, it’s time to start looking at the statistical relationship between them. Analysing the relationship between log(stem_diameter) and log(height) First we might want to look at the overall relationship between our two continuous variables and we can start with fitting a simple linear regression model. In R, we use lm to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of covariance. Fitting a linear model We specify our model through a formula log(stem_diameter) ~ log(height). This translates to log stem diameter as a function of height where stem_diameter is the response variable and height the predictor. lm_overall &lt;- lm(log(stem_diameter) ~ log(height), analysis_df) lm_overall ## ## Call: ## lm(formula = log(stem_diameter) ~ log(height), data = analysis_df) ## ## Coefficients: ## (Intercept) log(height) ## 0.5609 0.9439 By default lm prints a rather terse summary of the model. An easy way to print nice and tidy outputs of most models in R is by using functions from package broom. lm_overall %&gt;% broom::glance() ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.679 0.679 0.487 24613. 0 1 -8132. 16271. 16293. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Glance accepts a model object and returns a tibble with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information. lm_overall %&gt;% broom::tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.561 0.0145 38.7 5.36e-308 ## 2 log(height) 0.944 0.00602 157. 0 Tidy summarizes information about the components of a model. In the case of a linear model, components are the parameters associated with a regression i.e. the intercept and slope. Visualising our overall model To plot the relationship that the lm has fit, we plot a scatterplot using geom_point() and map log(height) to x (the predictor) and log(stem_diameter) to y (the response). We can also add a line to our plot using geom_smooth(). This plots a smooth over the data by default but can use method lm to plot lines using a linear model. analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2262 rows containing non-finite values (stat_smooth). ## Warning: Removed 2262 rows containing missing values (geom_point). Including an interaction with growth_form Inspecting the plot we can clearly see sub groups in our data. We already know that both our variables have very different distributions across growth_form. So let’s see if our model improves if we include growth_form in our model specification. Growth form is a categorical variable so when we include it in our regression, lm will fit separate coefficients for our model at every level of the factor. To include it, we add it to the predictor side of our formula. If we include it as an additive effect through +, only the intercept will vary across factor levels. If we fit it as an interaction using * both the slope and intercept are allowed to vary. lm_growth &lt;- lm(log(stem_diameter) ~ log(height) * growth_form, analysis_df) We can again examine our model using broom lm_growth %&gt;% broom::glance() ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.799 0.799 0.386 4195. 0 11 -5418. 10862. 10957. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; We can see that model coverage as indicated by r.squared is now higher and the p.value is still significant lm_growth %&gt;% broom::tidy() ## # A tibble: 12 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.187 0.0957 1.96 5.02e- 2 ## 2 log(height) 0.952 0.0490 19.4 1.08e-82 ## 3 growth_formsapling 0.223 0.0992 2.25 2.46e- 2 ## 4 growth_formsingle shrub -0.690 0.109 -6.34 2.44e-10 ## 5 growth_formsmall shrub 0.255 0.107 2.38 1.73e- 2 ## 6 growth_formmulti-bole tree 1.16 0.103 11.2 4.57e-29 ## 7 growth_formsingle bole tree 1.38 0.0978 14.1 1.27e-44 ## 8 log(height):growth_formsapling -0.643 0.0778 -8.27 1.45e-16 ## 9 log(height):growth_formsingle shrub 0.251 0.0645 3.89 1.00e- 4 ## 10 log(height):growth_formsmall shrub -0.561 0.151 -3.73 1.93e- 4 ## 11 log(height):growth_formmulti-bole tree -0.316 0.0519 -6.08 1.22e- 9 ## 12 log(height):growth_formsingle bole tree -0.376 0.0497 -7.57 3.88e-14 We see the model coefficients for each growth form slope and interaction. The first 2 row show the intercept and slope for the first level of growth_form ie small tree which is considered the reference level. The rest of the rows show the intercepts and slopes with respect to the values of the coeffients for small tree so they represent differences from the reference level, level 1. Visualising our model To include the interaction with growth_form we apply a grouping to our scatterplot through aesthetic colour. analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter), colour = growth_form)) + geom_point(alpha = 0.1) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2262 rows containing non-finite values (stat_smooth). ## Warning: Removed 2262 rows containing missing values (geom_point). Excellent! We now have specified both models and visualised them! Our analysis is complete. NOTE: the geom_smooth method of plotting model lines is not ideal and you will likely take more formal approaches to calculating and plotting confidence intervals. But for the purposes of of this workshop, we’ll stick with this. Lets add all our models and plots to our analysis.R script. Lets also add comment sections with machine readable names to our analysis.R script. Your analysis.R script should now look like this. # analysis-setup library(ggplot2) library(magrittr) individual &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;individual.csv&quot;)) %&gt;% dplyr::select(stem_diameter, height, growth_form) # analysis-filter-data analysis_df &lt;- individual %&gt;% dplyr::filter(!is.na(growth_form), growth_form != &quot;liana&quot;) # analysis-set-factor-levels gf_levels &lt;- table(analysis_df$growth_form) %&gt;% sort(decreasing = TRUE) %&gt;% names() analysis_df %&lt;&gt;% dplyr::mutate(growth_form = factor(growth_form, levels = gf_levels)) # analysis-fig1-barplot analysis_df %&gt;% ggplot(aes(y = growth_form, colour = growth_form, fill = growth_form)) + geom_bar(alpha = 0.5, show.legend = FALSE) # analysis-fg2-violinplots analysis_df %&gt;% tidyr::pivot_longer(cols = c(stem_diameter, height), names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = log(value), y = growth_form, colour = growth_form, fill = growth_form)) + geom_violin(alpha = 0.5, trim = T, show.legend = FALSE) + geom_boxplot(alpha = 0.7, show.legend = FALSE) + facet_grid(~var) # analysis-lm-overall lm_overall &lt;- lm(log(stem_diameter) ~ log(height), analysis_df) lm_overall %&gt;% broom::glance() lm_overall %&gt;% broom::tidy() # analysis-lm-fig3-overall analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) # analysis-lm-growth lm_growth &lt;- lm(log(stem_diameter) ~ log(height) * growth_form, analysis_df) lm_growth %&gt;% broom::glance() lm_growth %&gt;% broom::tidy() # analysis-lm-fig4-growth analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter), colour = growth_form)) + geom_point(alpha = 0.1) + geom_smooth(method = &quot;lm&quot;) "],["literate-programming-in-rmarkdown.html", "Literate Programming in rmarkdown Literate programming Literate programming in R Applications of Rmd in research Rmd in Practice Advanced .Rmd Getting help with markdown Resources", " Literate Programming in rmarkdown Literate programming Literate programming is a programming paradigm first introduced by Donald E. Knuth. Treat program as literature meant to be understandable to human beings move away from writing programs in the manner and order imposed by the computer focus instead on the logic and flow of human thought and understanding single document to integrate data analysis (executable code) with textual documentation, linking data, code, and text Why is this important in science: Enables reproducibility Enables reproducibility through packaging code, text and output into a single executable document. Enables open science Literate programming allows us to record and publish information and discussions about code and analyses in more readable formats. … highlight problems with users jumping straight into software implementations of methods (e.g. in r) that may lack documentation on biases and assumptions that are mentioned in the original papers. To help solve these problems, we make a number of suggestions including providing blog posts or videos to explain new methods in less technical terms, encouraging reproducibility and code sharing, making wiki-style pages summarising the literature on popular methods, more careful consideration and testing of whether a method is appropriate for a given question/data set, increased collaboration, and a shift from publishing purely novel methods to publishing improvements to existing methods and ways of detecting biases or testing model fit. Many of these points are applicable across methods in ecology and evolution, not just phylogenetic comparative methods. Literate programming in R Rmarkdown (Rmd) Overview Rmarkdown integrates: – a documentantion language (.md) – a programming language (R) Allows us to combine tools, processes and outputs into interactive evidence streams that are easily shareable, particularly through the web. A researchers perspective on Rmarkdown A reproducible workflow in action: Elements of R markdown markdown {.md} Simplified version of the html markup language. User can focus on communicating &amp; disseminating intended to be as easy-to-read and easy-to-write as possible. most powerful as a format for writing to the web. syntax is very small, corresponding only to a very small subset of HTML tags. clean and legible across platforms (even mobile) and outputs. formatting handled automatically html markup language also handled. code {r, python, SQL, … } Code chunks defined through special notation. Executed in sequence. Exceution of individual chunks controllable Analysis self-contained and reproducible Run in a fresh R session every time document is knit. A number of Language Engines are supported by knitr R (default) Python SQL Bash Rcpp Stan JavaScript CSS Can read appropriately annotated .R scripts in and call them within an .Rmd outputs Knit together through package knitr to a variety of useful formats Many great packages and applications build on rmarkdown. All this makes it incredibly versatile. Check out the Rmarkdown gallery. Superpower: Simple interface to powerful modern web technologies and libraries Publish to the web for free! RPubs: Publish rendered rmarkdown documents on the web with the click of a button http://rpubs.com/ GitHub: Host your site through gh-pages on GitHub. Can host entire websites, like this course material https://github.com/ Applications of Rmd in research Rmd documents can be useful for a number of research related materials Vignettes: long form documentation. Analyses Documentation (code &amp; data) Supplementary materials Reports Papers Personal research websites and blogs Theses! Useful features through bookdown: bibliographies and citations Rmd in Practice Exercise Part 1 We will use Rmarkdown to create a reproducible presentation document for our analysis, combining our code and it’s output with text in the form of markdown! Later, once we start working with git and GitHub, we’ll also publish it online. Create your first .Rmd! File &gt; New File &gt; RMarkdown… &gt; Document Save as index.Rmd Before knitting, the document needs to be saved. Give it a useful name, e.g. index.Rmd. Index has a special meaning for websites indicating the landing page of a given url. Render index.Rmd Render the document by clicking on the knit button. You can also render .Rmd documents to html using rmarkdown function render() rmarkdown::render(input = &quot;index.Rmd&quot;) Publish your .Rmd Register an account on RPubs Publish your rendered document (don’t worry, you can delete or overwrite it later) Complete YAML header The yaml header contains metadata about the document. It is contained between the --- separators at the top of the file and is encoded as YAML, a human friendly data serialization standard for all programming languages. The key thing to know about YAML is that indentation is extremely important!. So make sure you copy any example YAML code exactly, ensuring correct indentation. If you get errors, check your indentation. --- title: &quot;Untitled&quot; author: &quot;Anna Krystalli&quot; date: &quot;3/23/2018&quot; output: html_document --- Define outputs There are many output formats available in .Rmd. The output format is defined in the YAML header through parameter output. basic html_document Markdown was originally designed for HTML output, so it may not be surprising that the HTML format has the richest features among all output formats. Here we’ll be focusing on on the html_document output. This is the default output when creating a new blank .Rmd. title: &quot;Untitled&quot; author: &quot;Anna Krystalli&quot; date: &quot;3/23/2018&quot; output: html_document Add a floating table of contents We can add a table of contents (TOC) using the toc option and specify a floating toc using the toc_float option. For example: --- title: &quot;Untitled&quot; author: &quot;Anna Krystalli&quot; date: &quot;3/23/2018&quot; output: html_document: toc: true toc_float: true --- Choose a theme There are several options that control the appearance of HTML documents: theme specifies the Bootstrap theme to use for the page (themes are drawn from the Bootswatch theme library). Valid themes include default, cerulean, journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, and yeti. --- title: &quot;Untitled&quot; author: &quot;Anna Krystalli&quot; date: &quot;3/23/2018&quot; output: html_document: toc: true toc_float: true theme: cosmo --- Choose code highlights highlight specifies the syntax highlighting style. Supported styles include default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, and textmate. --- title: &quot;Untitled&quot; author: &quot;Anna Krystalli&quot; date: &quot;3/23/2018&quot; output: html_document: toc: true toc_float: true theme: cosmo highlight: zenburn --- Exercise Part 2 Customise your own Rmd Add a title. Add your name. Add a floating table of contents. Set a theme of your choice (see avalable themes here and the associated bootstrap styles here). Clear everything BELOW THE YAML header. You should be left with just this: --- title: &quot;Analysis of NEON Woody plant vegetation structure data&quot; author: &quot;Anna Krystalli&quot; date: &quot;3/23/2018&quot; output: html_document: toc: true toc_float: true theme: cosmo highlight: zenburn --- Markdown basics The text in an R Markdown document is written with the Markdown syntax. Precisely speaking, it is Pandoc’s Markdown. We use a small number of notations to markup our text with some common html tags text normal text normal text *italic text* italic text **bold text** bold text ***bold italic text*** bold italic text superscript^2^ superscript2 ~~strikethrough~~ strikethrough headers rmarkdown # Header 1 ## Header 2 ### Header 3 #### Header 4 ##### Header 5 ###### Header 6 rendered html unordered lists rmarkdown - first item in the list - second item in list - third item in list rendered html first item in the list second item in list third item in list ordered lists rmarkdown 1. first item in the list 1. second item in list 1. third item in list rendered html first item in the list second item in list third item in list quotes rmarkdown &gt; this text will be quoted rendered html this text will be quoted code annotate code inline rmarkdown `this text will appear as code` inline rendered html this text will appear as code inline Evaluate r code inline a &lt;- 10 rmarkdown the value of parameter *a* is `r a` rendered html the value of parameter a is 10 Images Provide either a path to a local image file or the URL of an image. rmarkdown ![](assets/cheat.png) rendered html resize images with html html in rmarkdown &lt;img src=&quot;assets/cheat.png&quot; width=&quot;200px&quot; /&gt; rendered html Basic tables in markdown rmarkdown Table Header | Second Header - | - Cell 1 | Cell 2 Cell 3 | Cell 4 rendered html Table Header Second Header Cell 1 Cell 2 Cell 3 Cell 4 Check out handy online .md table converter Links rmarkdown [Download R](http://www.r-project.org/) [RStudio](http://www.rstudio.com/) rendered html Download R RStudio Mathematical expressions Supports mathematical notations through MathJax. You can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs: $$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$ \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\] 💻 Exercise: Part 3 Create a \"Background\" section using headers Write a short description of the data set and analysis Write a short description of the NEON Woody plant vegetation structure dataset and the objective of the analysis. Have a look at the page on the NEON data portal for inspiration. Make use of markdown annotation to: highlight important information include links to sources or further information. Add an image Add an image related to the data. perhaps a logo or a relevant image to the organisms in question have a look online, especially on sites like unsplash that offer free to use images. include the source URL underneath for attribution. see if you can resize it. Chunks &amp; Inline R code R code chunks execute code. They can be used as a means to perform computations, render R output like text, tables, or graphics into documents or to simply display code for illustration without evaluating it. Inserting new chunks You can quickly insert an R code chunk with: the keyboard shortcut Ctrl + Alt + I (OS X: Cmd + Option + I) the Add Chunk command in the RStudio toolbar by typing the chunk delimiters ```{r} and ```. Chunk notation chunk notation in .rmd ```{r chunk-name} print(&#39;hello world!&#39;) ``` rendered html code and output print(&#39;hello world!&#39;) ## [1] &quot;hello world!&quot; Chunks can be labelled with chunk names, names must be unique. ```{r chunk-label} Chunk options Chunk options control how code and outputs are evaluated and presented. You have fine control over all these output via chunk options, which can be provided inside the curly braces (between ```{r and }). Chunk options are separated by commas, e.g.: ```{r, results=&#39;hide&#39;, fig.height=4} for more details see http://yihui.name/knitr/ controlling code display with echo chunk notation in .rmd ```{r hide-code, echo=FALSE} print(&#39;hello world!&#39;) ``` rendered html code and output ## [1] &quot;hello world!&quot; controlling code evaluation with eval chunk notation in .rmd ```{r dont-eval, eval=FALSE} print(&#39;hello world!&#39;) ``` rendered html code and output print(&#39;hello world!&#39;) setting document level default options knitr::opts_chunk$set(echo = TRUE, warning = F, message = F) reading chunks of code from an R script R -&gt; Rmd We can also read in chunks of code from an annotated .R (or any other language) script using knitr::read_chunks() This is extremely powerful beacuse it means we can recycle code and more importantly, present an analysis while maintaining a single copy of the source code in a separate script. Marking up chunks in a script Chunks are defined by the following notation in scripts ## @knitr. Names must be unique and there must be no empty code lines or other comments in between. ## @knitr descriptive-chunk-name1 code(&quot;you want to run as a chunk&quot;) ## @knitr descriptive-chunk-name2 - code(&quot;you want to run as a chunk&quot;) Chunks are read in using: knitr::read_chunk(&quot;path/to/Rscript&quot;) Call chunk by name Once the script is read in with knitr::read_chunk(), we can call individual chunks using their name. rmarkdown r chunk notation ```{r descriptive-chunk-name1} ``` read_chunk example code in .R script hello-world.R hello-world.R ## @knitr demo-read_chunk print(&quot;hello world&quot;) Read chunks from hello-world.R knitr::read_chunk(here::here(&quot;demos&quot;,&quot;hello-world.R&quot;)) Call chunk by name rmarkdown r chunk notation ```{r demo-read_chunk} ``` rendered html code and output print(&quot;hello world&quot;) ## [1] &quot;hello world&quot; Check chunks in the current session knitr:::knit_code$get() 💻 Exercise Part 4 For this exercise we’ll create a Data section, load our data in and present some summaries. We’ll use code contained in our analysis script. Annotate chunks in analysis.R script Annotate each chunk with the special comment ## @knitr followed by a space and the unique code chunk name. When finished, your script should look like: ## @knitr analysis-setup library(ggplot2) library(magrittr) individual &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;individual.csv&quot;)) %&gt;% dplyr::select(stem_diameter, height, growth_form) ## @knitr analysis-filter-data analysis_df &lt;- individual %&gt;% dplyr::filter(!is.na(growth_form), growth_form != &quot;liana&quot;) ## @knitr analysis-set-factor-levels gf_levels &lt;- table(analysis_df$growth_form) %&gt;% sort(decreasing = TRUE) %&gt;% names() analysis_df %&lt;&gt;% dplyr::mutate(growth_form = factor(growth_form, levels = gf_levels)) ## @knitr analysis-fig1-barplot analysis_df %&gt;% ggplot(aes(y = growth_form, colour = growth_form, fill = growth_form)) + geom_bar(alpha = 0.5, show.legend = FALSE) ## @knitr analysis-fg2-violinplots analysis_df %&gt;% tidyr::pivot_longer(cols = c(stem_diameter, height), names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = log(value), y = growth_form, colour = growth_form, fill = growth_form)) + geom_violin(alpha = 0.5, trim = T, show.legend = FALSE) + geom_boxplot(alpha = 0.7, show.legend = FALSE) + facet_grid(~var) ## @knitr analysis-lm-overall lm_overall &lt;- lm(log(stem_diameter) ~ log(height), analysis_df) lm_overall %&gt;% broom::glance() lm_overall %&gt;% broom::tidy() ## @knitr analysis-lm-fig3-overall analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) ## @knitr analysis-lm-growth lm_growth &lt;- lm(log(stem_diameter) ~ log(height) * growth_form, analysis_df) lm_growth %&gt;% broom::glance() lm_growth %&gt;% broom::tidy() ## @knitr analysis-lm-fig4-growth analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter), colour = growth_form)) + geom_point(alpha = 0.1) + geom_smooth(method = &quot;lm&quot;) Create a Data section and Create a Data section using headers. In a hiden chunk of code, read in the chunks from analysis.R using knitr::read_chunk(). Load the data by creating a new chunk invoking the analysis-setup chunk. Create some additional chunks and present statistical summaries (eg using summary()). Add some simple markdown notes about what the code has been doing. See if you can include the number of rows and columns in an inline description of the dataset (hint: use nrow() &amp; ncol(). Displaying data There are many ways you can display data and data properties in an .Rmd. printing tibbless tibble::as_tibble(airquality) ## # A tibble: 153 x 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 41 190 7.4 67 5 1 ## 2 36 118 8 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 10 NA 194 8.6 69 5 10 ## # … with 143 more rows Displaying knitr::kable() tables We can use other packages to create html tables from our data. The simplest is to use the knitr::kable() function. knitr::kable(head(airquality), caption = &quot;New York Air Quality Measurements&quot;) Table 1: New York Air Quality Measurements Ozone Solar.R Wind Temp Month Day 41 190 7.4 67 5 1 36 118 8.0 72 5 2 12 149 12.6 74 5 3 18 313 11.5 62 5 4 NA NA 14.3 56 5 5 28 NA 14.9 66 5 6 Displaying interactive DT::datatable() tables You can display interactive html tables using function DT::datatable(): DT::datatable(airquality, caption = &quot;New York Air Quality Measurements&quot;) 💻 Exercise Part 5 Display the data Choose any method you prefer and create a new chunk to display the data in the Data section. 💻 Exercise Part 6 Write the analysis section Create a new section called Analysis. Create chunks and invoke the rest of the steps in our analysis. Use lower level headers to break down the workflow and include some minimal explanatory text. Add a caption to each figure. Experiment with controlling figure output width or height using chunk options fig.width or fig.height. Details on chunk arguments related to plotting End the document with a Session Info Section Create a new section called Session Info Create a new chunk and use function sessionInfo() to print session information about your analysis environment My final index.Rmd looks like this: --- title: &quot;Analysis of NEON Woody plant vegetation structure data&quot; author: &quot;Anna Krystalli&quot; date: &quot;06/05/2020&quot; output: html_document: toc: true toc_float: true theme: cosmo highlight: zenburn --- # Background &lt;img src=&quot;https://data.neonscience.org/data-products/static/media/NSF-NEON-logo.192b6661.png&quot; width=&quot;40%&quot;&gt; The [NEON Woody plant vegetation structure dataset](https://data.neonscience.org/data-products/DP1.10098.001) contains structure measurements, including height, canopy diameter, and stem diameter, as well as mapped position of individual woody plants across the survey area. This data product contains the quality-controlled, native sampling resolution data from in-situ measurements of live and standing dead woody individuals and shrub groups, from all terrestrial NEON sites with qualifying woody vegetation. The exact measurements collected per individual depend on growth form, and these measurements are focused on enabling biomass and productivity estimation, estimation of shrub volume and biomass, and calibration / validation of multiple NEON airborne remote-sensing data products. Our analyses focus on the **relationship between individual stem height and diameter** and how that relationship **varies across growth forms**. # Data The data were downloaded from the NEON data portal and processed into a single table using script `data-raw/individual.R` ```{r, echo=FALSE} knitr::read_chunk(here::here(&quot;analysis.R&quot;)) ``` ### Read in data and setup analysis First we read in the data and select only the columns we are interested in, i.e `stem_diameter`, `height` and `growth_form` ```{r analysis-setup, message=FALSE} ``` ### Prepare data To prepare the data we exclude rows for which the value of `growth_form` was `NA` or `liana`. ```{r analysis-filter-data} ``` We also convert `growth_form` to a factor and set the levels according to to ascending counts of each level in the raw data. ```{r analysis-set-factor-levels} ``` ```{r} DT::datatable(analysis_df, caption = &quot;Table 1: Prepared analysis data&quot;) ``` Our prepared data is a tibble of `r ncol(analysis_df)` columns and `r nrow(analysis_df)` rows. ### Data properties #### Statistical summaries of our variables ```{r} summary(analysis_df) ``` ```{r analysis-fig1-barplot, fig.cap=&quot;Figure 1: Counts of growth forms&quot;} ``` ```{r analysis-fg2-violinplots, fig.cap=&quot;Figure 2: Distribution and statistical summaries of stem_diameter and height across growth forms.&quot;} ``` ## Analysis ### Modelling overall `stem_diameter` as a function of `height` Initially we fit a linear model of form `log(stem_diameter)` as a function of `log(height)` ```{r analysis-lm-overall} ``` Our model is statistically significant and has modest coverage, indicated by `r.squared` of `r broom::glance(lm_overall)$r.squared` ```{r analysis-lm-fig3-overall, fig.cap=&quot;Figure 3: Log stem diameter as a function of log height&quot;} ``` However, plotting our data reveals sub groups in the data. We can examine whether including `growth_form` in our analysis would improve our model fit by capturing variation explained by differing relationships across growth forms ### Including an interaction with `growth_form` We fit another model, this time including an interaction term for variable `growth_form` ```{r analysis-lm-growth} ``` Our model is still significant but this time explains a larger proportion of the variation (`r broom::glance(lm_growth)$r.squared`). ```{r analysis-lm-fig4-growth, fig.cap=&quot;Figure 4: Log stem diameter as a function of the interaction of log height and growth_form&quot;} ``` ## Session info ```{r} sessionInfo() ``` and renders to: Exercise Part 7 Publish your work Publish your report on Rpubs Add the link to your published document to our collaborative notepad Advanced .Rmd Extracting code from an .Rmd Rmd -&gt; R You can use knitr::purl() to tangle code out of an Rmd into an .R script. purl takes many of the same arguments as knit(). The most important additional argument is: documentation: an integer specifying the level of documentation to go the tangled script: 0 means pure code (discard all text chunks) 1 (default) means add the chunk headers to code 2 means add all text chunks to code as roxygen comments purl(&quot;file-to-extract-code-from.Rmd&quot;, documentation = 0) extract using purl Here i’m running a loop to extract the code in demo-rmd.Rmd for each documentation level file &lt;- here::here(&quot;demos&quot;,&quot;demo-rmd.Rmd&quot;) for(docu in 0:2){ knitr::purl(file, output = paste0(gsub(&quot;.Rmd&quot;, &quot;&quot;, file), &quot;_&quot;, docu, &quot;.R&quot;), documentation = docu, quiet = T) } demo-rmd_0.R knitr::opts_chunk$set(echo = TRUE) summary(cars) plot(pressure) demo-rmd_1.R ## ----setup, include=FALSE------------------------------------------------ knitr::opts_chunk$set(echo = TRUE) ## ----cars---------------------------------------------------------------- summary(cars) ## ----pressure, echo=FALSE------------------------------------------------ plot(pressure) demo-rmd_2.R #&#39; --- #&#39; title: &quot;Untitled&quot; #&#39; author: &quot;Anna Krystalli&quot; #&#39; date: &quot;3/23/2018&quot; #&#39; output: #&#39; html_document: #&#39; toc: true #&#39; toc_float: true #&#39; theme: cosmo #&#39; highlight: textmate #&#39; #&#39; --- #&#39; ## ----setup, include=FALSE------------------------------------------------ knitr::opts_chunk$set(echo = TRUE) #&#39; #&#39; ## R Markdown #&#39; #&#39; #&#39; This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;http://rmarkdown.rstudio.com&gt;. #&#39; #&#39; When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: #&#39; ## ----cars---------------------------------------------------------------- summary(cars) #&#39; #&#39; ## Including Plots #&#39; #&#39; You can also embed plots, for example: #&#39; ## ----pressure, echo=FALSE------------------------------------------------ plot(pressure) #&#39; #&#39; Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. #&#39; #&#39; 💻 Exercise: Part 8* purl your document Once your document is ready, try and extract the contents of your .Rmd into another .R script. ?purl html in rmarkdown marking up with html tags This text marked up in html &lt;strong&gt;Bold text&lt;/strong&gt; renders to this Bold text **This text marked up with Bootstrap alert css classes &lt;div class=&quot;alert alert-warning&quot;&gt;&lt;small&gt;this a is warning message&lt;/small&gt;&lt;/div&gt; renders to this a is warning message &lt;div class=&quot;alert alert-success&quot;&gt;&lt;small&gt;this a is success message&lt;/small&gt;&lt;/div&gt; renders to this a is success message embedding tweets This snipped copied from twitter in the embed format &lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;How cool does this tweet look embedded in &lt;a href=&quot;https://twitter.com/hashtag/rmarkdown?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#rmarkdown&lt;/a&gt;! 😎&lt;/p&gt;&amp;mdash; annakrystalli (@annakrystalli) &lt;a href=&quot;https://twitter.com/annakrystalli/status/977209749958791168?ref_src=twsrc%5Etfw&quot;&gt;March 23, 2018&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; renders to this How cool does this tweet look embedded in #rmarkdown! 😎 — annakrystalli (@annakrystalli) March 23, 2018 Embbed gifs, videos, widgets in this way Getting help with markdown To get help, you need a reproducible example github issues stackoverflow slack channels discussion boards reprex install.packages(&quot;reprex&quot;) Use function reprex::reprex() to produce a reproducible example in a custom markdown format for the venue of your choice \"gh\" for GitHub (default) \"so\" for StackOverflow, \"r\" or \"R\" for a runnable R script, with commented output interleaved. using reprex Copy the code you want to run. all required variables must be defined and libraries loaded In the console, call the reprex function reprex::reprex() the code is executed in a fresh environment and “code + commented output” is returned invisibly on the clipboard. Paste the result in the venue of your choice. Once published it will be rendered to html. bookdown Authoring with R Markdown. Offers: cross-references, citations, HTML widgets and Shiny apps, tables of content and section numbering The publication can be exported to HTML, PDF, and e-books (e.g. EPUB) Can even be used to write thesis! &lt;img src=“assets/logo_bookdown.png”, width=“200px”/&gt; &lt;img src=“assets/cover_bookdown.jpg”, width=“200px”/&gt; pkgdown For buidling package documentation Can use it to document any functional code you produce and demonstrate it’s us ethrough vignettes workflowr pkg Build analyses websites and organise your project The workflowr R package makes it easier for researchers to organize their projects and share their results with colleagues. ] blogdown For creating and mantaining blogs. Check out https://awesome-blogdown.com/, a curated list of awesome #rstats blogs in blogdown for inspiration! bookdown For creating and mantaining online books rOpenSci Software Review policies Geocomputation in R Thesisdown An updated R Markdown thesis template using the bookdown package Resources R Markdown: The Definitive guide Rmarkdown documentation Rmarkdown html_document format documentation Rstudio Rmarkdown cheatsheet Reproducible Research coursera MOOC Producing html documents from .R scripts using knitr::spin "],["version-control.html", "(PART) Version Control", " (PART) Version Control "],["version-control-with-git.html", "Version Control with Git What is Version control? 🤔 What is git? 🤔 What is GitHub 🤔 Git, Github &amp; Rstudio Demo How does Git work? Configure git &amp; GitHub Version Controlling projects Create repository on GitHub Host html content on GitHub Enable gh-pages Tracking changes Ignoring files through .gitignore Git tips Further Resources", " Version Control with Git Hands up - who has heard of version control software? What do you think it does? What is Version control? 🤔 The management of changes to documents, computer programs, large web sites, and other collections of information. Examples: Numbering of book editions Wikipedia’s Page history Where did it come from? The need for a logical way to organize and control revisions has existed for almost as long as writing has existed, but revision control became much more important, and complicated when the era of computing began Elements of a Version Control system Changes are usually identified by a number or letter code, termed the “revision number” Each revision is associated with a timestamp and the person making the change. Only changes to a file are recorded rather than saving a whole new copy. Revisions can be compared, restored, and with some types of files, merged. What is git? 🤔 Open source (free to use) Version control software. Usually accessed via the command line, or a client program. Where did it come from? Git development began in 2006 after many developers of the Linux kernel gave up access to BitKeeper (at the time the best but proprietary) Linus Torvalds on the name git: \"I’m an egotistical bastard, and I name all my projects after myself. First ‘Linux’, now ‘git’ More on the name in the source code original readme file Why use it in research? Exhibit A Figure 2: Image: xkcd CC BY-NC 2.5 What is GitHub 🤔 A website that allows you to store your Git repositories online and makes it easy to collaborate with others. They also provide other services like issue (bug) tracking and wikis. Similar services are GitLab and BitBucket. Why use it in research: To enable collaboration and track contributions images: Mozilla Science Lab CC-BY 4.0 Acts as a remote back-up Facilitates transparency Facilitates project management Facilitates sharing and collaboration Super-charges innovation by Open Sourcing Science Mozilla &amp; Working Open Open Source Basics Reinventing Discovery Macroecological and macroevolutionary patterns emerge in the universe of GNU/Linux operating systems Anatomy of GitHub Repo Readme files. Create a README.md file to explain what your project is, and how to install and use it. README.md is the file that is automatically displayed when you open a GitHub repo. License. Without some sort of licence, the contents of the repository are technically closed. Some allow users of the code to do anything they like with their code - these are known as permissive licences. Examples are the MIT Licence or Apache. https://choosealicense.com/ - does what it says on the tin and helps you choose a licence. Here are some resources to help you choose: https://tldrlegal.com/ - plain english explanations of licences in bullet form. Contributing guide - make a file called CONTRIBUTING.md and guidelines for contributors so they know what they should do if they want to help you out. Code of Conduct - good projects have codes of conduct to make sure that people are treated well. Github has an Code of Conduct wizard to make it easy to add one. Issues - use GitHub issues to record and discuss tasks. Git, Github &amp; Rstudio Before: git only through the terminal Rstudio &amp; usethis to the rescue! Rstudio + usethis 📦 == heavenly Git &amp; GitHub Initialise Rstudio project with Git by just checking a box! Forgot to? use usethis::use_git() visual panel to easily see the status of all your files interactive navigation through file version history Demo How does Git work? When a local directory becomes initialised with git, a hidden .git folder is added to it. it’s now called a repository New copies of files you tell git to track will be added to that .git folder. After adding, git will track any modifications to those files first commit - whole file added Any file unknown to git will have a yellow ? box next to it. The first time you commit a file you are adding it to .git, effectively telling it to start tracking the file second commit - only difference highlighted The first time you commit a file, only the changes are shown and any file that has uncommited modifications is shown with a blue M When all changes have been committed, the git panel is clear. Enough theory, how about in practice! Configure git &amp; GitHub Configure git First, git needs to know who you are so your commits can be attributed to you. usethis to the rescue again! Check your configuration usethis::use_git_config() Set your configuration Use your github username and and the email you used to sign-up on GitHub usethis::use_git_config( user.name = &quot;Jane&quot;, user.email = &quot;jane@example.org&quot;) Set up GITHUB PAT To authenticate with GitHub, you’ll also need a Personal Authorisation Token (PAT). usethis::browse_github_pat() will open up the GitHub panel to generate your PAT. Copy it and paste it into your .Renviron file as system variable GITHUB_PAT. usethis::edit_r_environ() Use edit_r_environ() to open and edit your .Renviron file Version Controlling projects Turn our project into a repository If you didn’t initialise git at the beginning of your project, you can do so now with usethis::use_git(): This will try to commit everything in the repo so far in one go! Override that behaviour by selecting a negative response when asked. usethis::use_git() ✔ Initialising Git repo ✔ Adding &#39;.Rhistory&#39;, &#39;.RData&#39; to &#39;.gitignore&#39; There are 10 uncommitted files: * &#39;.DS_Store&#39; * &#39;.gitignore&#39; * &#39;.Rbuildignore&#39; * &#39;analysis.R&#39; * &#39;data-raw/&#39; * &#39;data/&#39; * &#39;index.html&#39; * &#39;index.Rmd&#39; * &#39;R/&#39; * &#39;wood-survey.Rproj&#39; Is it ok to commit them? 1: Negative 2: Nope 3: I agree Selection: Next allow Rstudio to restart when asked: ● A restart of RStudio is required to activate the Git pane Restart now? 1: Absolutely 2: Absolutely not 3: Negative Committing files In our project, let’s have a look at the Rstudio Git tab. It shows all the files currently in the folder. The yellow ? indicates none of the files have been added to git yet. Add files To commit changes in a file just select it in the git pane. When changes to a file are commited for the first time, the whole file is indicated as Added (green A). Let’s focus on the files and analytical data we created so far. For now ignore the data-raw folder and all the other files we didn’t create: Commit changes Click on commit and write an appropriate commit message: Create a README Our repository also needs a README. We only need a simple plain markdown (.md) file for our README. We can create a template using usethis::use_readme_md() usethis::use_readme_md() Edit README Adapt the template, adding a short description about your project. Add and commit your new README Create repository on GitHub Now that we have set up a GITHUP_PAT, we can use function usethis::use_github() to create a GitHub repository for our project: usethis::use_github(protocol = &quot;https&quot;) ✔ Checking that current branch is &#39;master&#39; ● Check title and description Name: wood-survey Description: Are title and description ok? 1: No 2: Yes 3: Nope Answer affirmatively for the process to continue. Once the repo is created and any commmited files pushed, the repo is launched in the browser: Host html content on GitHub Let’s head to the repo and have a look at what we’ve shared. To host our html content on GitHub, we need to enable gh-pages in our repository. Go to repo Settings Enable gh-pages Review setup Ensure the Enforce HTTPS option is selected. Click on the link displayed and go check out your work! Copy the link. In the main repo page, edit the page details at the top and paste copied the url in the website field. Once added it provides easy access to the rendered content: Tracking changes Making a change to our index.Rmd Let’s add the link to the rendered content to our place holder in index.Rmd Let’s add and commit our changes Pushing changes to GitHub Click on the ️ button on the Git tab to push our changes up to the repository Let’s go have a look at the history 🕒 You will likely need to put re-enter your password if you use the Rstudio IDE buttons to push. To save have to do that everytime, have a look at this blogpost on How to connect RStudio Cloud with Github which has details on setting up an SSH key_ Otherwise you can use usethis::pr_push() which uses the GITHUB_PAT for authentication) Deleting files Create a new file, any type of file. Add and Commit it. Delete it Commit the deletion Look back through the history Ignoring files through .gitignore There may be files that you don’t want to commit to git, e.g. data files that are too large documents with sensitive information (eg authorisation tokens etc) intermediate files that you don’t need to save copies of. Tell git to ingnore them by adding them to the .gitignore file. When we open .gitgnore we see there are a number of files already added. Let’s the rest of the files we want to ignore. .Rproj.user .Rhistory .RData .Rbuildignore .DS_Store gitignore regex You can use regex (regular expressions) in .gitignore files to ignore files according to a pattern. directoryname/* will ignore all files in a directory. *.html will ignore any file ending in .html prefix “!” which negates the pattern So let’s use regex to ignore all files in attic/ and all files in data-raw/ apart from individual.R. Add the following to the bottom of .gitignore attic/* data-raw/* !data-raw/individual.R Commit .gitignore Now that we’ve determined which files we want to ignore, let’s commit .gitignore so we can have a record of it and track any changes. Git tips commit early, commit often commit logical bits of work together write meaninful messages Further Resources Git-it Happy with Git Oh Shit Git Never forget "],["collaborative-github.html", "Collaborative GitHub", " Collaborative GitHub View Slides "],["practical-github-rstudio-for-collaborative-coding.html", "Practical: GitHub &amp; Rstudio for collaborative coding EvoLottery Clone Github repo Make a change to the repo Commit changes locally to git Push changes to GitHub Create pull request", " Practical: GitHub &amp; Rstudio for collaborative coding EvoLottery Welcome to the evolutionary lottery of skull and beak morphology Beak and skull shapes in birds of prey (“raptors”) are strongly coupled and largely controlled by size. In this exercise, each participant will fork a GitHub repo, and contribute a file required to simulate the evolutionary trajectory of an imaginary species’ body size. We’ll use GitHub to collate all species files and plot them all up together at the end! We’ll also discover the skull and beak shapes associated with each simulated species size. Start! Clone Github repo Start with GitHub repo https://GitHub.com/RSE-Sheffield/collaborative_GitHub_exercise Fork it make your own copy of the repository on GitHub. Fork are linked and traceable GitHub makes a copy into your account 🚦 Clone repo copy repo link to create a new Rstudio project from the repository. Create new project in Rstudio Checkout from version control repository Clone project from a git repository Paste repo link copied from GitHub into Repository URL field. Click Create Project. Rstudio project now contains all files from the GitHub repo. Make a change to the repo make a copy of params_tmpl.R open params/params_tmpl.R SAVE AS NEW .R script in params/ folder Use species name of your choice to name new file. Please DO NOT OVERWRITE params/params_tmpl.R. Edit file Edit file with parameters of your choice and save. The parameters each participants need to supply are: sig2: A numeric value greater than 0 but smaller than 5 species.name: a character string e.g. \"anas_krystallinus\". Try to create a species name out of your name! color: a character string e.g. \"red\", \"#FFFFFF\" (Check out list of colours in R) NB: remember to save the changes to your file Commit changes locally to git In the git tab, select the new file you created and click Commit. Please ONLY COMMIT YOUR NEW FILE Write an informative commit message and click Commit your new file has now been commited Push changes to GitHub on the git tab click ⇧ to push changes to GitHub changes have now been updated in the GitHub repo Create pull request In your repository, create new pull request to merge fork to master repo (ie the original repo you forked) GitHub checks whether your requested merge creates any coflicts. If all is good, click on Create pull request Write an informative message explaining your changes to the master repo administrators. Click on Create pull request The repository owner will then review your PR and either merge it in or respond with some guidance if they spot a problem. Check original repo to see your merged changes We’ll merge all contributions and plot them together at the end! "],["closing.html", "(PART) Closing", " (PART) Closing "],["closing-remarks.html", "Closing Remarks", " Closing Remarks View Slides "],["additional.html", "(PART) Additonal materials", " (PART) Additonal materials "],["packaging-functionality.html", "Packaging functionality R Package Structure R Package conventions: Software Engineering approach Anatomy of an R package DESCRIPTION file Dependency management R/ Document functions with Roxygen tests/ 💻Create your first package 🚦 Functions in the R/ dir 🚦 Roxygen documentation 🚦 Personalise function Add some fun! 🚦 Check package integrity 🚦 Add dependencies Add License 🚦 Add Test create test file Write test 🚦 Complete package metadata Authors Add a title and description Add a date 🚦 Create README Commit and push to GitHub 🚦 Create documentation site", " Packaging functionality R Package Structure Used to share functionality with the R community Useful conventions Useful software development tools Easy publishing through GitHub R Package conventions: metadata: in the DESCRIPTION file functions in .R scripts in the R/ folder tests in the tests/ folder Documentation: functions using Roxygen notation workflows using .Rmd documents in the vignettes/ folder Software Engineering approach Following conventions allows us to make use of automated tools for: Checking and testing code Producing documentation for code and workflows Publishing, distributing and citing code Anatomy of an R package Let’s use pkgreviewr, a package I authored to help automate some aspects of the rOpenSci review process, as an example to examine some elements of what makes a package: DESCRIPTION file Capture metadata around the package - Functionality description - Creators - License Package: pkgreviewr Type: Package Title: rOpenSci package review project template Version: 0.1.1 Authors@R: c(person(&quot;Anna&quot;, &quot;Krystalli&quot;, email = &quot;annakrystalli@googlemail.com&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;)), person(&quot;Maëlle&quot;, &quot;Salmon&quot;, email = &quot;maelle.salmon@yahoo.se&quot;, role = &quot;aut&quot;)) Description: Creates files and collects materials necessary to complete an rOpenSci package review. Review files are prepopulated with review package specific metadata. Review package source code is also cloned for local testing and inspection. License: GPL-3 + file LICENSE URL: https://github.com/ropenscilabs/pkgreviewr BugReports: https://github.com/ropenscilabs/pkgreviewr/issues Encoding: UTF-8 LazyData: true Imports: devtools, git2r (&gt;= 0.23.0), usethis (&gt;= 1.2.0), here, reprex, gh, base64enc, whoami, magrittr, covr, goodpractice, assertthat, httr, rstudioapi, clipr, clisymbols, crayon, dplyr, glue, fs, urltools, shiny Suggests: testthat, mockery, knitr, rmarkdown RoxygenNote: 6.1.1 Remotes: ropensci/git2r VignetteBuilder: knitr Roxygen: list(markdown = TRUE) citation citation(&quot;geosphere&quot;) ## ## To cite package &#39;geosphere&#39; in publications use: ## ## Robert J. Hijmans (2019). geosphere: Spherical Trigonometry. R ## package version 1.5-10. https://CRAN.R-project.org/package=geosphere ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {geosphere: Spherical Trigonometry}, ## author = {Robert J. Hijmans}, ## year = {2019}, ## note = {R package version 1.5-10}, ## url = {https://CRAN.R-project.org/package=geosphere}, ## } Dependency management It’s the job of the DESCRIPTION to list the packages that your package needs to work. Imports: devtools, git2r (&gt;= 0.23.0), usethis (&gt;= 1.2.0), here, reprex, gh, base64enc, whoami, magrittr, covr, goodpractice, assertthat, httr, rstudioapi, clipr, clisymbols, crayon, dplyr, glue, fs, urltools, shiny Imports are necessary dependencies for the functions in your package to work Suggests: testthat, mockery, knitr, rmarkdown Suggests are dependencies that are not necessary for the functions in your package but might be neccessary to run all the vignettes or tests in your package R/ Keep all functions in R scripts in R/ folder . ├── github.R ├── pkgreview.R ├── pkgreviewr-package.R ├── render-templates.R ├── rmd-utils.R ├── style.R └── utils.R 0 directories, 7 files example function script Create a new function .R file in the R/ folder library(usethis) use_r(&quot;add&quot;) R └── add.R 0 directories, 1 files Document functions with Roxygen Document functions with Roxygen notation Automatically create help files on build #&#39; Add together two numbers. #&#39; #&#39; @param x A number. #&#39; @param y A number. #&#39; @return The sum of x and y. #&#39; @examples #&#39; add(1, 1) #&#39; add(10, 1) add &lt;- function(x, y) { x + y } tests/ Tests provide confidence in what the code is doing. Contents of pkgreviewr test folder . ├── testthat │ ├── setup.R │ ├── test-create-pkgreview.R │ ├── test-gh-calls.R │ ├── test-render-templates.R │ └── test-setup.R └── testthat.R 1 directory, 6 files Example test use_test(&quot;add&quot;) tests ├── testthat │ ├── test-add.R └── testthat.R context(&quot;test-add&quot;) test_that(&quot;add works&quot;, { expect_equal(add(2, 2), 4) }) The R package structure can help with providing a logical organisation of files, by providing a set of standard locations for certain types of files. To work with packages in RStudio we use the Build pane, which includes a variety of tools for building, documenting and testing packages. This will appear if Rstudio recognises the project as an R package. 💻Create your first package Let’s go ahead and create our first package! We do that as we would any project, but this time we select R package instead of New Project. Call your package mypackage. File &gt; New Project… &gt; New Directory &gt; R package &gt; mypackage Your new project should have the following structure. The build pane should also be visible. . ├── DESCRIPTION ├── NAMESPACE ├── R │ └── hello.R ├── man │ └── hello.Rd └── mypackage.Rproj 2 directories, 5 files 🚦 Functions in the R/ dir Let’s inspect hello.R it contains a function that takes now arguments and prints hello world to the console when called. The comments above are just that, comments and don’t serve any functional purpose. # Hello, world! # # This is an example function named &#39;hello&#39; # which prints &#39;Hello, world!&#39;. # # You can learn more about package authoring with RStudio at: # # http://r-pkgs.had.co.nz/ # # Some useful keyboard shortcuts for package authoring: # # Build and Reload Package: &#39;Cmd + Shift + B&#39; # Check Package: &#39;Cmd + Shift + E&#39; # Test Package: &#39;Cmd + Shift + T&#39; hello &lt;- function() { print(&quot;Hello, world!&quot;) } Install package. You can install a package locally from it’s source code with function install() library(devtools) install(&quot;.&quot;) You can now load it like any other package… library(&quot;mypackage&quot;) And use your function! hello() ## [1] &quot;Hello, world!&quot; 🚦 Roxygen documentation Roxygen2 allows you to write specially-structured comments preceeding each function definition to document: the inputs and outputs a description of what it does an example of how to use it These are processed automatically to produce .Rd help files for your functions and control which functions are exported to the package NAMESPACE. Let’s document our example function. First, clear the demo comments above the function and all contents of the exampleNAMESPACE. Also delete the file in the man folder. Insert Roxygen skeleton You can insert a Roxygen skeleton by placing the curson with a function and clicking: Code &gt; Insert Roxygen Skeleton #&#39; Title #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples hello &lt;- function() { print(&quot;Hello, world!&quot;) } Roxygen basics roxygen notation indicated by beginning line with #'. First line will be the title for the function. After title, include a blank #' line and then write a longer description. @param argument_name description of the argument. @return description of what the function returns. @export tells Roxygen2 to add this function to the NAMESPACE file, so that it will be accessible to users. @examples allows to include example of how to use a function Complete Roxygen documentation #&#39; Hello World! #&#39; #&#39; Print hello greeting #&#39; @return prints hello greeting to console #&#39; @export #&#39; #&#39; @examples #&#39; hello() hello &lt;- function() { print(&quot;Hello, world!&quot;) } Autogenerate documentation Use function devtools::document() to create documentation. This re-creates a hello.Rd helpfile in the man/ folder and populates the NAMESPACE with our functions devtools::document() Click Install and Restart to re-install the package and make the documentation available. You can configure your build tools in the Global Options to automatically build documentation every time you Install &amp; Rebuild 🚦 Personalise function Let’s go a step further and customise our function so that the greeting is from ourselves! #&#39; Hello World! #&#39; #&#39; Print hello greeting #&#39; @return prints hello greeting to console from me #&#39; @export #&#39; #&#39; @examples #&#39; hello() hello &lt;- function() { print(&quot;Hello, world from Anna&quot;) } Add some fun! Programming is most useful for having fun. So let’s make our function extra fun! We’ll use package cowsay install.packages(&quot;cowsay&quot;) which has a single function say, which does this… cowsay::say(&quot;Say whaaaaaat?&quot;, by = &quot;shark&quot;) ## ## -------------- ## Say whaaaaaat? ## -------------- ## \\ ## \\ ## \\ ## /&quot;&quot;-._ ## . &#39;-, ## : &#39;&#39;, ## ; * &#39;. ## &#39; * () &#39;. ## \\ \\ ## \\ _.---.._ &#39;. ## : .&#39; _.--&#39;&#39;-&#39;&#39; \\ ,&#39; ## .._ &#39;/.&#39; . ; ## ; `-. , \\&#39; ## ; `, ; ._\\ ## ; \\ _,-&#39; &#39;&#39;--._ ## : \\_,-&#39; &#39;-._ ## \\ ,-&#39; . &#39;-._ ## .&#39; __.-&#39;&#39;; \\...,__ &#39;. ## .&#39; _,-&#39; \\ \\ &#39;&#39;--.,__ &#39;\\ ## / _,--&#39; ; \\ ; \\^.} ## ;_,-&#39; ) \\ )\\ ) ; ## / \\/ \\_.,-&#39; ; ## / ; ## ,-&#39; _,-&#39;&#39;&#39;-. ,-., ; PFA ## ,-&#39; _.-&#39; \\ / |/&#39;-._...--&#39; ## :--`` )/ ## &#39; ## 😜 So let’s create a function that randomly chooses one of the animals available in cowsay to deliver the greeting, and also allow the user to customise who the recipient of the greeting is #&#39; Hello World! #&#39; #&#39; Print personalised hello greeting from me. #&#39; #&#39; @param name character string. Your name! #&#39; #&#39; @return prints hello greeting to console #&#39; @export #&#39; #&#39; @examples #&#39; hello() #&#39; hello(&quot;Lucy Elen&quot;) hello &lt;- function(name = NULL) { # create greeting if(is.null(name)){name &lt;- &quot;world&quot;} greeting &lt;- paste(&quot;Hello&quot;, name, &quot;from Anna!&quot;) # randomly sample an animal animal_names &lt;- names(cowsay::animals) i &lt;- sample(1:length(animal_names), 1) cowsay::say(greeting, animal_names[i]) } Document, Install and restart to load our changes hello(&quot;y&#39;all&quot;) ## ## ----- ## Hello y&#39;all from Anna! ## ------ ## \\ ## \\ ## \\ ## ________ ## __--´ ° `--__ ## __-´ ° ° `-__ ## (´ ° ° ° `) ## (° °| |° ° | | ) ## `&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;`|&#39;&#39;&#39;&#39;&#39;|´&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;´ ## | | ## |:::::| ## /:|:::::|:\\ ## /::|:::::|::\\ ## | | ## |^ ^| ## | _ | [FK] ## |_____| 🚦 Check package integrity An important part of the package development process is R CMD check. R CMD check automatically checks your code and can automatically detects many common problems that we’d otherwise discover the hard way. To check our package, we can: use devtools::check() press Ctrl/Cmd + Shift + E click on the Check tab in the Build panel. This: Ensures that the documentation is up-to-date by running devtools::document(). Bundles the package before checking it. More info on checks here. Both these run R CMD check which return three types of messages: ERRORs: Severe problems that you should fix regardless of whether or not you’re submitting to CRAN. WARNINGs: Likely problems that you must fix if you’re planning to submit to CRAN (and a good idea to look into even if you’re not). NOTEs: Mild problems. If you are submitting to CRAN, you should strive to eliminate all NOTEs, even if they are false positives. Let’s Check our package: Click on the Check button (📋 ) ── R CMD check results ──────────────────────────────────── mypackage 0.1.0 ──── Duration: 8.4s ❯ checking DESCRIPTION meta-information ... WARNING Non-standard license specification: What license is it under? Standardizable: FALSE ❯ checking dependencies in R code ... WARNING &#39;::&#39; or &#39;:::&#39; import not declared from: ‘cowsay’ 0 errors ✔ | 2 warnings ✖ | 0 notes ✔ Error: R CMD check found WARNINGs Execution halted Exited with status 1. Aha, so our checks have thrown up some warnings! First, it’s telling us we haven’t added a LICENSE. It’s also telling us that we have a dependency (import) from package cowsay which we haven’t documented in the DESCRIPTION file. usethis to the rescue! 🚦 Add dependencies Add cowsay as a dependency. usethis::use_package(&quot;cowsay&quot;) ✔ Setting active project to &#39;/Users/Anna/Desktop/mypackage&#39; ✔ Adding &#39;cowsay&#39; to Imports field in DESCRIPTION ● Refer to functions with `cowsay::fun()` Add License usethis::use_mit_license() Check again…All should be good! ── R CMD check results ──────────────────────────────────── mypackage 0.1.0 ──── Duration: 9.3s 0 errors ✔ | 0 warnings ✔ | 0 notes ✔ R CMD check succeeded 🚦 Add Test Testing is a vital part of package development. It ensures that our code does what you want it to do. Once you’re set up with a testing framework, the workflow is simple: Modify your code or tests. Test your package with Ctrl/Cmd + Shift + T or devtools::test(). Repeat until all tests pass. create test file To create a new test file (and the testing framework if required), use function usethis::use_test(). It’s good practice to name the test files after the .R files containing the functions being tested. use_test(&quot;hello&quot;) ✔ Setting active project to &#39;/Users/Anna/Documents/workflows/workshops/materials/mypackage&#39; ✔ Adding &#39;testthat&#39; to Suggests field in DESCRIPTION ✔ Creating &#39;tests/testthat/&#39; ✔ Writing &#39;tests/testthat.R&#39; ✔ Writing &#39;tests/testthat/test-hello.R&#39; ● Modify &#39;tests/testthat/test-hello.R&#39; This just created the following folders and files tests ├── testthat │ └── test-hello.R └── testthat.R 1 directory, 2 files It also added testthat to the suggested packages in the DESCRIPTION file. Suggests: testthat That’s because you don’t need test that to run the functions in mypackage, but you do if you want to run the tests. When the tests are run (either through running devtools::test(), clicking on More &gt; Test Package in the Build panel or Cmd/Ctrl + Shift + T), the code in each test script in directory testthat is run. test-hello.R Let’s load the library so we can explore the testthat testing framework library(testthat) context(&quot;test-hello&quot;) test_that(&quot;multiplication works&quot;, { expect_equal(2 * 2, 4) }) ## Test passed  If the test doesn’t pass it throws an error context(&quot;test-hello&quot;) test_that(&quot;multiplication works&quot;, { expect_equal(2 * 2, 5) }) ## ── Failure (&lt;text&gt;:4:3): multiplication works ────────────────────────────────── ## 2 * 2 not equal to 5. ## 1/1 mismatches ## [1] 4 - 5 == -1 Write test Let’s write a simple test to check that we are getting an expected output type. The first thing to note, looking at the say() documentation is that it takes an argument type which allows us to specify the output we want. It defaults message which means the output of the function is returned as a message. We can therefore use testthat::expect_message() context(&quot;test-hello&quot;) test_that(&quot;hello works&quot;, { expect_message(hello()) }) ## ## ----- ## Hello world from Anna! ## ------ ## \\ ## \\ ## &gt;&lt;((((º&gt; &gt;&lt;((((º&gt; &gt;&lt;((((º&gt; &gt;&lt;((((º&gt; &gt;&lt;((((º&gt; ## Kiyoko Gotanda ## ── Failure (&lt;text&gt;:4:3): hello works ─────────────────────────────────────────── ## `hello()` did not produce any messages. Now let’s test our package devtools::test() Success! ==&gt; devtools::test() Loading mypackage Testing mypackage ✔ | OK F W S | Context ✔ | 1 | test-hello ══ Results ════════════════════════════════════════════════════════ OK: 1 Failed: 0 Warnings: 0 Skipped: 0 🚦 Complete package metadata Let’s head to the DESCRIPTION file and complete the details. Authors First let’s complete the authors. Remove the current author and maintainer lines and replace it with the following line: Authors@R: person(&quot;First&quot;, &quot;Last&quot;, email = &quot;first.last@example.com&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;)) completed with your own details Add a title and description Complete the title and description fields with appropriate details. If you want to form a paragraph of text, make sure do indent the hanging lines by 4 spaces (one tab). And make sure that your Description field ends in a full-stop. Add a date Use today’s date in ISO format, ie 2019-04-10. This will populate a citation entry for us. Completed DESCRIPTION The complete DESCRIPTION file should look something like this: Package: mypackage Type: Package Title: Customised greetings from me! Version: 0.1.0 Authors@R: person(&quot;Anna&quot;, &quot;Krystalli&quot;, email = &quot;annakrystalli@googlemail.com&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;)) Description: Prints a customised greeting from myself, delivered by a friend. License: MIT + file LICENSE Encoding: UTF-8 LazyData: true RoxygenNote: 6.1.1 Imports: cowsay Suggests: testthat Date: 2019-04-10 Check your package. If all is good, document, install and restart! Now, check you’re package’s citation: citation(&quot;mypackage&quot;) ## ## To cite package &#39;mypackage&#39; in publications use: ## ## Anna Krystalli (2019). mypackage: Customised greetings from me!. R ## package version 0.1.0. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {mypackage: Customised greetings from me!}, ## author = {Anna Krystalli}, ## year = {2019}, ## note = {R package version 0.1.0}, ## } 🚦 Create README The final document you will need for your package is a README. usethis::use_readme_rmd() ✔ Writing &#39;README.Rmd&#39; ✔ Adding &#39;^README\\\\.Rmd$&#39; to &#39;.Rbuildignore&#39; ● Modify &#39;README.Rmd&#39; ✔ Writing &#39;.git/hooks/pre-commit&#39; Because it’s an .Rmd but GitHub can only display an md document as it’s landing page, this is a special .Rmd that renders to a markdown document rather than html. The function adds a check to .git to ensure you commit an up to date version on the md when you commit changes to the .Rmd. Complete the README, including an example. Commit and push to GitHub Now you have everything you need to share your package on GitHub so commit and push it up. Anyone will be able to install it using, eg: devtools::install_github(&quot;annakrystalli/mypackage&quot;) 🚦 Create documentation site You can use package pkgdown to create an online site for your documentation: install.packages(&quot;pkgdown&quot;) pkgdown::build_site() ══ Building pkgdown site ════════════════════════════════════════════ Reading from: &#39;/Users/Anna/Documents/workflows/dummy/mypackage&#39; Writing to: &#39;/Users/Anna/Documents/workflows/dummy/mypackage/docs&#39; ── Initialising site ──────────────────────────────────────────────── Copying &#39;../../../../../../Library/Frameworks/R.framework/Versions/3.5/Resources/library/pkgdown/assets/docsearch.css&#39; to &#39;docsearch.css&#39; Copying &#39;../../../../../../Library/Frameworks/R.framework/Versions/3.5/Resources/library/pkgdown/assets/docsearch.js&#39; to &#39;docsearch.js&#39; Copying &#39;../../../../../../Library/Frameworks/R.framework/Versions/3.5/Resources/library/pkgdown/assets/link.svg&#39; to &#39;link.svg&#39; Copying &#39;../../../../../../Library/Frameworks/R.framework/Versions/3.5/Resources/library/pkgdown/assets/pkgdown.css&#39; to &#39;pkgdown.css&#39; Copying &#39;../../../../../../Library/Frameworks/R.framework/Versions/3.5/Resources/library/pkgdown/assets/pkgdown.js&#39; to &#39;pkgdown.js&#39; ── Building home ──────────────────────────────────────────────────── Writing &#39;authors.html&#39; Reading &#39;LICENSE.md&#39; Writing &#39;LICENSE.html&#39; Writing &#39;LICENSE-text.html&#39; Reading &#39;README.Rmd&#39; Writing &#39;index.html&#39; ── Building function reference ────────────────────────────────────── Updating mypackage documentation Writing NAMESPACE Loading mypackage Writing NAMESPACE Writing &#39;reference/index.html&#39; Loading mypackage Reading &#39;man/hello.Rd&#39; Writing &#39;reference/hello.html&#39; ══ DONE ═════════════════════════════════════════════════════════════ ── Previewing site ────────────────────────────────────────────────── This creates html documentation for our package in the docs/ folder. Commit and push the docs/ folder to GitHub Make the site live by enabling gh-pages. Set it to serve content from the docs folder. Your package is now installable from GitHub, has online documentation and should have this final structure: . ├── DESCRIPTION ├── LICENSE ├── LICENSE.md ├── NAMESPACE ├── R │ └── hello.R ├── README.Rmd ├── README.md ├── docs │ ├── LICENSE-text.html │ ├── LICENSE.html │ ├── authors.html │ ├── docsearch.css │ ├── docsearch.js │ ├── index.html │ ├── link.svg │ ├── pkgdown.css │ ├── pkgdown.js │ ├── pkgdown.yml │ └── reference │ ├── hello.html │ └── index.html ├── man │ └── hello.Rd ├── mypackage.Rproj └── tests ├── testthat │ └── test-hello.R └── testthat.R 6 directories, 23 files Check out my complete example here "],["creating-a-research-compendium-with-rrtools.html", "Creating a research compendium with rrtools Background Enter the Research Compendium R community response General Project Organisation From raw to analytical data Separate function definition and application Use Rstudio projects Follow convention R Developer Tools rrtools: Research Compendia in R 💻 Workshop materials Data Workshop aims and objectives Setup 🚦 Create compendium 🚦 Update DESCRIPTION file 🚦Sharing a compendium on GitHub Create README update README 🚦 Create GitHub repository 🚦 Setting up the analysis folder Create analysis 🚦 Reproduce a paper in Rmd Setup data Inspect analysis.R file 🚦 Create journal article template using rticles 🚦 Update YAML 🚦 Add text 🚦 Update references 🚦 Update math 🚦 Add code Render final document to pdf 🚦 Add paper dependencies Final compendium", " Creating a research compendium with rrtools Background Research is increasingly computational Code and data are important research outputs yet, we still focus mainly on curating papers. Calls for openness stick: reproducibility crisis carrot: huge rewards from working open Yet we lag in conventions and technical infrastructure for such openness. Enter the Research Compendium The goal of a research compendium is to provide a standard and easily recognizable way for organizing the digital materials of a project to enable others to inspect, reproduce, and extend the research. Three Generic Principles Organize its files according to prevailing conventions: help other people recognize the structure of the project, supports tool building which takes advantage of the shared structure. Separate of data, method, and output, while making the relationship between them clear. Specify the computational environment that was used for the original analysis. R community response R packages can be used as a research compendium for organising and sharing files! R package file system structure for reproducible research Take advantage of the power of convention. Make use of great package development tools. See Ben Marwick, Carl Boettiger &amp; Lincoln Mullen (2018) Packaging Data Analytical Work Reproducibly Using R (and Friends), The American Statistician, 72:1, 80-88, DOI: &lt;10.1080/00031305.2017.1375986&gt; images: Kartik Ram: rstudio::conf 2019 talk Example use of the R package structure for a research compendium (source Marwick et al, 2018) General Project Organisation Good project layout helps ensure the Integrity of data Portability of the project Easier to pick the project back up after a break From raw to analytical data the reproducible pipeline Do not manually edit raw data Keep a clean pipeline of data processing from raw to analytical. Ideally, incorporate checks to ensure correct processing of data through to analytical. Check out rOpenSci package drake, an R-focused pipeline toolkit for reproducibility Separate function definition and application When your project is new and shiny, the script file usually contains many lines of directly executated code. As it matures, reusable chunks get pulled into their own functions. The actual analysis scripts then become relatively short, and use the functions defined in separate R scripts. Use Rstudio projects Keep your work tidy, portable and self-contained pkg here Use function here::here(\"path\", \"to\", \"file\") to create robust paths relative to the project root. eg here::here(&quot;path&quot;, &quot;to&quot;, &quot;file&quot;) ## [1] &quot;/Users/runner/work/rrresearchACCE20/rrresearchACCE20/path/to/file&quot; Follow convention It’s like agreeing that we will all drive on the left or the right. A hallmark of civilization is following conventions that constrain your behavior a little, in the name of public safety. Jenny Bryan on Project-oriented workflows A place for everything, everything in its place. Benjamin Franklin The benefits of following convention: You will be able to find your way around any of your projects You will be able to find your way around any project by others following same convention You will be able to find your way around any r package on GitHub! R Developer Tools Leverage tools and functionality for R package development manage dependencies make functionality available document functionality validate functionality version contol your project devtools, usethis, rrtools, Rstudio minimal analysis project An scripts/ directory that contains R scripts (.R), notebooks (.Rmd), and intermediate data. A DESCRIPTION file that provides metadata about the compendium. Most importantly, it would list the packages needed to run the analysis. Would contain field to indicate that this is an analysis, not a package. A reproducible analysis project would also contain: An R/ directory which contains R files that provide high-stakes functions. A data/ directory which contains high-stakes data. A tests/ directory that contains unit tests for the code and data. A vignettes/ directory that contains high-stakes reports. Autogenerated components: A man/ directory which contains roxygen2-generated documentation for the reusable functions and data. Online documentation in a docs/ folder. A shareable reproducible analysis project would also: Use Git + GitHub (or other public Git host) Use Travis or other continuous integration service Capture the computational environment so it can easily be recreated on a different computer. This involves at a minimum capturing package versions, but might also include capturing R version, and other external dependencies. Start small and build as necessary images: Kartik Ram: rstudio::conf 2019 talk rrtools: Research Compendia in R The goal of rrtools is to provide instructions, templates, and functions for making a basic compendium suitable for writing reproducible research with R. rrtools build on tools &amp; conventions for R package development to organise files manage dependencies share code document code check and test code rrtools extends and works with a number of R packages: devtools: functions for package development usethis: automates repetitive tasks that arise during project setup and development bookdown: facilitates writing books and long-form articles/reports with R Markdown 💻 Workshop materials Data On github: https://github.com/annakrystalli/rrtools-wkshp-materials/ click on Clone or download click on Download ZIP Unzip the file Workshop aims and objectives In this workshop we’ll use materials associated with a published paper (text, data and code) to create a research compendium around it. By the end of the workshop, you should be able to: Be able to Create a Research Compendium to manage and share resources associated with an academic publication. Be able to produce a reproducible manuscript from a single rmarkdown document. Appreciate the power of convention! Let’s dive in! Setup Install packages Next, let’s install the packages we’ll need, starting with rrtools (if you haven’t got devtools installed, you’ll need to before you can install rrtools from GitHub). # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;benmarwick/rrtools&quot;) Installing rrtools imports many of the packages we’ll need today (eg, have a look at the imports section of the DESCRIPTION file). Imports: devtools, git2r, whisker, rstudioapi, rmarkdown, knitr, bookdown, curl, RCurl, jsonlite, methods, httr, usethis, clisymbols, crayon, glue, readr (&gt;= 1.1.1) Now, install some additional packages we’ll need for the workshop. install.packages(c( # source paper analysis &quot;ggthemes&quot; # bibliographic / publishing &quot;citr&quot;, &quot;rticles&quot;, # documentation &quot;roxygen2&quot;, # graphics &quot;Cairo&quot;)) Get workshop materials Today we’ll be working with a subset of materials from the published compendium of code, data, and author’s manuscript: Carl Boettiger. (2018, April 17). cboettig/noise-phenomena: Supplement to: “From noise to knowledge: how randomness generates novel phenomena and reveals information” (Version revision-2). Zenodo. http://doi.org/10.5281/zenodo.1219780 accompanying the publication: Carl Boettiger . From noise to knowledge: how randomness generates novel phenomena and reveals information. Published in Ecology Letters, 22 May 2018 https://doi.org/10.1111/ele.13085 You can download the materials using usethis::use_course() and supplying a path to a destination folder to argument destdir: usethis::use_course(url = &quot;bit.ly/rrtools_wks&quot;, destdir = &quot;~/Desktop&quot;) This will download everything we need from a GitHub repository as a .zip file, unzip it and launch it in a new Rstudio session for us to explore. Inspect materials ├── README.md &lt;- .......................repo README ├── analysis.R &lt;- ......................analysis underlying paper ├── gillespie.csv &lt;- ...................data ├── paper.pdf &lt;- .......................LaTex pdf of the paper ├── paper.txt &lt;- .......................text body of the paper ├── refs.bib &lt;- ........................bibtex bibliographic file └── rrtools-wkshp-materials.Rproj &lt;- ...rstudio project file In this workshop we’ll attempt a partial reproduction of the original paper using the materials we’ve just downloaded. We’ll use this as an opportunity to create a new research compendium using rrtools and friends! 🎊 🚦 Create compendium Now that we’ve got all the materials we need, let’s start by *creating a blank research compendium for us to work in. load library First we need to load rrtools library(rrtools) ## ✖ Git is installed on this computer, but not configured for use. For more information on configuring and using Git, see http://happygitwithr.com/ This performs a quick check to confirm you have Git installed and configured If you do, you should see the following output in the console. ✔ Git is installed on this computer, your username is annakrystalli create compendium Now we’re ready to create our compendium. We use function rrtools::create_compendium and supply it with a path at which our compendium will be created. The final part of our path becomes the compendium name. Because the function effectively creates a package, only a single string of lowercase alpha characters is accepted as a name. so let’s go for rrcompendium as the final part of our path. To create rrcompendium in a directory called Documents/workflows/ I use: rrtools::create_compendium(&quot;~/Documents/workflows/rrcompendium&quot;) Go ahead and create a compendium at a location of your choice. Stick with compendium name rrcompendium for ease of following the materials. If the call was successfull you should see the following console output: ✔ Setting active project to &#39;/Users/Anna/Documents/workflows/rrcompendium&#39; ✔ Creating &#39;R/&#39; ✔ Creating &#39;man/&#39; ✔ Writing &#39;DESCRIPTION&#39; ✔ Writing &#39;NAMESPACE&#39; ✔ Writing &#39;rrcompendium.Rproj&#39; ✔ Adding &#39;.Rproj.user&#39; to &#39;.gitignore&#39; ✔ Adding &#39;^rrcompendium\\\\.Rproj$&#39;, &#39;^\\\\.Rproj\\\\.user$&#39; to &#39;.Rbuildignore&#39; ✔ Opening new project &#39;rrcompendium&#39; in RStudio ✔ The package rrcompendium has been created ✔ Opening the new compendium in a new RStudio session... Next, you need to: ↓ ↓ ↓ ● Edit the DESCRIPTION file ● Use other &#39;rrtools&#39; functions to add components to the compendium and a new Rstudio session launched for the compendium: Initiate git We can initialise our compendium with .git using: usethis::use_git() N.B. Beware, you may have ended up with two Rstudio sessions of rrcompendium. Make sure to only have one session of a single project at one time to avoid problems. Inspect templates . ├── DESCRIPTION &lt;- .............................package metadata | dependency management ├── NAMESPACE &lt;- ...............................AUTO-GENERATED on build ├── R &lt;- .......................................folder for functions ├── man &lt;- .....................................AUTO-GENERATED on build └── rrcompendium.Rproj &lt;- ......................rstudio project file rrtools::create_compendium() creates the bare backbone of infrastructure required for a research compendium. At this point it provides facilities to store general metadata about our compendium (eg bibliographic details to create a citation) and manage dependencies in the DESCRIPTION file and store and document functions in the R/ folder. Together these allow us to manage, install and share functionality associated with our project. 🚦 Update DESCRIPTION file Let’s update some basic details in the DESCRIPTION file: Package: rrcompendium Title: What the Package Does (One Line, Title Case) Version: 0.0.0.9000 Authors@R: person(given = &quot;First&quot;, family = &quot;Last&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;), email = &quot;first.last@example.com&quot;) Description: What the package does (one paragraph) License: What license it uses ByteCompile: true Encoding: UTF-8 LazyData: true Title Let’s start with giving our compendium a descriptive title: Title: Partial Reproduction of Boettiger Ecology Letters 2018;21:1255–1267 with rrtools Version We don’t need to change the version now but using semantic versioning for our compendium can be a really useful way to track versions. In general, versions below 0.0.1 are in development, hence the DESCRIPTION file defaults to 0.0.0.9000. Authors Next let’s specify the author of the compendium. Edit with your own details. Authors@R: person(given = &quot;Anna&quot;, family = &quot;Krystalli&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;), email = &quot;annakrystalli@googlemail.com&quot;) For more details on specifying authors, check documentation for ?person Description Let’s add a bit more detail about the contents of the compendium in the Description. Description: This repository contains the research compendium of the partial reproduction of Boettiger Ecology Letters 2018;21:1255–1267. The compendium contains all data, code, and text associated with this sub-section of the analysis License Finally, let’s add a license for the material we create. We’ll use an MIT license. Note however that his only covers the code. We can do this with: usethis::use_mit_license() ✔ Setting License field in DESCRIPTION to &#39;MIT + file LICENSE&#39; ✔ Writing &#39;LICENSE.md&#39; ✔ Adding &#39;^LICENSE\\\\.md$&#39; to &#39;.Rbuildignore&#39; This creates files LICENSE and LICENSE.md and updates the DESCRIPTION file with details of the license. License: MIT + file LICENSE Recap We’ve finished updating our DESCRIPTION file! 🎉 It should look a bit like this: Package: rrcompendium Title: Partial Reproduction of Boettiger Ecology Letters 2018;21:1255–1267 with rrtools Version: 0.0.0.9000 Authors@R: person(given = &quot;Anna&quot;, family = &quot;Krystalli&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;), email = &quot;annakrystalli@googlemail.com&quot;) Description: This repository contains the research compendium of the partial reproduction of Boettiger Ecology Letters 2018;21:1255–1267. The compendium contains all data, code, and text associated with this sub-section of the analysis. License: MIT + file LICENSE ByteCompile: true Encoding: UTF-8 LazyData: true and your project folder should contain: . ├── DESCRIPTION ├── LICENSE ├── LICENSE.md ├── NAMESPACE ├── R ├── man └── rrcompendium.Rproj Let’s commit our work and move on to preparing our compendium for sharing on GitHub. 🚦Sharing a compendium on GitHub Create README Every GitHub repository needs a README landing page. We can create an rrtools README template using: rrtools::use_readme_rmd() ✔ Creating &#39;README.Rmd&#39; from template. ✔ Adding &#39;README.Rmd&#39; to `.Rbuildignore`. ● Modify &#39;README.Rmd&#39; ✔ Rendering README.Rmd to README.md for GitHub. ✔ Adding code of conduct. ✔ Creating &#39;CONDUCT.md&#39; from template. ✔ Adding &#39;CONDUCT.md&#39; to `.Rbuildignore`. ✔ Adding instructions to contributors. ✔ Creating &#39;CONTRIBUTING.md&#39; from template. ✔ Adding &#39;CONTRIBUTING.md&#39; to `.Rbuildignore`. This generates README.Rmd and renders it to README.md, ready to display on GitHub. It contains: a template citation to show others how to cite your project. license information for the text, figures, code and data in your compendium --- output: github_document --- &lt;!-- README.md is generated from README.Rmd. Please edit that file --&gt; ``{r, echo = FALSE} knitr::opts_chunk$set( collapse = TRUE, comment = &quot;#&gt;&quot;, fig.path = &quot;README-&quot; ) `` # rrcompendium This repository contains the data and code for our paper: &gt; Authors, (YYYY). _Title of paper_. Name of journal/book &lt;https://doi.org/xxx/xxx&gt; Our pre-print is online here: &gt; Authors, (YYYY). _Title of paper_. Name of journal/book, Accessed 26 Apr 2021. Online at &lt;https://doi.org/xxx/xxx&gt; ### How to cite Please cite this compendium as: &gt; Authors, (2021). _Compendium of R code and data for &#39;Title of paper&#39;_. Accessed 26 Apr 2021. Online at &lt;https://doi.org/xxx/xxx&gt; ### How to download or install You can download the compendium as a zip from from this URL: &lt;/archive/master.zip&gt; Or you can install this compendium as an R package, rrcompendium, from GitHub with: ### Licenses **Text and figures :** [CC-BY-4.0](http://creativecommons.org/licenses/by/4.0/) **Code :** See the [DESCRIPTION](DESCRIPTION) file **Data :** [CC-0](http://creativecommons.org/publicdomain/zero/1.0/) attribution requested in reuse ### Contributions We welcome contributions from everyone. Before you get started, please see our [contributor guidelines](CONTRIBUTING.md). Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms. The call also adds two other markdown files: CONDUCT.md: a code of conduct for users CONTRIBUTING.md:: basic instructions for people who want to contribute to our compendium update README There’s five main edits we need to make to the template: edit compendium DOI details Although we don’t have a link to the DOI, we can complete the rest of the details and leave it as a place holder. This repository contains the data and code for our reproduction paper: &gt; Krystalli, A, (2018). _Partial Reproduction of Boettiger Ecology Letters 2018;21:1255–1267 with rrtools_. &lt;https://doi.org/{DOI-to-paper}&gt; edit paper.pdf DOI Our reproduction pre-print is online here: &gt; Krystalli, A, (2018). _Partial Reproduction of Boettiger Ecology Letters 2018;21:1255–1267 with rrtools_, Accessed 26 Apr 2021. Online at &lt;https://doi.org/{DOI-to-compendium}&gt; edit compendium citation Please cite this compendium as: &gt; Krystalli, A, (2021). _Compendium of R code and data for &#39;Partial Reproduction of Boettiger Ecology Letters 2018;21:1255–1267 with rrtools&#39;_. Accessed 26 Apr 2021. Online at &lt;https://doi.org/{DOI-to-compendium}&gt; update zip url This is a link to download a zipped file of the repository. To update the template, just paste the url of your compendium repository like so: ### How to download or install You can download the compendium as a zip from from this URL: &lt;https://github.com/annakrystalli/rrcompendium/archive/master.zip&gt; adjust data LICENSE Let’s adjust the data LICENSE to match the source compendium license, which is CC-BY 4.0. Let’s also add Carl Boettiger as copyright holder of the data. **Text and figures :** [CC-BY-4.0](http://creativecommons.org/licenses/by/4.0/), Copyright (c) 2018 Carl Boettiger. **Code :** See the [DESCRIPTION](DESCRIPTION) file **Data :** [CC-BY-4.0](http://creativecommons.org/licenses/by/4.0/), Copyright (c) 2018 Carl Boettiger. Remember to knit your README.Rmd to it’s .md version. 🚦 Create GitHub repository Next, we’ll create a GitHub repository to share our compendium. We’ll make use of our GITHUB_PAT and go for https authentication. We can do this with function: usethis::use_github(protocol = &quot;https&quot;) ✔ Setting active project to &#39;/Users/Anna/Documents/workflows/rrcompendium&#39; ● Check title and description Name: rrcompendium Description: Partial Reproduction of Boettiger Ecology Letters 2018;21:1255–1267 with rrtools The function will prompt you to confirm the name and description for your GitHub repo which it parses from our DESCRIPTION file. If everything looks good select the affirmative option. Are title and description ok? 1: Not now 2: Definitely 3: Nope If creation of the repo was successfull you should see the following console output: ✔ Creating GitHub repository ✔ Adding GitHub remote ✔ Adding GitHub links to DESCRIPTION ✔ Setting URL field in DESCRIPTION to &#39;https://github.com/annakrystalli/rrcompendium&#39; ✔ Setting BugReports field in DESCRIPTION to &#39;https://github.com/annakrystalli/rrcompendium/issues&#39; ✔ Pushing to GitHub and setting remote tracking branch ✔ Opening URL https://github.com/annakrystalli/rrcompendium Commit and push to GitHub We’ve now completed our rrtools README.Rmd! 🎉 Render it to update the README.md file which is displayed on GitHub Commit and push to GitHub. You’re Github repository README should look like this on the site: and your project folder should contain: . ├── CONDUCT.md ├── CONTRIBUTING.md ├── DESCRIPTION ├── LICENSE ├── LICENSE.md ├── NAMESPACE ├── R ├── README.Rmd ├── README.md ├── man └── rrcompendium.Rproj 🚦 Setting up the analysis folder Create analysis We now need an analysis folder to contain our analysis and paper. We can do this using function rrtools::use_analysis() The function has three location = options: top_level to create a top-level analysis/ directory inst to create an inst/ directory (so that all the sub-directories are available after the package is installed) vignettes to create a vignettes/ directory (and automatically update the DESCRIPTION). The default is a top-level analysis/. rrtools::use_analysis() ✔ Adding bookdown to Imports ✔ Creating &#39;analysis&#39; directory and contents ✔ Creating &#39;analysis&#39; ✔ Creating &#39;analysis/paper&#39; ✔ Creating &#39;analysis/figures&#39; ✔ Creating &#39;analysis/templates&#39; ✔ Creating &#39;analysis/data&#39; ✔ Creating &#39;analysis/data/raw_data&#39; ✔ Creating &#39;analysis/data/derived_data&#39; ✔ Creating &#39;references.bib&#39; from template. ✔ Creating &#39;paper.Rmd&#39; from template. Next, you need to: ↓ ↓ ↓ ↓ ● Write your article/report/thesis, start at the paper.Rmd file ● Add the citation style library file (csl) to replace the default provided here, see https://github.com/citation-style-language/ ● Add bibliographic details of cited items to the &#39;references.bib&#39; file ● For adding captions &amp; cross-referencing in an Rmd, see https://bookdown.org/yihui/bookdown/ ● For adding citations &amp; reference lists in an Rmd, see http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html Note that:  Your data files are tracked by Git and will be pushed to GitHub Regardless for location option, the contents of the created sub-directories are the same: analysis/ | ├── paper/ │ ├── paper.Rmd # this is the main document to edit │ └── references.bib # this contains the reference list information ├── figures/ # location of the figures produced by the Rmd | ├── data/ │ ├── DO-NOT-EDIT-ANY-FILES-IN-HERE-BY-HAND │ ├── raw_data/ # data obtained from elsewhere │ └── derived_data/ # data generated during the analysis | └── templates ├── journal-of-archaeological-science.csl | # this sets the style of citations &amp; reference list ├── template.docx # used to style the output of the paper.Rmd └── template.Rmd Let’s inspect paper.Rmd paper.Rmd is ready to write in and render with bookdown. It includes: a YAML header that identifies the references.bib file and the supplied csl file (Citation Style Language) to style the reference list) a colophon that adds some git commit details to the end of the document. This means that the output file (HTML/PDF/Word) is always traceable to a specific state of the code. references.bib The references.bib file has just one item to demonstrate the format. It is ready to insert more reference details. We can replace the supplied csl file with a different citation style from https://github.com/citation-style-language/ 🚦 Reproduce a paper in Rmd In this section we’re going to create a literate programming document to reproduce the paper in a format suitable for journal submission or as a pre-print. We’ll do this using the course materials we downloaded. In particular, we’re going to combine the code in analysis.R, the text in paper.txt and the references in the refs.bib file in an .Rmd document to reproduce paper.pdf. More information on working on academic journals with Bookdown Setup data Copy data to data/ To begin, let’s copy gillespie.csv from the course materials you downloaded in rrtools-wkshp-materials-master/ to the subfolder analysis/data/raw_data/ in rrcompendium Your data folder should now look like this: analysis/data ├── DO-NOT-EDIT-ANY-FILES-IN-HERE-BY-HAND ├── derived_data └── raw_data └── gillespie.csv Inspect analysis.R file Let’s also open analysis.R in the course materials and run the code. The script has some initial setup, then loads the data, recodes one of the columns for plotting and then plots the results of the simulation, which generates figure 1 in paper.pdf. analysis.R library(dplyr) library(readr) library(ggplot2) library(ggthemes) theme_set(theme_grey()) # load-data data &lt;- read_csv(here::here(&quot;gillespie.csv&quot;), col_types = &quot;cdiddd&quot;) # create colour palette colours &lt;- ptol_pal()(2) # recode-data data &lt;- data %&gt;% mutate(system_size = recode(system_size, large = &quot;A. 1000 total sites&quot;, small = &quot;B. 100 total sites&quot;)) # plot-gillespie data %&gt;% ggplot(aes(x = time)) + geom_hline(aes(yintercept = mean), lty=2, col=colours[2]) + geom_hline(aes(yintercept = minus_sd), lty=2, col=colours[2]) + geom_hline(aes(yintercept = plus_sd), lty=2, col=colours[2]) + geom_line(aes(y = n), col=colours[1]) + facet_wrap(~system_size, scales = &quot;free_y&quot;) 🚦 Create journal article template using rticles The rticles package is designed to simplify the creation of documents that conform to submission standards. A suite of custom R Markdown templates for popular journals is provided by the package. delete paper/ subdirectory First, let’s delete the current analysis/paper folder as we’re going to create a new paper.Rmd template. create new paper template This particular paper was published in Ecology Letters, an Elsevier Journal. We can create a new paper.Rmd template from the templates provided by rticles package. We can use the New R Markdown dialog Select: Template: Elesevier Journal Article Name: paper Location: ~/Documents/workflows/rrcompendium/analysis Or we can use rmarkdown::draft() to create articles: rmarkdown::draft(here::here(&quot;analysis&quot;,&quot;paper.Rmd&quot;), template = &quot;elsevier_article&quot;, package = &quot;rticles&quot;) Both these functions create the following files in a new directory analysis/paper. analysis/paper ├── elsarticle.cls ├── mybibfile.bib ├── numcompress.sty └── paper.Rmd The elsarticle.cls contains contains the citation language style for the references. The mybibfile.bib contains an example reference list. The new paper.Rmd is the file we will be working in. Let’s open it up and start editing it. 🚦 Update YAML The YAML header in Paper.Rmd contains document wide metadata and is pre-populated with some fields relevant to an academic publication. --- title: Short Paper author: - name: Alice Anonymous email: alice@example.com affiliation: Some Institute of Technology footnote: Corresponding Author - name: Bob Security email: bob@example.com affiliation: Another University address: - code: Some Institute of Technology address: Department, Street, City, State, Zip - code: Another University address: Department, Street, City, State, Zip abstract: | This is the abstract. It consists of two paragraphs. journal: &quot;An awesome journal&quot; date: &quot;2021-04-26&quot; bibliography: mybibfile.bib output: rticles::elsevier_article --- Here we’re going to reproduce paper.pdf as is, so we’ll actually be editing the file with details from the original publication. First, let’s clear all text BELOW the YAML header (which is delimited by ---. DO NOT delete the YAML header). Next, let’s open paper.txt from the course material which contains all text from the in paper.pdf. We can use it to complete some of the fields in the YAML header. title Add the paper title to this field title: &quot;From noise to knowledge: how randomness generates novel phenomena and reveals information&quot; author Here we specify author details. author: - name: &quot;Carl Boettiger&quot; affiliation: a email: &quot;cboettig@berkeley.edu&quot; address Here we specify the addresses associated with the affiliations specified in authors address: - code: a address: &quot;Dept of Environmental Science, Policy, and Management, University of California Berkeley, Berkeley CA 94720-3114, USA&quot; Note that the field code in address cross-references with the affiliations specified in author. bibliography Before specifying the bibliography, we need to copy the refs.bib file associated with paper.pdf from the course materials and save it in our analysis/paper subdirectory. Next we can set the refs.bib as the source for our paper’s bibliograpraphy: bibliography: refs.bib 🚦 layout We can add an additional field called layout which specifies the layout of the output and takes the following values. review: doublespace margins 3p: singlespace margins 5p: two-column Let’s use singlespace margins layout: 3p preamble We can add also an additional field called preamble. This allows us to include LaTeX packages and functions. We’ll use the following to add linenumbers and doublespacing. preamble: | \\usepackage[nomarkers]{endfloat} \\linenumbers \\usepackage{setspace} \\doublespacing 🚦 abstract This field should contain the abstract abstract: | # Abstract Noise, as the term itself suggests, is most often seen a nuisance to ecological insight, a inconvenient reality that must be acknowledged, a haystack that must be stripped away to reveal the processes of interest underneath. Yet despite this well-earned reputation, noise is often interesting in its own right: noise can induce novel phenomena that could not be understood from some underlying determinstic model alone. Nor is all noise the same, and close examination of differences in frequency, color or magnitude can reveal insights that would otherwise be inaccessible. Yet with each aspect of stochasticity leading to some new or unexpected behavior, the time is right to move beyond the familiar refrain of &quot;everything is important&quot; (Bjørnstad &amp; Grenfell 2001). Stochastic phenomena can suggest new ways of inferring process from pattern, and thus spark more dialog between theory and empirical perspectives that best advances the field as a whole. I highlight a few compelling examples, while observing that the study of stochastic phenomena are only beginning to make this translation into empirical inference. There are rich opportunities at this interface in the years ahead. output The output format. In this case, the template is correctly pre-populated with rticles::elsevier_article so no need to edit. output: rticles::elsevier_article 🚦 Add text Now let’s add the main body of the paper from paper.txt. add new page after abstract First, let’s a add a new page after the abstract using: \\newpage copy and paste text from paper.txt We do not need the details we’ve just completed the YAML with, so ignore the title, abstract etc and just copy everything in paper.txt from the Introduction header down to and including the reference section header. # Introduction: Noise the nuisance To many, stochasticity, or more simply, noise, is just that -- something which obscures patterns we are ... ... ... ... ... # Acknowledgements The author acknowledges feedback and advice from the editor, Tim Coulson and two anonymous reviewers. This work was supported in part by USDA National Institute of Food and Agriculture, Hatch project CA-B-INS-0162-H. # References Check pdf output Let’s knit our document and have our first look at the resulting pdf by clicking on the Knit tab. 🚦 Update references Next we’ll replace the flat citations in the text with real linked citation which can be used to auto-generate formatted inline citations and the references section. Insert formatted citations We’ll use the citr package, which provides functions and an RStudio addin to search a BibTeX-file to create and insert formatted Markdown citations into the current document. Once citr is installed and you have restarted your R session, the addin appears in the addin menu. The addin will automatically look up the Bib(La)TeX-file(s) specified in the YAML front matter. To insert a citation Select text to replace with a citation Launch citr addin: Search for citation to insert Select citation to insert Insert citation Carry on updating the rest of the citations. Don’t forget to check the abstract for citations! 🚦 Update math For the sake of time today, and not to open this topic too deeply here, I’ve included the following LaTex equation syntax in the text: \\begin{align} \\frac{\\mathrm{d} n}{\\mathrm{d} t} = \\underbrace{c n \\left(1 - \\frac{n}{N}\\right)}_{\\textrm{birth}} - \\underbrace{e n}_{\\textrm{death}}, \\label{levins} \\end{align} that generates equation 1 in the paper.pdf. \\[\\begin{align} \\frac{\\mathrm{d} n}{\\mathrm{d} t} = \\underbrace{c n \\left(1 - \\frac{n}{N}\\right)}_{\\textrm{birth}} - \\underbrace{e n}_{\\textrm{death}}, \\label{levins} \\end{align}\\] So you don’t need to edit anything here. Check Math expressions and Markdown extensions by bookdown for more information. update inline math Inline LaTeX equations and parameters can be written between a pair of dollar signs using the LaTeX syntax, e.g., $f(x) = 1/x$ generates \\(f(x) = 1/x\\). Using paper.pdf to identify mathematical expressions in the text (generally they appear in italic), edit your paper.Rmd, enclosing them between dollar signs. Check pdf output Let’s knit our document to check our references and maths annotations have been updated correctly by clicking on the Knit tab. 🚦 Add code Now that we’ve set up the text for our paper, let’s insert the code to generate figure 1. Add libraries chunk First let’s insert a libraries code chunk right at the top of the document to set up our analysis. Because it’s a setup chunk we set include = F which suppresses all output resulting from chunk evaluation. ```{r libraries, include=FALSE} ``` set document chunk options Now, let’s set some knitr options for the whole document by adding the following code to our libraries chunk: knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, dev=&quot;cairo_pdf&quot;, fig.width=7, fig.height=3.5) We’re setting default chunk options to: echo = FALSE message = FALSE warning = FALSE to suppress code, warnings and messages in the output, and dev=\"cairo_pdf\" fig.width=7 fig.height=3.5 to specify how figures will appear. add libraries Copy and paste the code for loading all the libraries from analysis.R. Add library rrcompendium so we can access function recode_system_size. The libraries chunk should now look like so: knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, dev=&quot;cairo_pdf&quot;, fig.width=7, fig.height=3.5) library(dplyr) library(readr) library(ggplot2) library(ggthemes) library(rrcompendium) Add set-theme chunk Right below the libraries chunk, insert a new chunk and call it set-theme Copy the code to set the plot theme and pasted into the set-theme code: theme_set(theme_grey()) Add figure1 chunk Now scroll down towards the bottom of the document and create a new chunk just above the Conclusions section. Call it figure1 Add code Copy and paste the remaining code into a new chunk which will create figure 1. # create colour palette colours &lt;- ptol_pal()(2) # load-data data &lt;- read_csv(here::here(&quot;gillespie.csv&quot;), col_types = &quot;cdiddd&quot;) # recode-data data &lt;- data %&gt;% mutate(system_size = recode(system_size, large = &quot;A. 1000 total sites&quot;, small= &quot;B. 100 total sites&quot;)) # plot-gillespie data %&gt;% ggplot(aes(x = time)) + geom_hline(aes(yintercept = mean), lty=2, col=colours[2]) + geom_hline(aes(yintercept = minus_sd), lty=2, col=colours[2]) + geom_hline(aes(yintercept = plus_sd), lty=2, col=colours[2]) + geom_line(aes(y = n), col=colours[1]) + facet_wrap(~system_size, scales = &quot;free_y&quot;) 🚦 Add caption Finally, let’s update chunk figure1 to include a figure caption. The text for the caption is at the bottom of paper.txt. We can include it in the chunk header through chunk option fig.cap like so: ```{r figure1, figure1, fig.cap=&quot;Population dynamics from a Gillespie simulation of the Levins model with large (N=1000, panel A) and small (N=100, panel B) number of sites (blue) show relatively weaker effects of demographic noise in the bigger system. Models are otherwise identical, with e = 0.2 and c = 1 (code in appendix A). Theoretical predictions for mean and plus/minus one standard deviation shown in horizontal re dashed lines.&quot;} ``` Render final document to pdf Let’s check our final work by re-knitting to pdf. You should be looking at something that looks a lot like paper.pdf 🚦 Add paper dependencies Finally, before we’re finished, let’s ensure the dependencies introduced in the paper are included. We can use rrtools::add_dependencies_to_description() rrtools::add_dependencies_to_description() This function scans script files (.R, .Rmd, .Rnw, .Rpres, etc.) for external package dependencies indicated by library(), require() or :: and adds those packages to the Imports field in the package DESCRIPTION: Imports: bookdown, dplyr, ggplot2 (&gt;= 3.0.0), ggthemes (&gt;= 3.5.0), here (&gt;= 0.1), knitr (&gt;= 1.20), rticles (&gt;= 0.6) 🔨 Install and Restart Commit and push to GitHub! Final compendium You can see the resulting rrcompendium here The complete compendium should contain the following files: . ├── CONDUCT.md ├── CONTRIBUTING.md ├── DESCRIPTION ├── LICENSE ├── LICENSE.md ├── NAMESPACE ├── R │ └── process-data.R ├── README.Rmd ├── README.md ├── analysis │ ├── data │ │ ├── DO-NOT-EDIT-ANY-FILES-IN-HERE-BY-HAND │ │ ├── derived_data │ │ └── raw_data │ │ └── gillespie.csv │ ├── figures │ ├── paper │ │ ├── elsarticle.cls │ │ ├── mybibfile.bib │ │ ├── numcompress.sty │ │ ├── paper.Rmd │ │ ├── paper.fff │ │ ├── paper.pdf │ │ ├── paper.spl │ │ ├── paper.tex │ │ ├── paper_files │ │ │ └── figure-latex │ │ │ └── figure1-1.pdf │ │ └── refs.bib │ └── templates │ ├── journal-of-archaeological-science.csl │ ├── template.Rmd │ └── template.docx ├── inst │ └── testdata │ └── gillespie.csv ├── man │ └── recode_system_size.Rd ├── rrcompendium.Rproj └── tests ├── testthat │ └── test-process-data.R └── testthat.R "],["appendix.html", "(PART) APPENDIX", " (PART) APPENDIX individual.R This file should contain your processing workflow and be saved at: data-raw/individual.R Create the file using: usethis::use_data_raw(&quot;individual&quot;) Your script should contain: ## code to prepare `individual` dataset goes here library(dplyr) source(here::here(&quot;R&quot;, &quot;geolocate.R&quot;)) # Read in and compile all individual data raw_data_path &lt;- here::here(&quot;data-raw&quot;, &quot;wood-survey-data-master&quot;) individual_path &lt;- fs::path(raw_data_path, &quot;individual&quot;) individual_paths &lt;- fs::dir_ls(individual_path) # read in all tables into one individual &lt;- purrr::map_df(individual_paths, ~readr::read_csv(file = .x, col_types = readr::cols(.default = &quot;c&quot;))) %&gt;% readr::type_convert() individual %&gt;% readr::write_csv(path = fs::path(raw_data_path, &quot;vst_individuals.csv&quot;)) # Combine NEON data tables # read in additonal table maptag &lt;- readr::read_csv(fs::path(raw_data_path, &quot;vst_mappingandtagging.csv&quot;)) %&gt;% select(-eventID) perplot &lt;- readr::read_csv(fs::path(raw_data_path, &quot;vst_perplotperyear.csv&quot;)) %&gt;% select(-eventID) individual %&lt;&gt;% left_join(maptag, by = &quot;individualID&quot;, suffix = c(&quot;&quot;, &quot;_map&quot;)) %&gt;% left_join(perplot, by = &quot;plotID&quot;, suffix = c(&quot;&quot;, &quot;_ppl&quot;)) %&gt;% assertr::assert(assertr::not_na, stemDistance, stemAzimuth, pointID, decimalLongitude, decimalLatitude, plotID) # ---- Geolocate individuals_functions ---- individual &lt;- individual %&gt;% dplyr::mutate(stemLat = get_stem_location(decimalLongitude = decimalLongitude, decimalLatitude = decimalLatitude, stemAzimuth = stemAzimuth, stemDistance = stemDistance)$lat, stemLon = get_stem_location(decimalLongitude = decimalLongitude, decimalLatitude = decimalLatitude, stemAzimuth = stemAzimuth, stemDistance = stemDistance)$lon) %&gt;% janitor::clean_names() # create data directory fs::dir_create(here::here(&quot;data&quot;)) # write out analytic file readr::write_csv(individual, here::here(&quot;data&quot;, &quot;individual.csv&quot;)) "],["geolocate-r.html", "geolocate.R", " geolocate.R This file should contain your function and be saved at: R/geolocate.R Create the file using: usethis::use_r(&quot;geolocate&quot;) Your script should contain: # Function get_stem_location &lt;- function(decimalLongitude, decimalLatitude, stemAzimuth, stemDistance) { # check inputs are correct type (numeric) checkmate::assert_numeric(decimalLatitude) checkmate::assert_numeric(decimalLongitude) checkmate::assert_numeric(stemAzimuth) checkmate::assert_numeric(stemDistance) out &lt;- geosphere::destPoint(p = cbind(decimalLongitude, decimalLatitude), b = stemAzimuth, d = stemDistance) %&gt;% tibble::as_tibble() # check output for NAs checkmate::assert_false(any(is.na(out))) return(out) } "],["analysis-r-1.html", "analysis.R", " analysis.R This R script should contain your analysis and be saved in the root directory as: analysis.R Your script should contain: ## @knitr analysis-setup library(ggplot2) library(magrittr) individual &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;individual.csv&quot;)) %&gt;% dplyr::select(stem_diameter, height, growth_form) ## @knitr analysis-filter-data analysis_df &lt;- individual %&gt;% dplyr::filter(!is.na(growth_form), growth_form != &quot;liana&quot;) ## @knitr analysis-set-factor-levels gf_levels &lt;- table(analysis_df$growth_form) %&gt;% sort(decreasing = TRUE) %&gt;% names() analysis_df %&lt;&gt;% dplyr::mutate(growth_form = factor(growth_form, levels = gf_levels)) ## @knitr analysis-fig1-barplot analysis_df %&gt;% ggplot(aes(y = growth_form, colour = growth_form, fill = growth_form)) + geom_bar(alpha = 0.5, show.legend = FALSE) ## @knitr analysis-fg2-violinplots analysis_df %&gt;% tidyr::pivot_longer(cols = c(stem_diameter, height), names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = log(value), y = growth_form, colour = growth_form, fill = growth_form)) + geom_violin(alpha = 0.5, trim = T, show.legend = FALSE) + geom_boxplot(alpha = 0.7, show.legend = FALSE) + facet_grid(~var) ## @knitr analysis-lm-overall lm_overall &lt;- lm(log(stem_diameter) ~ log(height), analysis_df) lm_overall %&gt;% broom::glance() lm_overall %&gt;% broom::tidy() ## @knitr analysis-lm-fig3-overall analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter))) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) ## @knitr analysis-lm-growth lm_growth &lt;- lm(log(stem_diameter) ~ log(height) * growth_form, analysis_df) lm_growth %&gt;% broom::glance() lm_growth %&gt;% broom::tidy() ## @knitr analysis-lm-fig4-growth analysis_df %&gt;% ggplot(aes(x = log(height), y = log(stem_diameter), colour = growth_form)) + geom_point(alpha = 0.1) + geom_smooth(method = &quot;lm&quot;) "],["index-rmd.html", "index.Rmd", " index.Rmd This Rmarkdown file should contain your analysis and be saved in the root directory: index.Rmd My final index.Rmd looks like this: --- title: &quot;Analysis of NEON Woody plant vegetation structure data&quot; author: &quot;Anna Krystalli&quot; date: &quot;06/05/2020&quot; output: html_document: toc: true toc_float: true theme: cosmo highlight: zenburn --- # Background &lt;img src=&quot;https://data.neonscience.org/data-products/static/media/NSF-NEON-logo.192b6661.png&quot; width=&quot;40%&quot;&gt; The [NEON Woody plant vegetation structure dataset](https://data.neonscience.org/data-products/DP1.10098.001) contains structure measurements, including height, canopy diameter, and stem diameter, as well as mapped position of individual woody plants across the survey area. This data product contains the quality-controlled, native sampling resolution data from in-situ measurements of live and standing dead woody individuals and shrub groups, from all terrestrial NEON sites with qualifying woody vegetation. The exact measurements collected per individual depend on growth form, and these measurements are focused on enabling biomass and productivity estimation, estimation of shrub volume and biomass, and calibration / validation of multiple NEON airborne remote-sensing data products. Our analyses focus on the **relationship between individual stem height and diameter** and how that relationship **varies across growth forms**. # Data The data were downloaded from the NEON data portal and processed into a single table using script `data-raw/individual.R` ```{r, echo=FALSE} knitr::read_chunk(here::here(&quot;analysis.R&quot;)) ``` ### Read in data and setup analysis First we read in the data and select only the columns we are interested in, i.e `stem_diameter`, `height` and `growth_form` ```{r analysis-setup, message=FALSE} ``` ### Prepare data To prepare the data we exclude rows for which the value of `growth_form` was `NA` or `liana`. ```{r analysis-filter-data} ``` We also convert `growth_form` to a factor and set the levels according to to ascending counts of each level in the raw data. ```{r analysis-set-factor-levels} ``` ```{r} DT::datatable(analysis_df, caption = &quot;Table 1: Prepared analysis data&quot;) ``` Our prepared data is a tibble of `r ncol(analysis_df)` columns and `r nrow(analysis_df)` rows. ### Data properties #### Statistical summaries of our variables ```{r} summary(analysis_df) ``` ```{r analysis-fig1-barplot, fig.cap=&quot;Figure 1: Counts of growth forms&quot;} ``` ```{r analysis-fg2-violinplots, fig.cap=&quot;Figure 2: Distribution and statistical summaries of stem_diameter and height across growth forms.&quot;} ``` ## Analysis ### Modelling overall `stem_diameter` as a function of `height` Initially we fit a linear model of form `log(stem_diameter)` as a function of `log(height)` ```{r analysis-lm-overall} ``` Our model is statistically significant and has modest coverage, indicated by `r.squared` of `r broom::glance(lm_overall)$r.squared` ```{r analysis-lm-fig3-overall, fig.cap=&quot;Figure 3: Log stem diameter as a function of log height&quot;} ``` However, plotting our data reveals sub groups in the data. We can examine whether including `growth_form` in our analysis would improve our model fit by capturing variation explained by differing relationships across growth forms ### Including an interaction with `growth_form` We fit another model, this time including an interaction term for variable `growth_form` ```{r analysis-lm-growth} ``` Our model is still significant but this time explains a larger proportion of the variation (`r broom::glance(lm_growth)$r.squared`). ```{r analysis-lm-fig4-growth, fig.cap=&quot;Figure 4: Log stem diameter as a function of the interaction of log height and growth_form&quot;} ``` ## Session info ```{r} sessionInfo() ``` and renders to: "],["gitignore.html", ".gitignore", " .gitignore This file should contain details of files to be ignored by version control and be saved in the root directory as: .gitignore Your file should contain: .Rproj.user .Rhistory .RData .Rbuildignore .DS_Store attic/* data-raw/* !data-raw/individual.R "],["about.html", "About", " About This course is funded by NERC through the ACCE Doctoral Partnership program It started in 2015 by Dr. Tom Webb &amp; Dr. Anna Krystalli from the University of Sheffield and has been continued since 2016 by Anna Krystalli. Since then it has gone from a one day course on Data management to a two day course, encompassing project management, version control and literate programming. It is aimed at preparing first year PhD students for managing their research code and data successfully, in line with shifting expectations on reproducibility, interoperability, accessibility and archiving of research materials. "],["setup.html", "Software Setup Software Requirements", " Software Setup Software Requirements This workshop assumes you have the R, RStudio and Git and Bash Shell software installed on your computer and a personal GitHub account. You will also need some geospatial system libraries installed. R R can be downloaded here. RStudio RStudio is an environment for developing using R. It can be downloaded here. You will need the Desktop version (&gt; 1.0) for your computer. The Bash Shell Bash is a commonly-used shell that gives you the power to do simple tasks more quickly. Windows Video Tutorial Download the Git for Windows installer. Run the installer and follow the steps bellow: Click on “Next”. Click on “Next”. Keep “Use Git from the Windows Command Prompt” selected and click on “Next”. If you forgot to do this programs that you need for the workshop will not work properly. If this happens rerun the installer and select the appropriate option. Click on “Next”. Keep “Checkout Windows-style, commit Unix-style line endings” selected and click on “Next”. Keep “Use Windows’ default console window” selected and click on “Next”. Click on “Install”. Click on “Finish”. If your “HOME” environment variable is not set (or you don’t know what this is): Open command prompt (Open Start Menu then type cmd and press [Enter]) Type the following line into the command prompt window exactly as shown: setx HOME “%USERPROFILE%” Press [Enter], you should see SUCCESS: Specified value was saved. Quit command prompt by typing exit then pressing [Enter] This will provide you with both Git and Bash in the Git Bash program. Mac OS X The default shell in all versions of Mac OS X is Bash, so no need to install anything. You access Bash from the Terminal (found in /Applications/Utilities). See the Git installation video tutorial for an example on how to open the Terminal. Linux The default shell is usually Bash, but if your machine is set up differently you can run it by opening a terminal and typing bash. There is no need to install anything. Geospatial Libraries Some of the workflows require geospatial packages like sf and have additional system requirements. Follow the installation instructions in sf package documentation according to your operating system. Git &amp; GitHub Required for the Version Control part of the the course Git is a version control system that lets you track who made changes to what when and has options for easily updating a shared or public version of your code on github.com. You will need a supported web browser (current versions of Chrome, Firefox or Safari, or Internet Explorer version 9 or above). You will also need an account at github.com. Basic GitHub accounts are free. We encourage you to create a GitHub account if you don’t have one already. Please consider what personal information you’d like to reveal. For example, you may want to review these instructions for keeping your email address private provided at GitHub. Windows Git should be installed on your computer as part of your Bash install (described above). Mac OS X Video Tutorial For OS X 10.9 and higher, install Git for Mac by downloading and running the most recent “mavericks” installer from this list. After installing Git, there will not be anything in your /Applications folder, as Git is a command line program. For older versions of OS X (10.5-10.8) use the most recent available installer labelled “snow-leopard” available here. Linux If Git is not already available on your machine you can try to install it via your distro’s package manager. For Debian/Ubuntu run sudo apt-get install git and for Fedora run sudo yum install git. Research Compendium Exercise For the final practical sessions, we will need to use LaTeX. If you don’t have LaTeX installed, consider installing TinyTeX, a custom LaTeX distribution based on TeX Live that is small in size but functions well in most cases, especially for R users. install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() Check docs before before installing. devtools requirements You might also need a set of development tools to install and run devtools. On Windows, download and install Rtools, and devtools takes care of the rest. On Mac, install the Xcode command line tools. On Linux, install the R development package, usually called r-devel or r-base-dev. Install R dependecies To be able to run materials locally, you will also need to install all the required R packages. Run the following code: dependencies &lt;- c(&quot;devtools&quot;, &quot;rmarkdown&quot;, &quot;knitr&quot;, &quot;tidyverse&quot;, &quot;bookdown&quot;, &quot;DT&quot;, &quot;skimr&quot;, &quot;gapminder&quot;, &quot;plotly&quot;, &quot;here&quot;, &quot;reprex&quot;, &quot;usethis&quot;, &quot;cowsay&quot;, &quot;testthat&quot;, &quot;ggthemes&quot;, &quot;fs&quot;, &quot;jsonlite&quot;, &quot;listviewer&quot;, &quot;checkmate&quot;, &quot;assertr&quot;, &quot;vroom&quot;, &quot;data.table&quot;, &quot;sloop&quot;, &quot;geosphere&quot;, &quot;raster&quot;, &quot;spData&quot;, &quot;tmap&quot;, &quot;leaflet&quot;, &quot;sf&quot;) # install CRAN dependencies install.packages(dependencies) # install github dependencies devtools::install_github(&quot;hadley/emo&quot;) devtools::install_github(&quot;benmarwick/rrtools&quot;) devtools::install_github(&quot;ropenscilabs/dataspice&quot;) devtools::install_github(&quot;karthik/holepunch&quot;) # install tinytex tinytex::install_tinytex() FAQs 1. Are there any advantages or disadvantages to setting up a github account with our university email address? Is it possible to change emails say when we finish our PhD? I personally prefer to use a non-institutional email for registering accounts to platforms I want smooth access to regardless of affiliation. However, there are advantages associated with affiliation with an academic institution on GitHub, namely that you get a free developer account. The most important benefit of that is that it gives you unlimited public AND private repositories. You can however add your academic email as a secondary email which will allow you to benefit from this academic research discount. You can also just use your academic address from the start and just change it once you move on. Find out more about claiming an academic discount here. "],["solutions.html", "Solutions", " Solutions List subsetting challenge xlist$b[2] ## [1] 2 xlist[[2]][2] ## [1] 2 xlist[[&quot;b&quot;]][2] ## [1] 2 Finding column name matches in two tables names(individual)[names(individual) %in% names(maptag)] ## [1] &quot;uid&quot; &quot;eventID&quot; &quot;individualID&quot; dataspice mega challenge! variables &lt;- readr::read_csv(here::here(&quot;data-raw&quot;, &quot;wood-survey-data-master&quot;, &quot;NEON_vst_variables.csv&quot;), col_types = readr::cols(.default = &quot;c&quot;)) %&gt;% dplyr::mutate(fieldName = janitor::make_clean_names(fieldName)) %&gt;% select(fieldName, description, units) attributes &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;metadata&quot;, &quot;attributes.csv&quot;)) %&gt;% select(-description, -unitText) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## fileName = col_character(), ## variableName = col_character(), ## description = col_logical(), ## unitText = col_logical() ## ) dplyr::left_join(attributes, variables, by = c(&quot;variableName&quot; = &quot;fieldName&quot;)) %&gt;% dplyr::rename(unitText = &quot;units&quot;) %&gt;% readr::write_csv(here::here(&quot;data&quot;, &quot;metadata&quot;, &quot;attributes.csv&quot;)) edit_attributes() "]]
